{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29f74e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Hard-PC\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# EDEK\n",
    "!pip install -q transformers rich torchmetrics sklearn pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47c95100",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'botocore'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mtypes\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mibm_boto3\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbag\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdb\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'botocore'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, types\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "import dask.bag as db\n",
    "import json\n",
    "import zipfile\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import csv\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import RichProgressBar\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from torchmetrics import F1\n",
    "from torchmetrics.functional import accuracy\n",
    "from transformers import RobertaTokenizer, RobertaModel, AutoTokenizer, AutoModel\n",
    "from transformers import AutoModel, AdamW, get_cosine_schedule_with_warmup\n",
    "import math\n",
    "from torchmetrics.functional.classification import auroc\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eac73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24d59e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_json_data = \"arxiv-metadata-oai-snapshot-json.zip\"\n",
    "pickled_dataset = 'dataset_pickle.pkl'\n",
    "pickled_dataset_onehot = 'dataset_multilabel.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd418d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __iter__(self): return 0\n",
    "\n",
    "ibm_api_key_id = 'SECRET'\n",
    "bucket = 'SECRET'\n",
    "\n",
    "client_c2b4ce19d76d4b7c87bdf6c8c84d662c = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id=ibm_api_key_id,\n",
    "    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url='https://s3.private.us.cloud-object-storage.appdomain.cloud')\n",
    "\n",
    "streaming_body_1 = client_c2b4ce19d76d4b7c87bdf6c8c84d662c.get_object(Bucket=bucket, Key='arxiv-metadata-oai-snapshot.json.zip')['Body']\n",
    "\n",
    "def save_to_cloud(key, file):\n",
    "    client_c2b4ce19d76d4b7c87bdf6c8c84d662c.upload_file(Bucket=bucket, Key=f'snapshot-{file}', Filename=file)\n",
    "\n",
    "    \n",
    "def get_file(key, filename):\n",
    "    client_c2b4ce19d76d4b7c87bdf6c8c84d662c.download_file(Bucket=bucket, Key=key, Filename=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3ce32c",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb034938",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_file('snapshot-uni_df.pkl', 'uni_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ebd1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_onehot = pd.read_pickle('uni_df.pkl')\n",
    "LABEL_COLUMNS = df_onehot.columns.tolist()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8fadb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e462d738",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArxivAbstracts(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text = df.abstract\n",
    "        self.labels = df[df.columns.difference(['abstract'])]\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, item_idx):\n",
    "        text = self.text[item_idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask= True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "          )\n",
    "               \n",
    "        return {\n",
    "          'ids': encoding[\"input_ids\"].flatten(),\n",
    "          'mask': encoding[\"attention_mask\"].flatten(),\n",
    "          'targets': torch.tensor(self.labels.iloc[item_idx].tolist(), dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9638f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 1337\n",
    "df_train, df_test = train_test_split(df_onehot, test_size=0.1, random_state=RANDOM_SEED, shuffle=True)\n",
    "df_train, df_validation = train_test_split(df_train, test_size=0.2, random_state=RANDOM_SEED, shuffle=True)\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "df_validation = df_validation.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed4c69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "class ArxivAbstractsModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, df_train, df_test, df_validation, tokenizer, batch_size=16, max_token_len=256):\n",
    "        super().__init__()\n",
    "        self.df_train = df_train\n",
    "        self.df_test = df_test\n",
    "        self.df_validation = df_validation\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = ArxivAbstracts(df=self.df_train, tokenizer=self.tokenizer, max_len= self.max_token_len)\n",
    "        self.val_dataset = ArxivAbstracts(df=self.df_validation, tokenizer=self.tokenizer, max_len= self.max_token_len)\n",
    "        self.test_dataset = ArxivAbstracts(df=self.df_test, tokenizer=self.tokenizer, max_len= self.max_token_len)\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36429e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilroberta-base')\n",
    "arxiv_data = ArxivAbstractsModule(df_train, df_test, df_validation, tokenizer, max_token_len=384)\n",
    "arxiv_data.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0b557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(arxiv_data.test_dataloader())/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f54881",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArxivAbstractsClassifier(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.embedding = nn.Embedding(100000, 60, padding_idx=1)\n",
    "        self.lstm = nn.LSTM(60, 100, batch_first=True, num_layers=3)\n",
    "        #self.pretrained_model = AutoModel.from_pretrained(config['model_name'], return_dict=True)\n",
    "        #self.hidden = torch.nn.Linear(self.pretrained_model.config.hidden_size, self.pretrained_model.config.hidden_size)\n",
    "        self.classifier = torch.nn.Linear(100, 151) \n",
    "        #torch.nn.init.xavier_uniform_(self.classifier.weight)\n",
    "        self.loss_func = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "        #self.dropout = torch.nn.Dropout()\n",
    "        self.save_hyperparameters()\n",
    "    \n",
    "    def forward(self, ids, mask, targets=None):\n",
    "        # roberta layer\n",
    "#         output = self.pretrained_model(input_ids=ids, attention_mask=mask)\n",
    "#         pooled_output = torch.mean(output.last_hidden_state, 1)\n",
    "#         # final logits\n",
    "#         pooled_output = self.dropout(pooled_output)\n",
    "#         pooled_output = self.hidden(pooled_output)\n",
    "#         pooled_output = F.relu(pooled_output)\n",
    "#         pooled_output = self.dropout(pooled_output)\n",
    "#         logits = self.classifier(pooled_output)\n",
    "        # calculate loss\n",
    "        embed = self.embedding(ids)\n",
    "    \n",
    "        lengths = torch.sum(mask, axis = 1)\n",
    "        \n",
    "        #print(lengths)\n",
    "            \n",
    "        packed_embedded = torch.nn.utils.rnn.pack_padded_sequence(embed, lengths.tolist(), batch_first=True, enforce_sorted=False)\n",
    "    \n",
    "        output, (h_n, c_n)  = self.lstm(packed_embedded)\n",
    "        \n",
    "        unpacked, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True, padding_value=1)\n",
    "        \n",
    "#         print(unpacked)\n",
    "        \n",
    "        logits = self.classifier(unpacked[range(ids.shape[0]), lengths - 1])\n",
    "        \n",
    "        loss = 0\n",
    "        if targets is not None:\n",
    "            loss = self.loss_func(logits.view(-1, 151), targets.view(-1, 151))\n",
    "        return loss, logits\n",
    "  \n",
    "    def training_step(self, batch, batch_index):\n",
    "        loss, outputs = self(**batch)\n",
    "        self.log(\"train_loss\", loss.item())\n",
    "        return {\"loss\":loss, \"predictions\": outputs, \"labels\": batch[\"targets\"]}\n",
    "\n",
    "    def validation_step(self, batch, batch_index):\n",
    "        loss, outputs = self(**batch)\n",
    "        self.log(\"val_loss\", loss.item())\n",
    "        return {\"val_loss\": loss, \"predictions\": outputs, \"labels\": batch[\"targets\"]}\n",
    "\n",
    "    def predict_step(self, batch, batch_index):\n",
    "        loss, outputs = self(**batch)\n",
    "        return torch.sigmoid(outputs)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters())\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a98f8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    monitor='val_loss',\n",
    "    filename='LSTM-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=2,\n",
    "    verbose=True,\n",
    "    mode='min'\n",
    ")\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"lstm\")\n",
    "\n",
    "config = {\n",
    "    'model_name': 'distilroberta-base',\n",
    "    'n_labels': 151,\n",
    "    'batch_size': 16,\n",
    "    'lr': 1.5e-6,\n",
    "    'warmup': 0.2, \n",
    "    'train_size': len(arxiv_data.train_dataloader()),\n",
    "    'weight_decay': 0.001,\n",
    "    'n_epochs': 30\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca16900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "model = ArxivAbstractsClassifier(config)\n",
    "trainer = Trainer(max_epochs = config['n_epochs'], \n",
    "                  gpus = 1, \n",
    "                  logger=logger, \n",
    "                  enable_checkpointing=True, \n",
    "                  callbacks=[checkpoint_callback, RichProgressBar(), EarlyStopping(monitor=\"val_loss\", mode=\"min\")])\n",
    "\n",
    "\n",
    "trainer.fit(model, datamodule=arxiv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac683e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"LSTM\"\n",
    "def save_dir_to_cloud(path, model_name):\n",
    "        for root,dirs,files in os.walk(path):\n",
    "            for file in files:\n",
    "                filepath = os.path.join(root,file)\n",
    "                save_to_cloud(f\"outs/{model_name}/{file}\", filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d412ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(f\"{model_name}.ckpt\")\n",
    "save_to_cloud(f'{model_name}.ckpt', f\"{model_name}.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a58b2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir_to_cloud(\"lightning_logs\", model_name)\n",
    "save_dir_to_cloud(\"checkpoints\", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1918c07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_file(f'snapshot-{model_name}.ckpt', f\"snapshot-{model_name}.ckpt\")\n",
    "# trainer = Trainer()\n",
    "# trainer.fit(model, datamodule=arxiv_data, ckpt_path=f\"snapshot-{model_name}.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57984dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ArxivAbstractsClassifier(config).load_from_checkpoint(f\"snapshot-{model_name}.ckpt\", config=config)\n",
    "trainer = Trainer(gpus = 1, \n",
    "                  logger=logger,\n",
    "                  callbacks=[RichProgressBar()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4960833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = trainer.predict(model, dataloaders=arxiv_data.test_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5317c10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.concatenate(preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e16b12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probably needs a more sophisticated thingy with model.eval + freezing\n",
    "# Or just incorporating that to the model above so that loading from a checkpoint will work correctly\n",
    "def predict(tresh = 0.5):\n",
    "    y_pred = [[int(val >= tresh) for val in pred] for pred in preds]\n",
    "    y_true = [tensor[\"targets\"] for tensor in arxiv_data.test_dataloader()]\n",
    "    y_true = np.concatenate([tensor.tolist() for tensor in y_true])\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"Acc score: \", accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fca4e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(tresh = 0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c45dee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(tresh = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affb0002",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(tresh = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f89b5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(tresh = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb0266b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
