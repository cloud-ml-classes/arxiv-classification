{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers rich torchmetrics sklearn pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, types\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "import dask.bag as db\n",
    "import json\n",
    "import zipfile\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import csv\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import RichProgressBar\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from torchmetrics import F1\n",
    "from torchmetrics.functional import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_json_data = \"arxiv-metadata-oai-snapshot-json.zip\"\n",
    "pickled_dataset = 'dataset_pickle.pkl'\n",
    "pickled_dataset_onehot = 'dataset_multilabel.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __iter__(self): return 0\n",
    "\n",
    "ibm_api_key_id = 'SECRET'\n",
    "bucket = 'SECRET'\n",
    "\n",
    "client_c2b4ce19d76d4b7c87bdf6c8c84d662c = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id=ibm_api_key_id,\n",
    "    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url='https://s3.private.us.cloud-object-storage.appdomain.cloud')\n",
    "\n",
    "streaming_body_1 = client_c2b4ce19d76d4b7c87bdf6c8c84d662c.get_object(Bucket=bucket, Key='arxiv-metadata-oai-snapshot.json.zip')['Body']\n",
    "\n",
    "def save_to_cloud(key, file):\n",
    "    client_c2b4ce19d76d4b7c87bdf6c8c84d662c.upload_file(Bucket=bucket, Key=f'snapshot-{file}', Filename=file)\n",
    "\n",
    "    \n",
    "def get_file(key, filename):\n",
    "    client_c2b4ce19d76d4b7c87bdf6c8c84d662c.download_file(Bucket=bucket, Key=key, Filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     get_file('arxiv-metadata-oai-snapshot.pkl', 'dataset_pickle.pkl')\n",
    "# except:\n",
    "#     print(\"Exception while pulling, will recreate from scratch\")\n",
    "    \n",
    "\n",
    "# if not os.path.exists(pickled_dataset):\n",
    "#     get_file('arxiv-metadata-oai-snapshot.json.zip', raw_json_data)\n",
    "#     with zipfile.ZipFile(raw_json_data, 'r') as zip_ref:\n",
    "#         zip_ref.extractall(\"dataset\")\n",
    "#     docs = db.read_text('./dataset/arxiv-metadata-oai-snapshot.json').map(json.loads)\n",
    "#     trim = lambda x: {'id': x['id'],\n",
    "#                   'category':x['categories'].split(' '),\n",
    "#                   'abstract':x['abstract'].replace('\\t', ' ')}\n",
    "#     docs_df = (docs.map(trim).compute())\n",
    "#     df = pd.DataFrame(docs_df)\n",
    "#     df.to_pickle(pickled_dataset)\n",
    "#     save_to_cloud('arxiv-metadata-oai-snapshot.pkl', pickled_dataset)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_pickle('dataset_pickle.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     get_file('snapshot-dataset_multilabel.pkl', 'dataset_multilabel.pkl')\n",
    "#     print(\"Got S3 file\")\n",
    "# except:\n",
    "#     print(\"Exception while pulling, will recreate from scratch\")\n",
    "    \n",
    "# if not os.path.exists('dataset_multilabel.pkl'):\n",
    "#     mlb = MultiLabelBinarizer()\n",
    "#     labels = mlb.fit_transform(df.category)\n",
    "#     df_onehot = pd.concat([df[['abstract']], pd.DataFrame(labels)], axis=1)\n",
    "#     df_onehot.columns = ['abstract'] + list(mlb.classes_)\n",
    "#     df_onehot.to_pickle('dataset_multilabel.pkl')\n",
    "#     save_to_cloud('snapshot-dataset_multilabel.pkl', 'dataset_multilabel.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_onehot = pd.read_pickle('dataset_multilabel.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABEL_COLUMNS = df_onehot.columns.tolist()[1:]\n",
    "# LABEL_COLUMNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying preprocessing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_file('arxiv-metadata-oai-snapshot.json.zip', raw_json_data)\n",
    "# with zipfile.ZipFile(raw_json_data, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(\"dataset\")\n",
    "# docs = db.read_text('./dataset/arxiv-metadata-oai-snapshot.json').map(json.loads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_latest_version = lambda x: x['versions'][-1]['created']\n",
    "# trim = lambda x: {'id': x['id'],\n",
    "#               'category':x['categories'].split(' '),\n",
    "#               'abstract':x['abstract'].replace('\\t', ' ')}\n",
    "# docs_df = (docs.filter(lambda x: int(get_latest_version(x).split(' ')[3]) > 2009).map(trim).compute())\n",
    "# df = pd.DataFrame(docs_df)\n",
    "# df.to_pickle(\"df2009.pkl\")\n",
    "# save_to_cloud('df2009.pkl', 'df2009.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_file('snapshot-df2009.pkl', 'df2009.pkl')\n",
    "# df = pd.read_pickle('df2009.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# df[\"abstract\"] = df[\"abstract\"].apply(lambda text: \" \".join([word for word in text.replace(\"\\n\", \" \").replace(\"\\t\", \" \").strip().split(\" \") if len(word) > 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequent_categories = pd.Series(np.concatenate(df['category'])).value_counts().reset_index(name=\"count\").query(\"count > 1000\")[\"index\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(frequent_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[df[\"category\"].apply(lambda cats: all(elem in frequent_categories for elem in cats))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[df[\"abstract\"].apply(lambda text: len(text.split()) > 128)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories_filtered = pd.Series(np.concatenate(df['category'])).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rich.progress import track\n",
    "\n",
    "# # Select 5k randomly, then at least 1k from each category, then drop duplicates\n",
    "# uni_df = df.sample(n = 5000)\n",
    "# for category, _ in track(categories_filtered.items(), total=len(categories_filtered), description=\"Pulling 1k from each cat\"):\n",
    "#     subset_df = df[df[\"category\"].apply(lambda cats: category in cats)]\n",
    "#     uni_df = pd.concat([subset_df.sample(n = min(1000, len(subset_df))), uni_df], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uni_df = uni_df.iloc[uni_df.astype(str).drop_duplicates().index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(uni_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uni_df = uni_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlb = MultiLabelBinarizer()\n",
    "# labels = mlb.fit_transform(uni_df.category)\n",
    "# df_onehot = pd.concat([uni_df[['abstract']], pd.DataFrame(labels)], axis=1)\n",
    "# df_onehot.columns = ['abstract'] + list(mlb.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_onehot.to_pickle('uni_df.pkl')\n",
    "# save_to_cloud('uni_df.pkl', 'uni_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_file('snapshot-cleaned_data_for_training.pkl', 'cleaned_data_for_training.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_onehot = pd.read_pickle('cleaned_data_for_training.pkl')\n",
    "LABEL_COLUMNS = df_onehot.columns.tolist()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_onehot_subs = df_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>cs.LG</th>\n",
       "      <th>quant-ph</th>\n",
       "      <th>hep-ph</th>\n",
       "      <th>hep-th</th>\n",
       "      <th>cs.CV</th>\n",
       "      <th>gr-qc</th>\n",
       "      <th>cond-mat.mes-hall</th>\n",
       "      <th>cond-mat.mtrl-sci</th>\n",
       "      <th>astro-ph.CO</th>\n",
       "      <th>...</th>\n",
       "      <th>cond-mat</th>\n",
       "      <th>alg-geom</th>\n",
       "      <th>q-alg</th>\n",
       "      <th>q-bio</th>\n",
       "      <th>dg-ga</th>\n",
       "      <th>adap-org</th>\n",
       "      <th>chao-dyn</th>\n",
       "      <th>funct-an</th>\n",
       "      <th>solv-int</th>\n",
       "      <th>patt-sol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The defining property of an artificial physica...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In this paper the certain 4-dimensional algebr...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An alternative mathematics based on qualitativ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In any cubic polynomial, the average of the sl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>For the first time, a general fractional calcu...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416676</th>\n",
       "      <td>Precision measurements of high energy top quar...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416677</th>\n",
       "      <td>Wide-field (&gt; 100 deg$^2$) hard X-ray coded-ap...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416678</th>\n",
       "      <td>Automatic evaluation of various text quality c...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416679</th>\n",
       "      <td>Ferrimagnetic TbFe or TbFeCo amorphous alloy t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416680</th>\n",
       "      <td>Double-differential three-jet production cross...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>416681 rows × 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 abstract  cs.LG  quant-ph  \\\n",
       "0       The defining property of an artificial physica...      0         0   \n",
       "1       In this paper the certain 4-dimensional algebr...      0         0   \n",
       "2       An alternative mathematics based on qualitativ...      0         0   \n",
       "3       In any cubic polynomial, the average of the sl...      0         0   \n",
       "4       For the first time, a general fractional calcu...      0         0   \n",
       "...                                                   ...    ...       ...   \n",
       "416676  Precision measurements of high energy top quar...      0         0   \n",
       "416677  Wide-field (> 100 deg$^2$) hard X-ray coded-ap...      0         0   \n",
       "416678  Automatic evaluation of various text quality c...      1         0   \n",
       "416679  Ferrimagnetic TbFe or TbFeCo amorphous alloy t...      0         0   \n",
       "416680  Double-differential three-jet production cross...      0         0   \n",
       "\n",
       "        hep-ph  hep-th  cs.CV  gr-qc  cond-mat.mes-hall  cond-mat.mtrl-sci  \\\n",
       "0            0       0      0      0                  0                  0   \n",
       "1            0       0      0      0                  0                  0   \n",
       "2            0       0      0      0                  0                  0   \n",
       "3            0       0      0      0                  0                  0   \n",
       "4            0       0      0      0                  0                  0   \n",
       "...        ...     ...    ...    ...                ...                ...   \n",
       "416676       1       0      0      0                  0                  0   \n",
       "416677       0       0      0      0                  0                  0   \n",
       "416678       0       0      0      0                  0                  0   \n",
       "416679       0       0      0      0                  0                  1   \n",
       "416680       0       0      0      0                  0                  0   \n",
       "\n",
       "        astro-ph.CO  ...  cond-mat  alg-geom  q-alg  q-bio  dg-ga  adap-org  \\\n",
       "0                 0  ...         0         0      0      0      0         0   \n",
       "1                 0  ...         0         0      0      0      0         0   \n",
       "2                 0  ...         0         0      0      0      0         0   \n",
       "3                 0  ...         0         0      0      0      0         0   \n",
       "4                 0  ...         0         0      0      0      0         0   \n",
       "...             ...  ...       ...       ...    ...    ...    ...       ...   \n",
       "416676            0  ...         0         0      0      0      0         0   \n",
       "416677            0  ...         0         0      0      0      0         0   \n",
       "416678            0  ...         0         0      0      0      0         0   \n",
       "416679            0  ...         0         0      0      0      0         0   \n",
       "416680            0  ...         0         0      0      0      0         0   \n",
       "\n",
       "        chao-dyn  funct-an  solv-int  patt-sol  \n",
       "0              0         0         0         0  \n",
       "1              0         0         0         0  \n",
       "2              0         0         0         0  \n",
       "3              0         0         0         0  \n",
       "4              0         0         0         0  \n",
       "...          ...       ...       ...       ...  \n",
       "416676         0         0         0         0  \n",
       "416677         0         0         0         0  \n",
       "416678         0         0         0         0  \n",
       "416679         0         0         0         0  \n",
       "416680         0         0         0         0  \n",
       "\n",
       "[416681 rows x 167 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_onehot_subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nowadays, application of Service Oriented Architecture is increasing rapidly; especially since introduction of distributed electronic services on the web. SOA software has a modular manner and works as a collaboration of independent software components. As a result, e-service approach is sufficient for software with independent components, each of which may be developed by a different company. Such software components and their cooperation form a composite service. Agile methodologies are the best candidate for developing small software components. Composite services and its building blocks are small pieces of software, making agile methodology a perfect fit for their development. In this paper, we introduce an agile method for service composition, inspired by agile patterns and practices. Therefore, across the agile manifesto, we can develop low cost, high quality composite services quickly using this method.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_onehot_subs['abstract'][200000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArxivAbstracts(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text = df.abstract\n",
    "        self.labels = df[df.columns.difference(['abstract'])] # <----------------- THIS SORTS LABELS!!!!!!!!!!!\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, item_idx):\n",
    "        text = self.text[item_idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask= True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "          )\n",
    "               \n",
    "        return {\n",
    "          'ids': encoding[\"input_ids\"].flatten(),\n",
    "          'mask': encoding[\"attention_mask\"].flatten(),\n",
    "          'targets': torch.tensor(self.labels.iloc[item_idx].tolist(), dtype=torch.float)  \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 1337\n",
    "df_train, df_test = train_test_split(df_onehot_subs, test_size=0.1, random_state=RANDOM_SEED, shuffle=True)\n",
    "df_train, df_validation = train_test_split(df_train, test_size=0.2, random_state=RANDOM_SEED, shuffle=True)\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "df_validation = df_validation.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "class ArxivAbstractsModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, df_train, df_test, df_validation, tokenizer, batch_size=16, max_token_len=256):\n",
    "        super().__init__()\n",
    "        self.df_train = df_train\n",
    "        self.df_test = df_test\n",
    "        self.df_validation = df_validation\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = ArxivAbstracts(df=self.df_train, tokenizer=self.tokenizer, max_len= self.max_token_len)\n",
    "        self.val_dataset = ArxivAbstracts(df=self.df_validation, tokenizer=self.tokenizer, max_len= self.max_token_len)\n",
    "        self.test_dataset = ArxivAbstracts(df=self.df_test, tokenizer=self.tokenizer, max_len= self.max_token_len)\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "arxiv_data = ArxivAbstractsModule(df_train, df_test, df_validation, tokenizer, max_token_len=384)\n",
    "arxiv_data.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "class ArxivAbstractsClassifier(pl.LightningModule):\n",
    "    def __init__(self, n_classes, lr=2e-5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-cased\", return_dict=True)\n",
    "        self.classifier = torch.nn.Linear(self.bert.config.hidden_size, n_classes) \n",
    "        self.lr = lr\n",
    "        self.criterion = torch.nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        self.f1_metrics = torch.nn.ModuleDict({\n",
    "            'f1_weighted': F1(num_classes=len(LABEL_COLUMNS), average='weighted', threshold=0.5),       \n",
    "            'f1_samples': F1(num_classes=len(LABEL_COLUMNS), average='samples', threshold=0.5), \n",
    "            'f1_micro': F1(num_classes=len(LABEL_COLUMNS), average='micro', threshold=0.5),\n",
    "            'f1_macro': F1(num_classes=len(LABEL_COLUMNS), average='macro', threshold=0.5),\n",
    "            'f1_weighted_t02': F1(num_classes=len(LABEL_COLUMNS), average='weighted', threshold=0.2),       \n",
    "            'f1_samples_t02': F1(num_classes=len(LABEL_COLUMNS), average='samples', threshold=0.2), \n",
    "            'f1_micro_t02': F1(num_classes=len(LABEL_COLUMNS), average='micro', threshold=0.2),\n",
    "            'f1_macro_t02': F1(num_classes=len(LABEL_COLUMNS), average='macro', threshold=0.2),\n",
    "        })\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def compute_metrics(self, outputs, labels):\n",
    "        metrics = {}\n",
    "        for name, metric in self.f1_metrics.items():\n",
    "            metrics[name] = metric(outputs, labels)\n",
    "        return metrics  \n",
    "    \n",
    "    def forward(self, ids, mask):\n",
    "        output = self.bert(input_ids=ids, attention_mask=mask)\n",
    "        output = self.classifier(output.pooler_output)   \n",
    "        return output\n",
    "    \n",
    "    def loss(self, batch, prediction):\n",
    "        return self.criterion(prediction, batch[\"targets\"])\n",
    "    \n",
    "    def __generic_step(self, batch, batch_idx, namespace: str):\n",
    "        input_ids = batch[\"ids\"]\n",
    "        attention_mask = batch[\"mask\"]\n",
    "        labels = batch[\"targets\"]\n",
    "        \n",
    "        outputs = self(input_ids, attention_mask)\n",
    "        predictions = torch.sigmoid(outputs)\n",
    "        loss = self.loss(batch, outputs)\n",
    "        metrics = self.compute_metrics(predictions, labels.int())\n",
    "        \n",
    "        if namespace != \"pred\":\n",
    "            self.log_dict({f'{namespace}_loss': loss, **\n",
    "                           {f'{namespace}_{k}': v for k, v in metrics.items()}})\n",
    "        \n",
    "        return {\"loss\": loss, \"predictions\": predictions, \"labels\": labels}\n",
    "        \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        return self.__generic_step(batch, batch_idx, namespace=\"pred\")[\"predictions\"]\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.__generic_step(batch, batch_idx, namespace=\"train\")[\"loss\"]\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.__generic_step(batch, batch_idx, namespace=\"val\")[\"loss\"]\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.__generic_step(batch, batch_idx, namespace=\"test\")[\"loss\"]\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(params=self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': scheduler,\n",
    "            'monitor': 'val_loss'\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    monitor='val_loss',\n",
    "    filename='ArxivAbstractsClassifierPreprocess-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=2,\n",
    "    verbose=True,\n",
    "    mode='min'\n",
    ")\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"arxiv-abstracts-logs\")\n",
    "# early_stopping_callback = EarlyStopping(monitor='val_loss', patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Epoch 0   </span> <span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">33/23439</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:01:08 • 13:10:23</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">0.49it/s</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">loss: 0.511 v_num: 1 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37mEpoch 0   \u001b[0m \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m33/23439\u001b[0m \u001b[38;5;245m0:01:08 • 13:10:23\u001b[0m \u001b[38;5;249m0.49it/s\u001b[0m \u001b[37mloss: 0.511 v_num: 1 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer\n",
       ".py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
       "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer\n",
       ".py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
       "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 6s, sys: 18.2 s, total: 1min 24s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "N_EPOCHS = 4\n",
    "model = ArxivAbstractsClassifier(n_classes=len(LABEL_COLUMNS))\n",
    "trainer = Trainer(max_epochs = N_EPOCHS , \n",
    "                  gpus = 1, \n",
    "                  logger=logger, \n",
    "                  enable_checkpointing=True,\n",
    "#                   callbacks=[early_stopping_callback], \n",
    "                  callbacks=[checkpoint_callback, RichProgressBar()])\n",
    "\n",
    "\n",
    "trainer.fit(model, datamodule=arxiv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"abrract_class_bert_cleaned_v1\"\n",
    "def save_dir_to_cloud(path, model_name):\n",
    "        for root,dirs,files in os.walk(path):\n",
    "            for file in files:\n",
    "                filepath = os.path.join(root,file)\n",
    "                save_to_cloud(f\"outs/{model_name}/{file}\", filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(f\"{model_name}.ckpt\")\n",
    "save_to_cloud(f'{model_name}.ckpt', f\"{model_name}.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir_to_cloud(\"lightning_logs\", model_name)\n",
    "save_dir_to_cloud(\"checkpoints\", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_file(f'snapshot-{model_name}.ckpt', f\"snapshot-{model_name}.ckpt\")\n",
    "# trainer = Trainer()\n",
    "# trainer.fit(model, datamodule=arxiv_data, ckpt_path=f\"snapshot-{model_name}.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = ArxivAbstractsClassifier(n_classes=len(LABEL_COLUMNS)).load_from_checkpoint(f\"snapshot-{model_name}.ckpt\", n_classes=len(LABEL_COLUMNS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(gpus = 1, \n",
    "                  logger=logger,\n",
    "                  callbacks=[RichProgressBar()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.test(model, datamodule=arxiv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Predicting</span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">2605/2605</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">1:27:17 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">0.50it/s</span>  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37mPredicting\u001b[0m \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m2605/2605\u001b[0m \u001b[38;5;245m1:27:17 • 0:00:00\u001b[0m \u001b[38;5;249m0.50it/s\u001b[0m  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = trainer.predict(model, dataloaders=arxiv_data.test_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.concatenate(preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probably needs a more sophisticated thingy with model.eval + freezing\n",
    "# Or just incorporating that to the model above so that loading from a checkpoint will work correctly\n",
    "def predict(tresh = 0.5):\n",
    "    y_pred = [[int(val >= tresh) for val in pred] for pred in preds]\n",
    "    y_true = [tensor[\"targets\"] for tensor in arxiv_data.test_dataloader()]\n",
    "    y_true = np.concatenate([tensor.tolist() for tensor in y_true])\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"Acc score: \", accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.64      0.82      0.72      1018\n",
      "           4       0.79      0.84      0.82       806\n",
      "           5       0.59      0.86      0.70       883\n",
      "           6       0.65      0.84      0.73       944\n",
      "           7       0.67      0.70      0.68       815\n",
      "           8       0.70      0.81      0.75      1136\n",
      "           9       0.00      0.00      0.00         0\n",
      "          10       0.00      0.00      0.00         0\n",
      "          11       0.46      0.67      0.54       589\n",
      "          12       0.55      0.76      0.64      1172\n",
      "          13       0.63      0.62      0.63      1264\n",
      "          14       0.29      0.12      0.17       343\n",
      "          15       0.73      0.81      0.77       660\n",
      "          16       0.58      0.84      0.69       885\n",
      "          17       0.46      0.71      0.56      1284\n",
      "          18       0.58      0.82      0.68      1020\n",
      "          19       0.76      0.86      0.81       624\n",
      "          20       0.43      0.48      0.45      1728\n",
      "          21       0.60      0.66      0.63       223\n",
      "          22       0.65      0.67      0.66       488\n",
      "          23       0.40      0.26      0.31       345\n",
      "          24       0.56      0.70      0.62       300\n",
      "          25       0.76      0.83      0.79      1031\n",
      "          26       0.68      0.85      0.75       847\n",
      "          27       0.70      0.86      0.77      1841\n",
      "          28       0.46      0.57      0.51       631\n",
      "          29       0.69      0.69      0.69       449\n",
      "          30       0.57      0.61      0.59       758\n",
      "          31       0.76      0.71      0.74       235\n",
      "          32       0.38      0.61      0.47       489\n",
      "          33       0.56      0.70      0.62       827\n",
      "          34       0.59      0.55      0.57       190\n",
      "          35       0.63      0.61      0.62       193\n",
      "          36       0.00      0.00      0.00         0\n",
      "          37       0.57      0.65      0.61       277\n",
      "          38       0.68      0.85      0.75       591\n",
      "          39       0.55      0.65      0.60       597\n",
      "          40       0.58      0.68      0.62       636\n",
      "          41       0.55      0.85      0.66      1368\n",
      "          42       0.59      0.79      0.68      3988\n",
      "          43       0.57      0.81      0.67       500\n",
      "          44       0.43      0.60      0.50       359\n",
      "          45       0.44      0.60      0.51       300\n",
      "          46       0.55      0.41      0.47        94\n",
      "          47       0.48      0.65      0.55       696\n",
      "          48       0.58      0.47      0.52       621\n",
      "          49       0.57      0.74      0.64       737\n",
      "          50       0.20      0.01      0.02        87\n",
      "          51       0.00      0.00      0.00         0\n",
      "          52       0.33      0.30      0.31       192\n",
      "          53       0.62      0.72      0.67       333\n",
      "          54       0.70      0.87      0.78       801\n",
      "          55       0.56      0.42      0.48        76\n",
      "          56       0.84      0.94      0.89       600\n",
      "          57       0.81      0.72      0.76       575\n",
      "          58       0.57      0.76      0.66       871\n",
      "          59       0.58      0.63      0.60      1012\n",
      "          60       0.00      0.00      0.00         0\n",
      "          61       0.49      0.54      0.51       130\n",
      "          62       0.41      0.46      0.44       151\n",
      "          63       0.23      0.17      0.19        78\n",
      "          64       0.84      0.94      0.89       588\n",
      "          65       0.56      0.75      0.64       711\n",
      "          66       0.40      0.53      0.45       787\n",
      "          67       0.49      0.56      0.52       721\n",
      "          68       0.00      0.00      0.00         0\n",
      "          69       0.69      0.85      0.76      1163\n",
      "          70       0.64      0.85      0.73       910\n",
      "          71       0.67      0.85      0.75       511\n",
      "          72       0.68      0.87      0.77      1552\n",
      "          73       0.64      0.81      0.72      1492\n",
      "          74       0.56      0.59      0.58      1451\n",
      "          75       0.65      0.53      0.59       184\n",
      "          76       0.72      0.71      0.72       600\n",
      "          77       0.63      0.76      0.69       727\n",
      "          78       0.54      0.70      0.61       267\n",
      "          79       0.53      0.47      0.50       330\n",
      "          80       0.54      0.80      0.65       974\n",
      "          81       0.52      0.75      0.61       177\n",
      "          82       0.55      0.52      0.53       206\n",
      "          83       0.65      0.74      0.69       484\n",
      "          84       0.45      0.60      0.52       724\n",
      "          85       0.49      0.67      0.56       480\n",
      "          86       0.43      0.07      0.12        43\n",
      "          87       0.48      0.55      0.51        78\n",
      "          88       0.64      0.75      0.69       325\n",
      "          89       0.55      0.73      0.63       278\n",
      "          90       0.54      0.43      0.48        63\n",
      "          91       0.55      0.85      0.67      1368\n",
      "          92       0.33      0.51      0.40        82\n",
      "          93       0.62      0.74      0.67       264\n",
      "          94       0.38      0.53      0.44       193\n",
      "          95       0.56      0.60      0.58      1451\n",
      "          96       0.63      0.72      0.67       956\n",
      "          97       0.75      0.76      0.76       553\n",
      "          98       0.58      0.76      0.66       201\n",
      "          99       0.55      0.64      0.59      1098\n",
      "         100       0.58      0.70      0.63       938\n",
      "         101       0.51      0.59      0.55       188\n",
      "         102       0.42      0.62      0.50       224\n",
      "         103       0.55      0.77      0.64       406\n",
      "         104       0.55      0.74      0.63       135\n",
      "         105       0.50      0.56      0.53       176\n",
      "         106       0.54      0.74      0.63       745\n",
      "         107       0.39      0.44      0.41       279\n",
      "         108       0.51      0.64      0.57       426\n",
      "         109       0.00      0.00      0.00         0\n",
      "         110       0.49      0.61      0.54       275\n",
      "         111       0.59      0.52      0.56       162\n",
      "         112       0.67      0.76      0.71       623\n",
      "         113       0.73      0.78      0.75       837\n",
      "         114       0.00      0.00      0.00         0\n",
      "         115       0.78      0.77      0.77       251\n",
      "         116       0.71      0.67      0.69       308\n",
      "         117       0.47      0.51      0.49       601\n",
      "         118       0.49      0.20      0.28        91\n",
      "         119       0.67      0.75      0.70       578\n",
      "         120       0.50      0.72      0.59       756\n",
      "         121       0.64      0.55      0.59       675\n",
      "         122       0.38      0.46      0.41       270\n",
      "         123       0.45      0.42      0.44       762\n",
      "         124       0.39      0.30      0.34       407\n",
      "         125       0.94      0.72      0.82       129\n",
      "         126       0.67      0.77      0.72       854\n",
      "         127       0.44      0.51      0.47       240\n",
      "         128       0.58      0.62      0.60       373\n",
      "         129       0.61      0.69      0.65       139\n",
      "         130       0.67      0.69      0.68       663\n",
      "         131       0.62      0.75      0.68       366\n",
      "         132       0.63      0.78      0.70       934\n",
      "         133       0.72      0.82      0.76       595\n",
      "         134       0.36      0.25      0.30        71\n",
      "         135       0.60      0.76      0.67       877\n",
      "         136       0.66      0.76      0.71       378\n",
      "         137       0.00      0.00      0.00         0\n",
      "         138       0.00      0.00      0.00         0\n",
      "         139       0.55      0.68      0.61       235\n",
      "         140       0.37      0.49      0.42        90\n",
      "         141       0.62      0.63      0.63       179\n",
      "         142       0.65      0.62      0.63       216\n",
      "         143       0.69      0.81      0.75       521\n",
      "         144       0.00      0.00      0.00         0\n",
      "         145       0.65      0.77      0.70       547\n",
      "         146       0.36      0.35      0.35       524\n",
      "         147       0.63      0.26      0.37       100\n",
      "         148       0.50      0.43      0.46       120\n",
      "         149       0.51      0.35      0.41       101\n",
      "         150       0.43      0.46      0.45       188\n",
      "         151       0.43      0.47      0.45       116\n",
      "         152       0.41      0.47      0.43        92\n",
      "         153       0.52      0.48      0.50        52\n",
      "         154       0.41      0.66      0.51        59\n",
      "         155       0.63      0.63      0.63        91\n",
      "         156       0.50      0.68      0.58       151\n",
      "         157       0.59      0.72      0.65        81\n",
      "         158       0.71      0.78      0.75      1904\n",
      "         159       0.00      0.00      0.00         0\n",
      "         160       0.49      0.39      0.44       742\n",
      "         161       0.40      0.60      0.48       400\n",
      "         162       0.54      0.75      0.63       817\n",
      "         163       0.45      0.60      0.51      1818\n",
      "         164       0.88      0.11      0.20        63\n",
      "         165       0.55      0.74      0.63       745\n",
      "\n",
      "   micro avg       0.59      0.70      0.64     87364\n",
      "   macro avg       0.51      0.58      0.54     87364\n",
      "weighted avg       0.59      0.70      0.64     87364\n",
      " samples avg       0.64      0.75      0.65     87364\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc score:  0.22659531066260288\n"
     ]
    }
   ],
   "source": [
    "predict(tresh = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.68      0.76      0.72      1018\n",
      "           4       0.82      0.82      0.82       806\n",
      "           5       0.63      0.84      0.72       883\n",
      "           6       0.68      0.82      0.74       944\n",
      "           7       0.72      0.67      0.69       815\n",
      "           8       0.73      0.76      0.75      1136\n",
      "           9       0.00      0.00      0.00         0\n",
      "          10       0.00      0.00      0.00         0\n",
      "          11       0.51      0.61      0.56       589\n",
      "          12       0.60      0.70      0.64      1172\n",
      "          13       0.68      0.55      0.61      1264\n",
      "          14       0.38      0.09      0.14       343\n",
      "          15       0.79      0.77      0.78       660\n",
      "          16       0.62      0.82      0.71       885\n",
      "          17       0.51      0.64      0.56      1284\n",
      "          18       0.62      0.77      0.68      1020\n",
      "          19       0.79      0.83      0.81       624\n",
      "          20       0.51      0.40      0.45      1728\n",
      "          21       0.64      0.63      0.63       223\n",
      "          22       0.70      0.60      0.64       488\n",
      "          23       0.47      0.21      0.29       345\n",
      "          24       0.59      0.65      0.62       300\n",
      "          25       0.77      0.79      0.78      1031\n",
      "          26       0.70      0.82      0.76       847\n",
      "          27       0.73      0.83      0.78      1841\n",
      "          28       0.49      0.50      0.50       631\n",
      "          29       0.73      0.67      0.70       449\n",
      "          30       0.63      0.56      0.59       758\n",
      "          31       0.79      0.69      0.74       235\n",
      "          32       0.43      0.54      0.48       489\n",
      "          33       0.60      0.66      0.63       827\n",
      "          34       0.65      0.51      0.57       190\n",
      "          35       0.67      0.53      0.59       193\n",
      "          36       0.00      0.00      0.00         0\n",
      "          37       0.61      0.62      0.62       277\n",
      "          38       0.71      0.83      0.76       591\n",
      "          39       0.60      0.62      0.61       597\n",
      "          40       0.65      0.63      0.64       636\n",
      "          41       0.58      0.83      0.68      1368\n",
      "          42       0.64      0.73      0.68      3988\n",
      "          43       0.59      0.76      0.66       500\n",
      "          44       0.49      0.56      0.52       359\n",
      "          45       0.50      0.55      0.53       300\n",
      "          46       0.64      0.37      0.47        94\n",
      "          47       0.52      0.60      0.56       696\n",
      "          48       0.65      0.42      0.51       621\n",
      "          49       0.60      0.67      0.64       737\n",
      "          50       1.00      0.01      0.02        87\n",
      "          51       0.00      0.00      0.00         0\n",
      "          52       0.35      0.22      0.27       192\n",
      "          53       0.65      0.68      0.66       333\n",
      "          54       0.74      0.85      0.79       801\n",
      "          55       0.57      0.37      0.45        76\n",
      "          56       0.85      0.93      0.89       600\n",
      "          57       0.83      0.69      0.75       575\n",
      "          58       0.61      0.72      0.66       871\n",
      "          59       0.61      0.59      0.60      1012\n",
      "          60       0.00      0.00      0.00         0\n",
      "          61       0.54      0.47      0.50       130\n",
      "          62       0.46      0.42      0.44       151\n",
      "          63       0.35      0.12      0.17        78\n",
      "          64       0.84      0.93      0.88       588\n",
      "          65       0.61      0.71      0.65       711\n",
      "          66       0.45      0.44      0.44       787\n",
      "          67       0.52      0.50      0.51       721\n",
      "          68       0.00      0.00      0.00         0\n",
      "          69       0.73      0.81      0.77      1163\n",
      "          70       0.67      0.79      0.73       910\n",
      "          71       0.71      0.82      0.76       511\n",
      "          72       0.72      0.85      0.78      1552\n",
      "          73       0.69      0.78      0.73      1492\n",
      "          74       0.60      0.54      0.57      1451\n",
      "          75       0.73      0.51      0.60       184\n",
      "          76       0.75      0.65      0.70       600\n",
      "          77       0.68      0.72      0.70       727\n",
      "          78       0.58      0.67      0.62       267\n",
      "          79       0.58      0.38      0.46       330\n",
      "          80       0.59      0.76      0.66       974\n",
      "          81       0.57      0.70      0.63       177\n",
      "          82       0.58      0.46      0.51       206\n",
      "          83       0.69      0.71      0.70       484\n",
      "          84       0.51      0.56      0.53       724\n",
      "          85       0.54      0.63      0.58       480\n",
      "          86       0.00      0.00      0.00        43\n",
      "          87       0.53      0.49      0.51        78\n",
      "          88       0.67      0.70      0.68       325\n",
      "          89       0.61      0.70      0.65       278\n",
      "          90       0.63      0.41      0.50        63\n",
      "          91       0.58      0.83      0.69      1368\n",
      "          92       0.39      0.44      0.41        82\n",
      "          93       0.68      0.71      0.69       264\n",
      "          94       0.42      0.48      0.45       193\n",
      "          95       0.60      0.55      0.57      1451\n",
      "          96       0.67      0.68      0.67       956\n",
      "          97       0.78      0.73      0.75       553\n",
      "          98       0.64      0.72      0.68       201\n",
      "          99       0.60      0.58      0.59      1098\n",
      "         100       0.64      0.65      0.64       938\n",
      "         101       0.56      0.54      0.55       188\n",
      "         102       0.46      0.56      0.50       224\n",
      "         103       0.60      0.73      0.66       406\n",
      "         104       0.63      0.70      0.66       135\n",
      "         105       0.53      0.49      0.51       176\n",
      "         106       0.59      0.69      0.64       745\n",
      "         107       0.46      0.38      0.42       279\n",
      "         108       0.55      0.58      0.56       426\n",
      "         109       0.00      0.00      0.00         0\n",
      "         110       0.55      0.56      0.55       275\n",
      "         111       0.62      0.48      0.54       162\n",
      "         112       0.70      0.69      0.69       623\n",
      "         113       0.76      0.74      0.75       837\n",
      "         114       0.00      0.00      0.00         0\n",
      "         115       0.80      0.75      0.78       251\n",
      "         116       0.74      0.64      0.68       308\n",
      "         117       0.52      0.44      0.48       601\n",
      "         118       0.57      0.09      0.15        91\n",
      "         119       0.71      0.70      0.71       578\n",
      "         120       0.54      0.65      0.59       756\n",
      "         121       0.67      0.49      0.56       675\n",
      "         122       0.45      0.40      0.42       270\n",
      "         123       0.47      0.33      0.39       762\n",
      "         124       0.45      0.23      0.30       407\n",
      "         125       0.96      0.71      0.81       129\n",
      "         126       0.70      0.74      0.72       854\n",
      "         127       0.48      0.43      0.45       240\n",
      "         128       0.63      0.58      0.61       373\n",
      "         129       0.68      0.65      0.66       139\n",
      "         130       0.70      0.64      0.67       663\n",
      "         131       0.67      0.71      0.69       366\n",
      "         132       0.68      0.75      0.71       934\n",
      "         133       0.75      0.80      0.77       595\n",
      "         134       0.52      0.24      0.33        71\n",
      "         135       0.63      0.70      0.66       877\n",
      "         136       0.70      0.73      0.71       378\n",
      "         137       0.00      0.00      0.00         0\n",
      "         138       0.00      0.00      0.00         0\n",
      "         139       0.58      0.61      0.60       235\n",
      "         140       0.43      0.37      0.40        90\n",
      "         141       0.67      0.58      0.62       179\n",
      "         142       0.67      0.58      0.63       216\n",
      "         143       0.72      0.78      0.75       521\n",
      "         144       0.00      0.00      0.00         0\n",
      "         145       0.68      0.74      0.71       547\n",
      "         146       0.40      0.27      0.32       524\n",
      "         147       0.72      0.18      0.29       100\n",
      "         148       0.54      0.37      0.44       120\n",
      "         149       0.57      0.28      0.37       101\n",
      "         150       0.48      0.44      0.46       188\n",
      "         151       0.54      0.41      0.47       116\n",
      "         152       0.46      0.34      0.39        92\n",
      "         153       0.55      0.40      0.47        52\n",
      "         154       0.47      0.64      0.54        59\n",
      "         155       0.66      0.49      0.57        91\n",
      "         156       0.54      0.62      0.58       151\n",
      "         157       0.68      0.62      0.65        81\n",
      "         158       0.76      0.74      0.75      1904\n",
      "         159       0.00      0.00      0.00         0\n",
      "         160       0.52      0.30      0.38       742\n",
      "         161       0.44      0.54      0.48       400\n",
      "         162       0.58      0.69      0.63       817\n",
      "         163       0.49      0.52      0.51      1818\n",
      "         164       1.00      0.05      0.09        63\n",
      "         165       0.58      0.69      0.63       745\n",
      "\n",
      "   micro avg       0.63      0.66      0.64     87364\n",
      "   macro avg       0.56      0.53      0.53     87364\n",
      "weighted avg       0.63      0.66      0.64     87364\n",
      " samples avg       0.67      0.71      0.65     87364\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc score:  0.25872951114737575\n"
     ]
    }
   ],
   "source": [
    "predict(tresh = 0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.00      0.00      0.00         3\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.34      0.37      0.36        84\n",
      "           5       0.00      0.00      0.00         0\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.00      0.00      0.00         0\n",
      "           8       0.00      0.00      0.00         0\n",
      "           9       0.00      0.00      0.00         1\n",
      "          10       0.00      0.00      0.00         0\n",
      "          11       0.00      0.00      0.00         1\n",
      "          12       0.00      0.00      0.00         0\n",
      "          13       0.40      0.09      0.15        22\n",
      "          14       0.00      0.00      0.00         3\n",
      "          15       0.00      0.00      0.00         0\n",
      "          16       0.00      0.00      0.00         2\n",
      "          17       0.19      0.10      0.13        80\n",
      "          18       0.11      0.07      0.09        27\n",
      "          19       0.11      0.21      0.14        34\n",
      "          20       0.34      0.29      0.31        42\n",
      "          21       0.17      0.02      0.04        48\n",
      "          22       0.00      0.00      0.00         1\n",
      "          23       0.26      0.17      0.20        42\n",
      "          24       0.15      0.23      0.18       122\n",
      "          25       0.00      0.00      0.00        15\n",
      "          26       0.37      0.50      0.42        14\n",
      "          27       0.00      0.00      0.00         6\n",
      "          28       0.00      0.00      0.00         0\n",
      "          29       0.00      0.00      0.00        17\n",
      "          30       0.00      0.00      0.00         2\n",
      "          31       0.00      0.00      0.00         2\n",
      "          32       0.00      0.00      0.00         0\n",
      "          33       0.00      0.00      0.00        13\n",
      "          34       0.00      0.00      0.00         0\n",
      "          35       0.00      0.00      0.00         0\n",
      "          36       0.00      0.00      0.00         0\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         1\n",
      "          39       0.00      0.00      0.00        10\n",
      "          40       0.00      0.00      0.00         4\n",
      "          41       0.00      0.00      0.00         0\n",
      "          42       0.00      0.00      0.00         0\n",
      "          43       0.00      0.00      0.00         0\n",
      "          44       0.00      0.00      0.00         1\n",
      "          45       0.00      0.00      0.00         3\n",
      "          46       0.00      0.00      0.00         0\n",
      "          47       0.00      0.00      0.00         1\n",
      "          48       1.00      0.08      0.15        12\n",
      "          49       0.00      0.00      0.00         2\n",
      "          50       0.00      0.00      0.00         4\n",
      "          51       0.00      0.00      0.00         2\n",
      "          52       0.00      0.00      0.00         0\n",
      "          53       0.00      0.00      0.00         0\n",
      "          54       0.00      0.00      0.00         1\n",
      "          55       0.00      0.00      0.00         3\n",
      "          56       0.00      0.00      0.00         0\n",
      "          57       0.00      0.00      0.00         0\n",
      "          58       0.00      0.00      0.00         0\n",
      "          59       0.00      0.00      0.00         0\n",
      "          60       0.00      0.00      0.00         2\n",
      "          61       0.00      0.00      0.00         0\n",
      "          62       0.00      0.00      0.00         2\n",
      "          63       0.00      0.00      0.00         1\n",
      "          64       0.00      0.00      0.00         0\n",
      "          65       0.00      0.00      0.00         0\n",
      "          66       0.00      0.00      0.00         0\n",
      "          67       0.00      0.00      0.00         0\n",
      "          68       0.00      0.00      0.00         0\n",
      "          69       0.00      0.00      0.00         0\n",
      "          70       0.00      0.00      0.00         0\n",
      "          71       0.00      0.00      0.00         0\n",
      "          72       0.00      0.00      0.00         0\n",
      "          73       0.00      0.00      0.00         0\n",
      "          74       0.00      0.00      0.00         0\n",
      "          75       0.00      0.00      0.00         5\n",
      "          76       0.18      0.35      0.24        81\n",
      "          77       0.16      0.16      0.16        44\n",
      "          78       0.29      0.11      0.15        19\n",
      "          79       0.31      0.68      0.42       273\n",
      "          80       0.21      0.52      0.30       244\n",
      "          81       0.25      0.41      0.32       374\n",
      "          82       0.72      0.58      0.64       139\n",
      "          83       0.77      0.82      0.79       628\n",
      "          84       0.59      0.74      0.66       245\n",
      "          85       0.46      0.73      0.57       143\n",
      "          86       0.40      0.51      0.45       158\n",
      "          87       0.70      0.72      0.71       418\n",
      "          88       0.44      0.63      0.52        59\n",
      "          89       0.71      0.65      0.68       165\n",
      "          90       0.53      0.82      0.64       436\n",
      "          91       0.65      0.63      0.64       213\n",
      "          92       0.39      0.69      0.50       195\n",
      "          93       0.24      0.40      0.30        53\n",
      "          94       0.47      0.59      0.52        41\n",
      "          95       0.58      0.77      0.67       206\n",
      "          96       0.58      0.83      0.68       306\n",
      "          97       0.26      0.38      0.31        21\n",
      "          98       0.00      0.00      0.00        12\n",
      "          99       0.42      0.51      0.46        70\n",
      "         100       0.66      0.74      0.69       117\n",
      "         101       0.41      0.43      0.42        89\n",
      "         102       0.26      0.41      0.32       374\n",
      "         103       0.53      0.36      0.43        56\n",
      "         104       0.76      0.76      0.76       321\n",
      "         105       0.62      0.74      0.67       132\n",
      "         106       0.38      0.36      0.37        64\n",
      "         107       0.75      0.80      0.77       347\n",
      "         108       0.62      0.83      0.71       374\n",
      "         109       0.46      0.52      0.49       147\n",
      "         110       0.51      0.59      0.55       219\n",
      "         111       0.53      0.67      0.59       129\n",
      "         112       0.36      0.61      0.45        79\n",
      "         113       0.71      0.62      0.66       112\n",
      "         114       0.50      0.79      0.61        14\n",
      "         115       0.19      0.29      0.23        65\n",
      "         116       0.55      0.77      0.64       299\n",
      "         117       1.00      0.14      0.24        22\n",
      "         118       0.60      0.68      0.64       219\n",
      "         119       0.62      0.76      0.68       255\n",
      "         120       0.69      0.75      0.72       398\n",
      "         121       0.90      0.94      0.92      1273\n",
      "         122       0.26      0.35      0.30        48\n",
      "         123       0.80      0.79      0.80        86\n",
      "         124       0.48      0.70      0.57        46\n",
      "         125       0.00      0.00      0.00         0\n",
      "         126       0.36      0.60      0.45        43\n",
      "         127       0.57      0.68      0.62       236\n",
      "         128       0.41      0.58      0.48       139\n",
      "         129       0.49      0.61      0.54       155\n",
      "         130       0.33      0.40      0.36       131\n",
      "         131       0.44      0.16      0.24       142\n",
      "         132       0.37      0.58      0.45       104\n",
      "         133       0.45      0.56      0.50        71\n",
      "         134       0.62      0.68      0.65       137\n",
      "         135       0.48      0.66      0.56       284\n",
      "         136       0.67      0.67      0.67        39\n",
      "         137       0.27      0.67      0.38        33\n",
      "         138       0.62      0.75      0.68       126\n",
      "         139       0.41      0.59      0.49        32\n",
      "         140       0.60      0.68      0.64       259\n",
      "         141       0.72      0.81      0.76       122\n",
      "         142       0.15      0.18      0.17        22\n",
      "         143       0.56      0.85      0.67       109\n",
      "         144       0.42      0.42      0.42        31\n",
      "         145       0.00      0.00      0.00         4\n",
      "         146       0.35      0.75      0.48       100\n",
      "         147       0.06      0.07      0.07        40\n",
      "         148       0.54      0.79      0.64        58\n",
      "         149       0.54      0.33      0.41        21\n",
      "         150       0.51      0.80      0.62        25\n",
      "         151       0.50      0.59      0.54        29\n",
      "         152       0.60      0.86      0.70        36\n",
      "         153       0.00      0.00      0.00        17\n",
      "         154       0.57      0.62      0.59        53\n",
      "         155       0.18      0.27      0.22        56\n",
      "         156       0.00      0.00      0.00        11\n",
      "         157       0.25      0.08      0.12        13\n",
      "         158       0.40      0.33      0.36         6\n",
      "         159       0.00      0.00      0.00         0\n",
      "         160       1.00      0.09      0.17        11\n",
      "         161       0.00      0.00      0.00         0\n",
      "         162       0.00      0.00      0.00         1\n",
      "         163       0.00      0.00      0.00         9\n",
      "         164       0.00      0.00      0.00         2\n",
      "         165       0.28      0.93      0.43        14\n",
      "         166       0.00      0.00      0.00         7\n",
      "         167       0.83      0.95      0.89      2079\n",
      "         168       0.29      0.49      0.36        79\n",
      "         169       0.00      0.00      0.00         0\n",
      "         170       0.00      0.00      0.00         1\n",
      "         171       0.00      0.00      0.00         3\n",
      "         172       0.00      0.00      0.00         1\n",
      "         173       0.00      0.00      0.00         0\n",
      "         174       0.69      0.62      0.65       112\n",
      "         175       0.50      0.57      0.53         7\n",
      "\n",
      "   micro avg       0.56      0.69      0.62     15481\n",
      "   macro avg       0.26      0.29      0.26     15481\n",
      "weighted avg       0.59      0.69      0.63     15481\n",
      " samples avg       0.68      0.77      0.68     15481\n",
      "\n",
      "Acc score:  0.3878\n"
     ]
    }
   ],
   "source": [
    "predict(tresh = 0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.00      0.00      0.00         3\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.58      0.13      0.21        84\n",
      "           5       0.00      0.00      0.00         0\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.00      0.00      0.00         0\n",
      "           8       0.00      0.00      0.00         0\n",
      "           9       0.00      0.00      0.00         1\n",
      "          10       0.00      0.00      0.00         0\n",
      "          11       0.00      0.00      0.00         1\n",
      "          12       0.00      0.00      0.00         0\n",
      "          13       1.00      0.09      0.17        22\n",
      "          14       0.00      0.00      0.00         3\n",
      "          15       0.00      0.00      0.00         0\n",
      "          16       0.00      0.00      0.00         2\n",
      "          17       0.00      0.00      0.00        80\n",
      "          18       0.00      0.00      0.00        27\n",
      "          19       0.00      0.00      0.00        34\n",
      "          20       0.53      0.19      0.28        42\n",
      "          21       0.00      0.00      0.00        48\n",
      "          22       0.00      0.00      0.00         1\n",
      "          23       0.00      0.00      0.00        42\n",
      "          24       0.25      0.02      0.04       122\n",
      "          25       0.00      0.00      0.00        15\n",
      "          26       0.57      0.29      0.38        14\n",
      "          27       0.00      0.00      0.00         6\n",
      "          28       0.00      0.00      0.00         0\n",
      "          29       0.00      0.00      0.00        17\n",
      "          30       0.00      0.00      0.00         2\n",
      "          31       0.00      0.00      0.00         2\n",
      "          32       0.00      0.00      0.00         0\n",
      "          33       0.00      0.00      0.00        13\n",
      "          34       0.00      0.00      0.00         0\n",
      "          35       0.00      0.00      0.00         0\n",
      "          36       0.00      0.00      0.00         0\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       0.00      0.00      0.00         1\n",
      "          39       0.00      0.00      0.00        10\n",
      "          40       0.00      0.00      0.00         4\n",
      "          41       0.00      0.00      0.00         0\n",
      "          42       0.00      0.00      0.00         0\n",
      "          43       0.00      0.00      0.00         0\n",
      "          44       0.00      0.00      0.00         1\n",
      "          45       0.00      0.00      0.00         3\n",
      "          46       0.00      0.00      0.00         0\n",
      "          47       0.00      0.00      0.00         1\n",
      "          48       0.00      0.00      0.00        12\n",
      "          49       0.00      0.00      0.00         2\n",
      "          50       0.00      0.00      0.00         4\n",
      "          51       0.00      0.00      0.00         2\n",
      "          52       0.00      0.00      0.00         0\n",
      "          53       0.00      0.00      0.00         0\n",
      "          54       0.00      0.00      0.00         1\n",
      "          55       0.00      0.00      0.00         3\n",
      "          56       0.00      0.00      0.00         0\n",
      "          57       0.00      0.00      0.00         0\n",
      "          58       0.00      0.00      0.00         0\n",
      "          59       0.00      0.00      0.00         0\n",
      "          60       0.00      0.00      0.00         2\n",
      "          61       0.00      0.00      0.00         0\n",
      "          62       0.00      0.00      0.00         2\n",
      "          63       0.00      0.00      0.00         1\n",
      "          64       0.00      0.00      0.00         0\n",
      "          65       0.00      0.00      0.00         0\n",
      "          66       0.00      0.00      0.00         0\n",
      "          67       0.00      0.00      0.00         0\n",
      "          68       0.00      0.00      0.00         0\n",
      "          69       0.00      0.00      0.00         0\n",
      "          70       0.00      0.00      0.00         0\n",
      "          71       0.00      0.00      0.00         0\n",
      "          72       0.00      0.00      0.00         0\n",
      "          73       0.00      0.00      0.00         0\n",
      "          74       0.00      0.00      0.00         0\n",
      "          75       0.00      0.00      0.00         5\n",
      "          76       0.29      0.09      0.13        81\n",
      "          77       0.00      0.00      0.00        44\n",
      "          78       0.00      0.00      0.00        19\n",
      "          79       0.42      0.38      0.40       273\n",
      "          80       0.31      0.22      0.25       244\n",
      "          81       0.34      0.20      0.25       374\n",
      "          82       0.82      0.45      0.58       139\n",
      "          83       0.85      0.73      0.79       628\n",
      "          84       0.76      0.62      0.68       245\n",
      "          85       0.58      0.61      0.59       143\n",
      "          86       0.57      0.30      0.40       158\n",
      "          87       0.80      0.63      0.70       418\n",
      "          88       0.65      0.54      0.59        59\n",
      "          89       0.84      0.50      0.62       165\n",
      "          90       0.66      0.72      0.69       436\n",
      "          91       0.82      0.47      0.60       213\n",
      "          92       0.59      0.55      0.57       195\n",
      "          93       0.47      0.28      0.35        53\n",
      "          94       0.57      0.32      0.41        41\n",
      "          95       0.68      0.68      0.68       206\n",
      "          96       0.71      0.72      0.71       306\n",
      "          97       1.00      0.10      0.17        21\n",
      "          98       0.00      0.00      0.00        12\n",
      "          99       0.69      0.31      0.43        70\n",
      "         100       0.78      0.66      0.71       117\n",
      "         101       0.55      0.26      0.35        89\n",
      "         102       0.34      0.20      0.25       374\n",
      "         103       0.70      0.12      0.21        56\n",
      "         104       0.84      0.62      0.71       321\n",
      "         105       0.81      0.66      0.73       132\n",
      "         106       0.71      0.19      0.30        64\n",
      "         107       0.83      0.73      0.78       347\n",
      "         108       0.74      0.74      0.74       374\n",
      "         109       0.57      0.31      0.41       147\n",
      "         110       0.64      0.42      0.51       219\n",
      "         111       0.69      0.51      0.59       129\n",
      "         112       0.57      0.39      0.47        79\n",
      "         113       0.78      0.57      0.66       112\n",
      "         114       0.62      0.36      0.45        14\n",
      "         115       0.43      0.14      0.21        65\n",
      "         116       0.65      0.68      0.66       299\n",
      "         117       0.00      0.00      0.00        22\n",
      "         118       0.72      0.63      0.67       219\n",
      "         119       0.76      0.70      0.73       255\n",
      "         120       0.82      0.67      0.74       398\n",
      "         121       0.93      0.92      0.93      1273\n",
      "         122       0.21      0.06      0.10        48\n",
      "         123       0.88      0.76      0.81        86\n",
      "         124       0.72      0.61      0.66        46\n",
      "         125       0.00      0.00      0.00         0\n",
      "         126       0.74      0.40      0.52        43\n",
      "         127       0.68      0.56      0.61       236\n",
      "         128       0.48      0.23      0.31       139\n",
      "         129       0.67      0.45      0.53       155\n",
      "         130       0.43      0.18      0.26       131\n",
      "         131       0.55      0.04      0.08       142\n",
      "         132       0.50      0.29      0.37       104\n",
      "         133       0.70      0.39      0.50        71\n",
      "         134       0.70      0.52      0.60       137\n",
      "         135       0.66      0.51      0.58       284\n",
      "         136       0.88      0.38      0.54        39\n",
      "         137       0.37      0.48      0.42        33\n",
      "         138       0.72      0.70      0.71       126\n",
      "         139       0.48      0.34      0.40        32\n",
      "         140       0.70      0.57      0.63       259\n",
      "         141       0.81      0.76      0.78       122\n",
      "         142       0.00      0.00      0.00        22\n",
      "         143       0.68      0.73      0.71       109\n",
      "         144       0.50      0.10      0.16        31\n",
      "         145       0.00      0.00      0.00         4\n",
      "         146       0.46      0.49      0.47       100\n",
      "         147       0.00      0.00      0.00        40\n",
      "         148       0.66      0.66      0.66        58\n",
      "         149       0.00      0.00      0.00        21\n",
      "         150       0.67      0.16      0.26        25\n",
      "         151       0.75      0.52      0.61        29\n",
      "         152       0.82      0.64      0.72        36\n",
      "         153       0.00      0.00      0.00        17\n",
      "         154       0.70      0.49      0.58        53\n",
      "         155       1.00      0.02      0.04        56\n",
      "         156       0.00      0.00      0.00        11\n",
      "         157       0.00      0.00      0.00        13\n",
      "         158       0.00      0.00      0.00         6\n",
      "         159       0.00      0.00      0.00         0\n",
      "         160       0.00      0.00      0.00        11\n",
      "         161       0.00      0.00      0.00         0\n",
      "         162       0.00      0.00      0.00         1\n",
      "         163       0.00      0.00      0.00         9\n",
      "         164       0.00      0.00      0.00         2\n",
      "         165       0.36      0.71      0.48        14\n",
      "         166       0.00      0.00      0.00         7\n",
      "         167       0.88      0.93      0.91      2079\n",
      "         168       1.00      0.03      0.05        79\n",
      "         169       0.00      0.00      0.00         0\n",
      "         170       0.00      0.00      0.00         1\n",
      "         171       0.00      0.00      0.00         3\n",
      "         172       0.00      0.00      0.00         1\n",
      "         173       0.00      0.00      0.00         0\n",
      "         174       0.78      0.57      0.66       112\n",
      "         175       1.00      0.29      0.44         7\n",
      "\n",
      "   micro avg       0.74      0.58      0.65     15481\n",
      "   macro avg       0.30      0.20      0.23     15481\n",
      "weighted avg       0.69      0.58      0.61     15481\n",
      " samples avg       0.75      0.67      0.68     15481\n",
      "\n",
      "Acc score:  0.4771\n"
     ]
    }
   ],
   "source": [
    "predict(tresh = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cs.LG', 'quant-ph', 'hep-ph', 'hep-th', 'cs.CV', 'gr-qc',\n",
       "       'cond-mat.mes-hall', 'cond-mat.mtrl-sci', 'astro-ph.CO', 'stat.ML',\n",
       "       ...\n",
       "       'cond-mat', 'alg-geom', 'q-alg', 'q-bio', 'dg-ga', 'adap-org',\n",
       "       'chao-dyn', 'funct-an', 'solv-int', 'patt-sol'],\n",
       "      dtype='object', length=166)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_abstract = 'Despite recent progress in generative adversarial network(GAN)-based vocoders, where the model generates raw waveform conditioned on mel spectrogram, it is still challenging to synthesize high-fidelity audio for numerous speakers across varied recording environments. In this work, we present BigVGAN, a universal vocoder that generalizes well under various unseen conditions in zero-shot setting. We introduce periodic nonlinearities and anti-aliased representation into the generator, which brings the desired inductive bias for waveform synthesis and significantly improves audio quality. Based on our improved generator and the state-of-the-art discriminators, we train our GAN vocoder at the largest scale up to 112M parameters, which is unprecedented in the literature. In particular, we identify and address the training instabilities specific to such scale, while maintaining high-fidelity output without over-regularization. Our BigVGAN achieves the state-of-the-art zero-shot performance for various out-of-distribution scenarios, including new speakers, novel languages, singing voices, music and instrumental audio in unseen (even noisy) recording environments. We will release our code and model at: this https URL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = [\n",
    "    hard_abstract\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/wsuser/ipykernel_382/3092842459.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_predict[cat] = len(abstracts) * [0]\n"
     ]
    }
   ],
   "source": [
    "df_predict = pd.DataFrame({'abstract': abstracts})\n",
    "\n",
    "for cat in df_train.columns[1:]:\n",
    "    df_predict[cat] = len(abstracts) * [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>cs.LG</th>\n",
       "      <th>quant-ph</th>\n",
       "      <th>hep-ph</th>\n",
       "      <th>hep-th</th>\n",
       "      <th>cs.CV</th>\n",
       "      <th>gr-qc</th>\n",
       "      <th>cond-mat.mes-hall</th>\n",
       "      <th>cond-mat.mtrl-sci</th>\n",
       "      <th>astro-ph.CO</th>\n",
       "      <th>...</th>\n",
       "      <th>cond-mat</th>\n",
       "      <th>alg-geom</th>\n",
       "      <th>q-alg</th>\n",
       "      <th>q-bio</th>\n",
       "      <th>dg-ga</th>\n",
       "      <th>adap-org</th>\n",
       "      <th>chao-dyn</th>\n",
       "      <th>funct-an</th>\n",
       "      <th>solv-int</th>\n",
       "      <th>patt-sol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Despite recent progress in generative adversar...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  cs.LG  quant-ph  hep-ph  \\\n",
       "0  Despite recent progress in generative adversar...      0         0       0   \n",
       "\n",
       "   hep-th  cs.CV  gr-qc  cond-mat.mes-hall  cond-mat.mtrl-sci  astro-ph.CO  \\\n",
       "0       0      0      0                  0                  0            0   \n",
       "\n",
       "   ...  cond-mat  alg-geom  q-alg  q-bio  dg-ga  adap-org  chao-dyn  funct-an  \\\n",
       "0  ...         0         0      0      0      0         0         0         0   \n",
       "\n",
       "   solv-int  patt-sol  \n",
       "0         0         0  \n",
       "\n",
       "[1 rows x 167 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytorch_lightning as pl\n",
    "class PredictArxivAbstractsModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, df_predict, tokenizer, batch_size=1, max_token_len=256):\n",
    "        super().__init__()\n",
    "        self.df_predict = df_predict\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = max_token_len\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.predict_dataset = ArxivAbstracts(df=self.df_predict, tokenizer=self.tokenizer, max_len= self.max_token_len)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.predict_dataset, batch_size=self.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "   abstracts = [abstract]\n",
    "    df_predict = pd.DataFrame({'abstract': abstracts}) \n",
    "    categories = df_train.columns[1:]\n",
    "    for cat in categories:\n",
    "        df_predict[cat] = len(abstracts) * [0]\n",
    "        \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "    arxiv_data = PredictArxivAbstractsModule(df_predict, tokenizer, max_token_len=384)\n",
    "    arxiv_data.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Predicting</span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">1/1</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:00:00 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">0.00it/s</span>  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37mPredicting\u001b[0m \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m1/1\u001b[0m \u001b[38;5;245m0:00:00 • 0:00:00\u001b[0m \u001b[38;5;249m0.00it/s\u001b[0m  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = trainer.predict(model, dataloaders=arxiv_data.predict_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(abstract):\n",
    "    abstracts = [abstract]\n",
    "    df_predict = pd.DataFrame({'abstract': abstracts}) \n",
    "    categories = df_train.columns[1:]\n",
    "    for cat in categories:\n",
    "        df_predict[cat] = len(abstracts) * [0]\n",
    "        \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "    arxiv_data = PredictArxivAbstractsModule(df_predict, tokenizer, max_token_len=384)\n",
    "    arxiv_data.setup()\n",
    "    \n",
    "    predictions = trainer.predict(model, dataloaders=arxiv_data.predict_dataloader())\n",
    "    predictions = predictions[0].numpy()[0]\n",
    "        \n",
    "    predictions_proba = []\n",
    "    predictions_labels = []\n",
    "        \n",
    "    for cat, prob in zip(categories, predictions):\n",
    "        if prob >= 0.25:\n",
    "            predictions_labels.append(cat)\n",
    "            predictions_proba.append(prob)\n",
    "        \n",
    "    return list(zip(predictions_labels, predictions_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Predicting</span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">1/1</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:00:00 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">0.00it/s</span>  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37mPredicting\u001b[0m \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m1/1\u001b[0m \u001b[38;5;245m0:00:00 • 0:00:00\u001b[0m \u001b[38;5;249m0.00it/s\u001b[0m  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('cs.RO', 0.81435853),\n",
       " ('cs.DC', 0.9684285),\n",
       " ('physics.app-ph', 0.9479388),\n",
       " ('funct-an', 0.27130815)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(hard_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a37be126389345768afe419a442ccda8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398973792a5245ccbdd8bce569541874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_abstract(model, abstract):\n",
    "    text = abstract\n",
    "    \n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        None,\n",
    "        add_special_tokens=True,\n",
    "        max_length=384,\n",
    "        padding=\"max_length\",\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask= True,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "      )\n",
    "    \n",
    "    test_prediction = model(ids = encoding[\"input_ids\"], mask = encoding[\"attention_mask\"])\n",
    "    test_prediction = torch.sigmoid(test_prediction).flatten().detach().numpy()\n",
    "    \n",
    "    predictions_proba = []\n",
    "    predictions_labels = []\n",
    "\n",
    "    for cat, prob in zip(sorted(LABEL_COLUMNS), test_prediction):\n",
    "        if prob >= 0.25:\n",
    "            predictions_labels.append(cat)\n",
    "            predictions_proba.append(prob)\n",
    "    \n",
    "    return sorted(list(zip(predictions_labels, predictions_proba)), key=lambda elem: elem[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('stat.ML', 0.27130842),\n",
       " ('cs.LG', 0.8143588),\n",
       " ('eess.AS', 0.9479388),\n",
       " ('cs.SD', 0.9684285)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract = 'Despite recent progress in generative adversarial network(GAN)-based vocoders, where the model generates raw waveform conditioned on mel spectrogram, it is still challenging to synthesize high-fidelity audio for numerous speakers across varied recording environments. In this work, we present BigVGAN, a universal vocoder that generalizes well under various unseen conditions in zero-shot setting. We introduce periodic nonlinearities and anti-aliased representation into the generator, which brings the desired inductive bias for waveform synthesis and significantly improves audio quality. Based on our improved generator and the state-of-the-art discriminators, we train our GAN vocoder at the largest scale up to 112M parameters, which is unprecedented in the literature. In particular, we identify and address the training instabilities specific to such scale, while maintaining high-fidelity output without over-regularization. Our BigVGAN achieves the state-of-the-art zero-shot performance for various out-of-distribution scenarios, including new speakers, novel languages, singing voices, music and instrumental audio in unseen (even noisy) recording environments. We will release our code and model at: this https URL'\n",
    "\n",
    "predict_abstract(model, abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract = \"Function as a Service (FaaS) paradigm is getting widespread and is envisioned to be the next generation of cloud computing systems that mitigates the burden for programmers and cloud solution architects. However, FaaS does not explicitly deal with the data, and developers have to intervene and undergo the burden of managing the application data, often, via separate cloud services (e.g., AWS RDS). We overcome this problem of FaaS by borrowing the notion of object from the object-oriented programming into the serverless systems. We propose a new paradigm on top of the function abstraction, known as Object as a Service (OaaS), that offers encapsulation and abstraction benefits. The OaaS incorporates the application data into the object abstraction and relieves the developers from dealing with separate cloud services for the data management. It also unlocks opportunities for built-in optimization features, such as software reusability, data locality, and caching. Moreover, OaaS enables dataflow programming such that the developers define a workflow of functions transparently without getting involved into the synchronization and parallelism aspects. We implemented a prototype of the OaaS platform that is low-overhead and scalable. We evaluated it under real-world settings in terms of the ease-of-use, imposed overhead, and scalability. The results demonstrate that OaaS streamlines cloud programming and offers scalability with a minor overhead to the underlying cloud system\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/abs/2206.05361"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cs.SE', 0.5767369), ('cs.DC', 0.7603023)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_abstract(model, abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
