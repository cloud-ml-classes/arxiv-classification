{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDEK\n",
    "!pip install -q transformers rich torchmetrics sklearn pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, types\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "import dask.bag as db\n",
    "import json\n",
    "import zipfile\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import csv\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import RichProgressBar\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from torchmetrics import F1\n",
    "from torchmetrics.functional import accuracy\n",
    "from transformers import RobertaTokenizer, RobertaModel, AutoTokenizer, AutoModel\n",
    "from transformers import AutoModel, AdamW, get_cosine_schedule_with_warmup\n",
    "import math\n",
    "from torchmetrics.functional.classification import auroc\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_json_data = \"arxiv-metadata-oai-snapshot-json.zip\"\n",
    "pickled_dataset = 'dataset_pickle.pkl'\n",
    "pickled_dataset_onehot = 'dataset_multilabel.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __iter__(self): return 0\n",
    "\n",
    "ibm_api_key_id = 'SECRET'\n",
    "bucket = 'SECRET'\n",
    "\n",
    "client_c2b4ce19d76d4b7c87bdf6c8c84d662c = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id=ibm_api_key_id,\n",
    "    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url='https://s3.private.us.cloud-object-storage.appdomain.cloud')\n",
    "\n",
    "streaming_body_1 = client_c2b4ce19d76d4b7c87bdf6c8c84d662c.get_object(Bucket=bucket, Key='arxiv-metadata-oai-snapshot.json.zip')['Body']\n",
    "\n",
    "def save_to_cloud(key, file):\n",
    "    client_c2b4ce19d76d4b7c87bdf6c8c84d662c.upload_file(Bucket=bucket, Key=f'snapshot-{file}', Filename=file)\n",
    "\n",
    "    \n",
    "def get_file(key, filename):\n",
    "    client_c2b4ce19d76d4b7c87bdf6c8c84d662c.download_file(Bucket=bucket, Key=key, Filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     get_file('arxiv-metadata-oai-snapshot.pkl', 'dataset_pickle.pkl')\n",
    "# except:\n",
    "#     print(\"Exception while pulling, will recreate from scratch\")\n",
    "    \n",
    "\n",
    "# if not os.path.exists(pickled_dataset):\n",
    "#     get_file('arxiv-metadata-oai-snapshot.json.zip', raw_json_data)\n",
    "#     with zipfile.ZipFile(raw_json_data, 'r') as zip_ref:\n",
    "#         zip_ref.extractall(\"dataset\")\n",
    "#     docs = db.read_text('./dataset/arxiv-metadata-oai-snapshot.json').map(json.loads)\n",
    "#     trim = lambda x: {'id': x['id'],\n",
    "#                   'category':x['categories'].split(' '),\n",
    "#                   'abstract':x['abstract'].replace('\\t', ' ')}\n",
    "#     docs_df = (docs.map(trim).compute())\n",
    "#     df = pd.DataFrame(docs_df)\n",
    "#     df.to_pickle(pickled_dataset)\n",
    "#     save_to_cloud('arxiv-metadata-oai-snapshot.pkl', pickled_dataset)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_pickle('dataset_pickle.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     get_file('snapshot-dataset_multilabel.pkl', 'dataset_multilabel.pkl')\n",
    "#     print(\"Got S3 file\")\n",
    "# except:\n",
    "#     print(\"Exception while pulling, will recreate from scratch\")\n",
    "    \n",
    "# if not os.path.exists('dataset_multilabel.pkl'):\n",
    "#     mlb = MultiLabelBinarizer()\n",
    "#     labels = mlb.fit_transform(df.category)\n",
    "#     df_onehot = pd.concat([df[['abstract']], pd.DataFrame(labels)], axis=1)\n",
    "#     df_onehot.columns = ['abstract'] + list(mlb.classes_)\n",
    "#     df_onehot.to_pickle('dataset_multilabel.pkl')\n",
    "#     save_to_cloud('snapshot-dataset_multilabel.pkl', 'dataset_multilabel.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_onehot = pd.read_pickle('dataset_multilabel.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABEL_COLUMNS = df_onehot.columns.tolist()[1:]\n",
    "# LABEL_COLUMNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying preprocessing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_file('arxiv-metadata-oai-snapshot.json.zip', raw_json_data)\n",
    "# with zipfile.ZipFile(raw_json_data, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(\"dataset\")\n",
    "# docs = db.read_text('./dataset/arxiv-metadata-oai-snapshot.json').map(json.loads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_latest_version = lambda x: x['versions'][-1]['created']\n",
    "# trim = lambda x: {'id': x['id'],\n",
    "#               'category':x['categories'].split(' '),\n",
    "#               'abstract':x['abstract'].replace('\\t', ' ')}\n",
    "# docs_df = (docs.filter(lambda x: int(get_latest_version(x).split(' ')[3]) > 2009).map(trim).compute())\n",
    "# df = pd.DataFrame(docs_df)\n",
    "# df.to_pickle(\"df2009.pkl\")\n",
    "# save_to_cloud('df2009.pkl', 'df2009.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_file('snapshot-df2009.pkl', 'df2009.pkl')\n",
    "# df = pd.read_pickle('df2009.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# df[\"abstract\"] = df[\"abstract\"].apply(lambda text: \" \".join([word for word in text.replace(\"\\n\", \" \").replace(\"\\t\", \" \").strip().split(\" \") if len(word) > 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequent_categories = pd.Series(np.concatenate(df['category'])).value_counts().reset_index(name=\"count\").query(\"count > 1000\")[\"index\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(frequent_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[df[\"category\"].apply(lambda cats: all(elem in frequent_categories for elem in cats))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[df[\"abstract\"].apply(lambda text: len(text.split()) > 128)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories_filtered = pd.Series(np.concatenate(df['category'])).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rich.progress import track\n",
    "\n",
    "# # Select 5k randomly, then at least 1k from each category, then drop duplicates\n",
    "# uni_df = df.sample(n = 5000)\n",
    "# for category, _ in track(categories_filtered.items(), total=len(categories_filtered), description=\"Pulling 1k from each cat\"):\n",
    "#     subset_df = df[df[\"category\"].apply(lambda cats: category in cats)]\n",
    "#     uni_df = pd.concat([subset_df.sample(n = min(1000, len(subset_df))), uni_df], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uni_df = uni_df.iloc[uni_df.astype(str).drop_duplicates().index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(uni_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uni_df = uni_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlb = MultiLabelBinarizer()\n",
    "# labels = mlb.fit_transform(uni_df.category)\n",
    "# df_onehot = pd.concat([uni_df[['abstract']], pd.DataFrame(labels)], axis=1)\n",
    "# df_onehot.columns = ['abstract'] + list(mlb.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_onehot.to_pickle('uni_df.pkl')\n",
    "# save_to_cloud('uni_df.pkl', 'uni_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_file('snapshot-uni_df.pkl', 'uni_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_onehot = pd.read_pickle('uni_df.pkl')\n",
    "LABEL_COLUMNS = df_onehot.columns.tolist()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['astro-ph.CO',\n",
       " 'astro-ph.EP',\n",
       " 'astro-ph.GA',\n",
       " 'astro-ph.HE',\n",
       " 'astro-ph.IM',\n",
       " 'astro-ph.SR',\n",
       " 'cond-mat.dis-nn',\n",
       " 'cond-mat.mes-hall',\n",
       " 'cond-mat.mtrl-sci',\n",
       " 'cond-mat.other',\n",
       " 'cond-mat.quant-gas',\n",
       " 'cond-mat.soft',\n",
       " 'cond-mat.stat-mech',\n",
       " 'cond-mat.str-el',\n",
       " 'cond-mat.supr-con',\n",
       " 'cs.AI',\n",
       " 'cs.AR',\n",
       " 'cs.CC',\n",
       " 'cs.CE',\n",
       " 'cs.CG',\n",
       " 'cs.CL',\n",
       " 'cs.CR',\n",
       " 'cs.CV',\n",
       " 'cs.CY',\n",
       " 'cs.DB',\n",
       " 'cs.DC',\n",
       " 'cs.DL',\n",
       " 'cs.DM',\n",
       " 'cs.DS',\n",
       " 'cs.ET',\n",
       " 'cs.FL',\n",
       " 'cs.GR',\n",
       " 'cs.GT',\n",
       " 'cs.HC',\n",
       " 'cs.IR',\n",
       " 'cs.IT',\n",
       " 'cs.LG',\n",
       " 'cs.LO',\n",
       " 'cs.MA',\n",
       " 'cs.MM',\n",
       " 'cs.MS',\n",
       " 'cs.NA',\n",
       " 'cs.NE',\n",
       " 'cs.NI',\n",
       " 'cs.OH',\n",
       " 'cs.PF',\n",
       " 'cs.PL',\n",
       " 'cs.RO',\n",
       " 'cs.SC',\n",
       " 'cs.SD',\n",
       " 'cs.SE',\n",
       " 'cs.SI',\n",
       " 'cs.SY',\n",
       " 'econ.EM',\n",
       " 'econ.GN',\n",
       " 'econ.TH',\n",
       " 'eess.AS',\n",
       " 'eess.IV',\n",
       " 'eess.SP',\n",
       " 'eess.SY',\n",
       " 'gr-qc',\n",
       " 'hep-ex',\n",
       " 'hep-lat',\n",
       " 'hep-ph',\n",
       " 'hep-th',\n",
       " 'math-ph',\n",
       " 'math.AC',\n",
       " 'math.AG',\n",
       " 'math.AP',\n",
       " 'math.AT',\n",
       " 'math.CA',\n",
       " 'math.CO',\n",
       " 'math.CT',\n",
       " 'math.CV',\n",
       " 'math.DG',\n",
       " 'math.DS',\n",
       " 'math.FA',\n",
       " 'math.GM',\n",
       " 'math.GN',\n",
       " 'math.GR',\n",
       " 'math.GT',\n",
       " 'math.HO',\n",
       " 'math.IT',\n",
       " 'math.KT',\n",
       " 'math.LO',\n",
       " 'math.MG',\n",
       " 'math.MP',\n",
       " 'math.NA',\n",
       " 'math.NT',\n",
       " 'math.OA',\n",
       " 'math.OC',\n",
       " 'math.PR',\n",
       " 'math.QA',\n",
       " 'math.RA',\n",
       " 'math.RT',\n",
       " 'math.SG',\n",
       " 'math.SP',\n",
       " 'math.ST',\n",
       " 'nlin.AO',\n",
       " 'nlin.CD',\n",
       " 'nlin.PS',\n",
       " 'nlin.SI',\n",
       " 'nucl-ex',\n",
       " 'nucl-th',\n",
       " 'physics.acc-ph',\n",
       " 'physics.ao-ph',\n",
       " 'physics.app-ph',\n",
       " 'physics.atm-clus',\n",
       " 'physics.atom-ph',\n",
       " 'physics.bio-ph',\n",
       " 'physics.chem-ph',\n",
       " 'physics.class-ph',\n",
       " 'physics.comp-ph',\n",
       " 'physics.data-an',\n",
       " 'physics.ed-ph',\n",
       " 'physics.flu-dyn',\n",
       " 'physics.gen-ph',\n",
       " 'physics.geo-ph',\n",
       " 'physics.hist-ph',\n",
       " 'physics.ins-det',\n",
       " 'physics.med-ph',\n",
       " 'physics.optics',\n",
       " 'physics.plasm-ph',\n",
       " 'physics.pop-ph',\n",
       " 'physics.soc-ph',\n",
       " 'physics.space-ph',\n",
       " 'q-bio.BM',\n",
       " 'q-bio.CB',\n",
       " 'q-bio.GN',\n",
       " 'q-bio.MN',\n",
       " 'q-bio.NC',\n",
       " 'q-bio.PE',\n",
       " 'q-bio.QM',\n",
       " 'q-bio.SC',\n",
       " 'q-bio.TO',\n",
       " 'q-fin.CP',\n",
       " 'q-fin.EC',\n",
       " 'q-fin.GN',\n",
       " 'q-fin.MF',\n",
       " 'q-fin.PM',\n",
       " 'q-fin.PR',\n",
       " 'q-fin.RM',\n",
       " 'q-fin.ST',\n",
       " 'q-fin.TR',\n",
       " 'quant-ph',\n",
       " 'stat.AP',\n",
       " 'stat.CO',\n",
       " 'stat.ME',\n",
       " 'stat.ML',\n",
       " 'stat.OT',\n",
       " 'stat.TH']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL_COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.multiclass import OneVsRestClassifier\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# pipeline = Pipeline([\n",
    "#     ('tfidf', TfidfVectorizer(analyzer='word', \n",
    "#                               smooth_idf=False,\n",
    "#                               stop_words='english', \n",
    "#                               min_df = 3, \n",
    "#                               max_df = 0.95, \n",
    "#                               strip_accents='unicode', \n",
    "#                               sublinear_tf = True)),\n",
    "#     ('clf', OneVsRestClassifier(LogisticRegression(multi_class='multinomial', max_iter=300, verbose=1, n_jobs=-1)))\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM_SEED = 1337\n",
    "# df_train, df_test = train_test_split(df_onehot, test_size=0.1, random_state=RANDOM_SEED, shuffle=True)\n",
    "# df_train, df_validation = train_test_split(df_train, test_size=0.2, random_state=RANDOM_SEED, shuffle=True)\n",
    "# df_train = df_train.reset_index(drop=True)\n",
    "# df_test = df_test.reset_index(drop=True)\n",
    "# df_validation = df_validation.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = df_train[\"abstract\"]\n",
    "# y_train = df_train.loc[:, ~df_train.columns.isin(['abstract'])]\n",
    "# X_test = df_test[\"abstract\"]\n",
    "# y_test = df_test.loc[:, ~df_test.columns.isin(['abstract'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_proba = pipeline.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict(tresh = 0.5):\n",
    "#     y_pred = [[int(val >= tresh) for val in pred] for pred in preds_proba]\n",
    "#     y_true = y_test\n",
    "#     print(classification_report(y_true, y_pred))\n",
    "#     print(\"Acc score: \", accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict(0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# y_pred = [[int(val >= 0.2) for val in pred] for pred in preds_proba]\n",
    "# y_true = y_test\n",
    "# cm = multilabel_confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_onehot_subs = df_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_onehot_subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArxivAbstracts(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text = df.abstract\n",
    "        self.labels = df[df.columns.difference(['abstract'])]\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, item_idx):\n",
    "        text = self.text[item_idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask= True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "          )\n",
    "               \n",
    "        return {\n",
    "          'ids': encoding[\"input_ids\"].flatten(),\n",
    "          'mask': encoding[\"attention_mask\"].flatten(),\n",
    "          'targets': torch.tensor(self.labels.iloc[item_idx].tolist(), dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 1337\n",
    "df_train, df_test = train_test_split(df_onehot, test_size=0.1, random_state=RANDOM_SEED, shuffle=True)\n",
    "df_train, df_validation = train_test_split(df_train, test_size=0.2, random_state=RANDOM_SEED, shuffle=True)\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "df_validation = df_validation.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "class ArxivAbstractsModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, df_train, df_test, df_validation, tokenizer, batch_size=16, max_token_len=256):\n",
    "        super().__init__()\n",
    "        self.df_train = df_train\n",
    "        self.df_test = df_test\n",
    "        self.df_validation = df_validation\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = ArxivAbstracts(df=self.df_train, tokenizer=self.tokenizer, max_len= self.max_token_len)\n",
    "        self.val_dataset = ArxivAbstracts(df=self.df_validation, tokenizer=self.tokenizer, max_len= self.max_token_len)\n",
    "        self.test_dataset = ArxivAbstracts(df=self.df_test, tokenizer=self.tokenizer, max_len= self.max_token_len)\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45cabba591274ded972db7cc9a87414f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2beb7ae411514ecdbe086452743d7a75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0860e15fdb304b21a38611898ca7292e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfb9c8fa3e1544188441ceee8c7940c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilroberta-base')\n",
    "arxiv_data = ArxivAbstractsModule(df_train, df_test, df_validation, tokenizer, max_token_len=384)\n",
    "arxiv_data.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53.625"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(arxiv_data.test_dataloader())/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArxivAbstractsClassifier(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.embedding = nn.Embedding(100000, 60, padding_idx=1)\n",
    "        self.gru = nn.GRU(60, 100, batch_first=True, num_layers=3)\n",
    "        #self.pretrained_model = AutoModel.from_pretrained(config['model_name'], return_dict=True)\n",
    "        #self.hidden = torch.nn.Linear(self.pretrained_model.config.hidden_size, self.pretrained_model.config.hidden_size)\n",
    "        self.classifier = torch.nn.Linear(100, 151) \n",
    "        #torch.nn.init.xavier_uniform_(self.classifier.weight)\n",
    "        self.loss_func = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "        #self.dropout = torch.nn.Dropout()\n",
    "        self.save_hyperparameters()\n",
    "    \n",
    "    def forward(self, ids, mask, targets=None):\n",
    "        # roberta layer\n",
    "#         output = self.pretrained_model(input_ids=ids, attention_mask=mask)\n",
    "#         pooled_output = torch.mean(output.last_hidden_state, 1)\n",
    "#         # final logits\n",
    "#         pooled_output = self.dropout(pooled_output)\n",
    "#         pooled_output = self.hidden(pooled_output)\n",
    "#         pooled_output = F.relu(pooled_output)\n",
    "#         pooled_output = self.dropout(pooled_output)\n",
    "#         logits = self.classifier(pooled_output)\n",
    "        # calculate loss\n",
    "        embed = self.embedding(ids)\n",
    "    \n",
    "        lengths = torch.sum(mask, axis = 1)\n",
    "        \n",
    "        #print(lengths)\n",
    "            \n",
    "        packed_embedded = torch.nn.utils.rnn.pack_padded_sequence(embed, lengths.tolist(), batch_first=True, enforce_sorted=False)\n",
    "    \n",
    "        output, h_n = self.gru(packed_embedded)\n",
    "        \n",
    "        unpacked, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True, padding_value=1)\n",
    "        \n",
    "#         print(unpacked)\n",
    "        \n",
    "        logits = self.classifier(unpacked[range(ids.shape[0]), lengths - 1])\n",
    "        \n",
    "        loss = 0\n",
    "        if targets is not None:\n",
    "            loss = self.loss_func(logits.view(-1, 151), targets.view(-1, 151))\n",
    "        return loss, logits\n",
    "  \n",
    "    def training_step(self, batch, batch_index):\n",
    "        loss, outputs = self(**batch)\n",
    "        self.log(\"train_loss\", loss.item())\n",
    "        return {\"loss\":loss, \"predictions\": outputs, \"labels\": batch[\"targets\"]}\n",
    "\n",
    "    def validation_step(self, batch, batch_index):\n",
    "        loss, outputs = self(**batch)\n",
    "        self.log(\"val_loss\", loss.item())\n",
    "        return {\"val_loss\": loss, \"predictions\": outputs, \"labels\": batch[\"targets\"]}\n",
    "\n",
    "    def predict_step(self, batch, batch_index):\n",
    "        loss, outputs = self(**batch)\n",
    "        return torch.sigmoid(outputs)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters())\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    monitor='val_loss',\n",
    "    filename='GRU-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=2,\n",
    "    verbose=True,\n",
    "    mode='min'\n",
    ")\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"gru2\")\n",
    "\n",
    "config = {\n",
    "    'model_name': 'distilroberta-base',\n",
    "    'n_labels': 151,\n",
    "    'batch_size': 16,\n",
    "    'lr': 1.5e-6,\n",
    "    'warmup': 0.2, \n",
    "    'train_size': len(arxiv_data.train_dataloader()),\n",
    "    'weight_decay': 0.001,\n",
    "    'n_epochs': 30\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Epoch 0   </span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">╺━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">1555/7716</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:02:47 • 0:11:04</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">9.28it/s</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">loss: 0.0765 v_num: 0 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37mEpoch 0   \u001b[0m \u001b[38;2;98;6;224m━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m1555/7716\u001b[0m \u001b[38;5;245m0:02:47 • 0:11:04\u001b[0m \u001b[38;5;249m9.28it/s\u001b[0m \u001b[37mloss: 0.0765 v_num: 0 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer\n",
       ".py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
       "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer\n",
       ".py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
       "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 39s, sys: 19.2 s, total: 2min 58s\n",
      "Wall time: 2min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "model = ArxivAbstractsClassifier(config)\n",
    "trainer = Trainer(max_epochs = config['n_epochs'], \n",
    "                  gpus = 1, \n",
    "                  logger=logger, \n",
    "                  enable_checkpointing=True, \n",
    "                  callbacks=[checkpoint_callback, RichProgressBar(), EarlyStopping(monitor=\"val_loss\", mode=\"min\")])\n",
    "\n",
    "\n",
    "trainer.fit(model, datamodule=arxiv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"GRU\"\n",
    "def save_dir_to_cloud(path, model_name):\n",
    "        for root,dirs,files in os.walk(path):\n",
    "            for file in files:\n",
    "                filepath = os.path.join(root,file)\n",
    "                save_to_cloud(f\"outs/{model_name}/{file}\", filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(f\"{model_name}.ckpt\")\n",
    "save_to_cloud(f'{model_name}.ckpt', f\"{model_name}.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir_to_cloud(\"lightning_logs\", model_name)\n",
    "save_dir_to_cloud(\"checkpoints\", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_file(f'snapshot-{model_name}.ckpt', f\"snapshot-{model_name}.ckpt\")\n",
    "# trainer = Trainer()\n",
    "# trainer.fit(model, datamodule=arxiv_data, ckpt_path=f\"snapshot-{model_name}.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = ArxivAbstractsClassifier(config).load_from_checkpoint(f\"snapshot-{model_name}.ckpt\", config=config)\n",
    "trainer = Trainer(gpus = 1, \n",
    "                  logger=logger,\n",
    "                  callbacks=[RichProgressBar()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.test(model, datamodule=arxiv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Predicting</span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">858/858</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:00:51 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">16.56it/s</span>  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37mPredicting\u001b[0m \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m858/858\u001b[0m \u001b[38;5;245m0:00:51 • 0:00:00\u001b[0m \u001b[38;5;249m16.56it/s\u001b[0m  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = trainer.predict(model, dataloaders=arxiv_data.test_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.concatenate(preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probably needs a more sophisticated thingy with model.eval + freezing\n",
    "# Or just incorporating that to the model above so that loading from a checkpoint will work correctly\n",
    "def predict(tresh = 0.5):\n",
    "    y_pred = [[int(val >= tresh) for val in pred] for pred in preds]\n",
    "    y_true = [tensor[\"targets\"] for tensor in arxiv_data.test_dataloader()]\n",
    "    y_true = np.concatenate([tensor.tolist() for tensor in y_true])\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"Acc score: \", accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(tresh = 0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(tresh = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(tresh = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(tresh = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 1).\n",
       "Contents of stderr:\n",
       "Error: A logdir or db must be specified. For example `tensorboard --logdir mylogdir` or `tensorboard --db sqlite:~/.tensorboard.db`. Run `tensorboard --helpfull` for details and examples."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --event_file=cos://us-geo/arxivabstractstolabelspreds-donotdelete-pr-oehm4wj3avyjr3/snapshot-lightning_logs/arxiv-abstracts-logs/version_0/events.out.tfevents.1654964710.notebook-gpu1rt22199390efe221c4a9c9314e6fc8bfffde0-8645f562s2gc.503.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
