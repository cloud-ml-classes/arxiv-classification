{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers rich torchmetrics sklearn pytorch-lightning sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, types\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "import dask.bag as db\n",
    "import json\n",
    "import zipfile\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import csv\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import RichProgressBar\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from torchmetrics import F1\n",
    "from torchmetrics.functional import accuracy\n",
    "from transformers import RobertaTokenizer, RobertaModel, AutoTokenizer, AutoModel\n",
    "from transformers import AutoModel, AdamW, get_cosine_schedule_with_warmup\n",
    "import math\n",
    "from torchmetrics.functional.classification import auroc\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "from rich.progress import track\n",
    "import pytorch_lightning as pl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘dataset’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_json_data = \"arxiv-metadata-oai-snapshot-json.zip\"\n",
    "pickled_dataset = 'dataset_pickle.pkl'\n",
    "pickled_dataset_onehot = 'dataset_multilabel.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __iter__(self): return 0\n",
    "\n",
    "client_c2b4ce19d76d4b7c87bdf6c8c84d662c = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id='SECRET',\n",
    "    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url='https://s3.private.us.cloud-object-storage.appdomain.cloud')\n",
    "\n",
    "streaming_body_1 = client_c2b4ce19d76d4b7c87bdf6c8c84d662c.get_object(Bucket='SECRET', Key='arxiv-metadata-oai-snapshot.json.zip')['Body']\n",
    "\n",
    "def save_to_cloud(key, file):\n",
    "    client_c2b4ce19d76d4b7c87bdf6c8c84d662c.upload_file(Bucket='SECRET', Key=f'snapshot-{file}', Filename=file)\n",
    "\n",
    "    \n",
    "def get_file(key, filename):\n",
    "    client_c2b4ce19d76d4b7c87bdf6c8c84d662c.download_file(Bucket='SECRET', Key=key, Filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_file('snapshot-uni_df.pkl', 'uni_df.pkl')\n",
    "\n",
    "df_onehot = pd.read_pickle('uni_df.pkl')\n",
    "LABEL_COLUMNS = df_onehot.columns.tolist()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArxivAbstracts(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text = df.abstract\n",
    "        self.labels = df[df.columns.difference(['abstract'])]\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, item_idx):\n",
    "        text = self.text[item_idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask= True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "          )\n",
    "               \n",
    "        return {\n",
    "          'ids': encoding[\"input_ids\"].flatten(),\n",
    "          'mask': encoding[\"attention_mask\"].flatten(),\n",
    "          'targets': torch.tensor(self.labels.iloc[item_idx].tolist(), dtype=torch.float)  \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 1337\n",
    "df_train, df_test = train_test_split(df_onehot, test_size=0.1, random_state=RANDOM_SEED, shuffle=True)\n",
    "df_train, df_validation = train_test_split(df_train, test_size=0.2, random_state=RANDOM_SEED, shuffle=True)\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "df_validation = df_validation.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "class ArxivAbstractsModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, df_train, df_test, df_validation, tokenizer, batch_size=16, max_token_len=256):\n",
    "        super().__init__()\n",
    "        self.df_train = df_train\n",
    "        self.df_test = df_test\n",
    "        self.df_validation = df_validation\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = ArxivAbstracts(df=self.df_train, tokenizer=self.tokenizer, max_len= self.max_token_len)\n",
    "        self.val_dataset = ArxivAbstracts(df=self.df_validation, tokenizer=self.tokenizer, max_len= self.max_token_len)\n",
    "        self.test_dataset = ArxivAbstracts(df=self.df_test, tokenizer=self.tokenizer, max_len= self.max_token_len)\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "arxiv_data = ArxivAbstractsModule(df_train, df_test, df_validation, tokenizer, max_token_len=384)\n",
    "arxiv_data.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArxivAbstractsClassifier(pl.LightningModule):\n",
    "    def __init__(self, n_classes, lr=2e-5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-cased\", return_dict=True)\n",
    "        self.classifier = torch.nn.Linear(self.bert.config.hidden_size, n_classes) \n",
    "        self.lr = lr\n",
    "        self.criterion = torch.nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        self.f1_metrics = torch.nn.ModuleDict({\n",
    "            'f1_weighted': F1(num_classes=len(LABEL_COLUMNS), average='weighted', threshold=0.5),       \n",
    "            'f1_samples': F1(num_classes=len(LABEL_COLUMNS), average='samples', threshold=0.5), \n",
    "            'f1_micro': F1(num_classes=len(LABEL_COLUMNS), average='micro', threshold=0.5),\n",
    "            'f1_macro': F1(num_classes=len(LABEL_COLUMNS), average='macro', threshold=0.5),\n",
    "            'f1_weighted_t02': F1(num_classes=len(LABEL_COLUMNS), average='weighted', threshold=0.2),       \n",
    "            'f1_samples_t02': F1(num_classes=len(LABEL_COLUMNS), average='samples', threshold=0.2), \n",
    "            'f1_micro_t02': F1(num_classes=len(LABEL_COLUMNS), average='micro', threshold=0.2),\n",
    "            'f1_macro_t02': F1(num_classes=len(LABEL_COLUMNS), average='macro', threshold=0.2),\n",
    "        })\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def compute_metrics(self, outputs, labels):\n",
    "        metrics = {}\n",
    "        for name, metric in self.f1_metrics.items():\n",
    "            metrics[name] = metric(outputs, labels)\n",
    "        return metrics  \n",
    "    \n",
    "    def forward(self, ids=None, mask=None):\n",
    "        print(ids)\n",
    "        print(mask)\n",
    "#         if ids is None:\n",
    "#             ids = batch[0]\n",
    "#         if mask is None:\n",
    "#             mask = batch[1]\n",
    "        \n",
    "        output = self.bert(input_ids=ids, attention_mask=mask)\n",
    "        output = self.classifier(output.pooler_output)   \n",
    "        return output\n",
    "    \n",
    "    def loss(self, batch, prediction):\n",
    "        return self.criterion(prediction, batch[\"targets\"])\n",
    "    \n",
    "    def __generic_step(self, batch, batch_idx, namespace: str):\n",
    "        input_ids = batch[\"ids\"]\n",
    "        attention_mask = batch[\"mask\"]\n",
    "        \n",
    "        outputs = self(input_ids, attention_mask)\n",
    "        predictions = torch.sigmoid(outputs)\n",
    "        loss = self.loss(batch, outputs)\n",
    "        \n",
    "        if namespace != \"pred\":\n",
    "            labels = batch[\"targets\"]\n",
    "            metrics = self.compute_metrics(predictions, labels.int())\n",
    "            self.log_dict({f'{namespace}_loss': loss, **\n",
    "                           {f'{namespace}_{k}': v for k, v in metrics.items()}})\n",
    "        \n",
    "        return {\"loss\": loss, \"predictions\": predictions}\n",
    "        \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        return self.__generic_step(batch, batch_idx, namespace=\"pred\")[\"predictions\"]\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.__generic_step(batch, batch_idx, namespace=\"train\")[\"loss\"]\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.__generic_step(batch, batch_idx, namespace=\"val\")[\"loss\"]\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.__generic_step(batch, batch_idx, namespace=\"test\")[\"loss\"]\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(params=self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': scheduler,\n",
    "            'monitor': 'val_loss'\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ArxivAbstractsClassifierPreprocessData5Epoch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"arxiv-abstracts-logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_file(f'snapshot-{model_name}.ckpt', f'snapshot-{model_name}.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wml_credentials = {\n",
    "    'apikey': 'SECRET',\n",
    "    \"url\"         : \"https://eu-gb.ml.cloud.ibm.com\",\n",
    "}\n",
    "\n",
    "space_id = \"6f48a9aa-b319-450c-b6f1-bd3a789704d4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning import APIClient\n",
    "\n",
    "client = APIClient(wml_credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------  ------------  ------------------------\n",
      "ID                                    NAME          CREATED\n",
      "6f48a9aa-b319-450c-b6f1-bd3a789704d4  arxiv-papers  2022-06-11T20:25:39.649Z\n",
      "------------------------------------  ------------  ------------------------\n"
     ]
    }
   ],
   "source": [
    "client.spaces.list(limit=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SUCCESS'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.set.default_space(space_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ArxivAbstractsClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=151, bias=True)\n",
       "  (criterion): BCEWithLogitsLoss()\n",
       "  (f1_metrics): ModuleDict(\n",
       "    (f1_weighted): F1()\n",
       "    (f1_samples): F1()\n",
       "    (f1_micro): F1()\n",
       "    (f1_macro): F1()\n",
       "    (f1_weighted_t02): F1()\n",
       "    (f1_samples_t02): F1()\n",
       "    (f1_micro_t02): F1()\n",
       "    (f1_macro_t02): F1()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ArxivAbstractsClassifier(n_classes=151).load_from_checkpoint(f\"snapshot-{model_name}.ckpt\", n_classes=151)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sample = None\n",
    "for d in arxiv_data.val_dataloader():\n",
    "    input_sample = d\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': tensor([[  101,  2508,  2225,  ...,     0,     0,     0],\n",
       "         [  101,  1130,  1251,  ...,     0,     0,     0],\n",
       "         [  101,  1188,  2526,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,   142,  2050,  ...,     0,     0,     0],\n",
       "         [  101, 11336, 16236,  ..., 23896,  7008,   102],\n",
       "         [  101,  5732, 10393,  ...,     0,     0,     0]]),\n",
       " 'mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'targets': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampled = (input_sample[\"ids\"], input_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  101,  2508,  2225,  6660,  1103,  9539,  8096,   113,   154,  2036,\n",
       "            114,  4520,  1104,   170,  6307, 12650,  5114,  2007,  1107, 16811,\n",
       "           1766,  3417,  5315, 10221, 15049,   117,  1126, 19849,  2805,  1115,\n",
       "           6808, 22015,  1158,  1103,  1107, 16811,  1766, 10221,  2817,  1105,\n",
       "            187, 20517,  1158,  1103,  3378, 10221,  3205,  1506,  1103,  6307,\n",
       "          12650,  5114,  2007,  2473,   119, 16890,  4648, 15049, 19717,  1116,\n",
       "           2999,  2805,  1105,  2274,  5602,  1159,  1106, 18011,   119,  1130,\n",
       "           1142,  2526,   117,  1195, 10541,   170,  2281,  3442,  1104, 10099,\n",
       "           1103,   154,  2036,  4520,  1606,   170,  7483, 14377,  8297,  1115,\n",
       "           1884, 18337,  5430,  1103,  1107, 16811,  1766, 10221, 15442,  8516,\n",
       "           1166,  1159,  1114,  1103,  1703,  2965, 10972,   119,  9040, 14377,\n",
       "          13267, 14403,   117,  1842,   118,  1159,  9437,  1104,  1103,   154,\n",
       "           2036,  4520,  1443, 23465, 22015,  1158,  1103,  1107, 16811,  1766,\n",
       "          10221,  1137, 25602,  1107, 16811,  1766,  2805,   119,  1284,  1148,\n",
       "          10541,  1103,  3442,  1120,  1103, 12553,   153, 12606, 27384,  6307,\n",
       "           1394, 16811,  1766,  1114,  1103,  1494,  1104,   170,  3539, 17599,\n",
       "          15191,  2180,  1197,  4442,   113,   141, 18219,   114,  1105,   170,\n",
       "          16288,  6112, 11194,  5822,  4907,  5220,  1106,  2773,  1412,  1654,\n",
       "           1104,  1103,  2905, 20987, 26717,  1104,  1103, 25842,  6168,   119,\n",
       "           1109, 10442,  9932, 11934,  1132,  2503,   118, 17169,  1606, 14314,\n",
       "           1116,  1105,  1103,  2686,  1132,  9221,  2913,  1222,  1103,  1747,\n",
       "           3062,  4520,  2888,  1606,  1103,  2361,   187, 20517,  1158,  3442,\n",
       "            119,  4428,   117,  1195,  6058,  1103,  5531,  1106,  2233,  2888,\n",
       "          18311, 27702,  2716,  1121,  1103,   149, 26351,  1708,  6307,  1394,\n",
       "          16811,  1766,   117,  4000,  1103,   175, 14517,  7706,  1104,  1142,\n",
       "           3442,  1106, 13139,   170,   154,  2036,  4520,  1443, 25602,  2999,\n",
       "           2805,   119,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/torch/onnx/utils.py:90: UserWarning: 'enable_onnx_checker' is deprecated and ignored. It will be removed in the next PyTorch release. To proceed despite ONNX checker failures, catch torch.onnx.ONNXCheckerError.\n",
      "  warnings.warn(\"'enable_onnx_checker' is deprecated and ignored. It will be removed in \"\n",
      "/opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/torch/onnx/utils.py:103: UserWarning: `use_external_data_format' is deprecated and ignored. Will be removed in next PyTorch release. The code will work as it is False if models are not larger than 2GB, Otherwise set to False because of size limits imposed by Protocol Buffers.\n",
      "  warnings.warn(\"`use_external_data_format' is deprecated and ignored. Will be removed in next \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2508,  2225,  ...,     0,     0,     0],\n",
      "        [  101,  1130,  1251,  ...,     0,     0,     0],\n",
      "        [  101,  1188,  2526,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,   142,  2050,  ...,     0,     0,     0],\n",
      "        [  101, 11336, 16236,  ..., 23896,  7008,   102],\n",
      "        [  101,  5732, 10393,  ...,     0,     0,     0]])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/torch/onnx/symbolic_helper.py:325: UserWarning: Type cannot be inferred, which might cause exported graph to produce incorrect results.\n",
      "  warnings.warn(\"Type cannot be inferred, which might cause exported graph to produce incorrect results.\")\n",
      "[W shape_type_inference.cpp:434] Warning: Constant folding in symbolic shape inference fails: index_select(): Index is supposed to be a vector\n",
      "Exception raised from index_select_out_cpu_ at /opt/conda/conda-bld/pytorch-base_1645116610908/work/aten/src/ATen/native/TensorAdvancedIndexing.cpp:887 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x6a (0x7f4eea37c98a in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/torch/lib/libc10.so)\n",
      "frame #1: at::native::index_select_out_cpu_(at::Tensor const&, long, at::Tensor const&, at::Tensor&) + 0x3d0 (0x7f4f19adbd60 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: at::native::index_select_cpu_(at::Tensor const&, long, at::Tensor const&) + 0xf8 (0x7f4f19addb28 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: <unknown function> + 0x16e7ed7 (0x7f4f1a24fed7 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: at::_ops::index_select::redispatch(c10::DispatchKeySet, at::Tensor const&, long, at::Tensor const&) + 0xce (0x7f4f19d7c5ae in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: <unknown function> + 0x2c0752b (0x7f4f1b76f52b in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0x2c07ba9 (0x7f4f1b76fba9 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: at::_ops::index_select::call(at::Tensor const&, long, at::Tensor const&) + 0x17c (0x7f4f19e386ec in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #8: torch::jit::onnx_constant_fold::runTorchBackendForOnnx(torch::jit::Node const*, std::vector<at::Tensor, std::allocator<at::Tensor> >&, int) + 0x1b16 (0x7f4f1f9a6156 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #9: <unknown function> + 0xba9085 (0x7f4f1f9f5085 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #10: torch::jit::ONNXShapeTypeInference(torch::jit::Node*, std::map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, c10::IValue, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, c10::IValue> > > const&, int) + 0xa6e (0x7f4f1f9fc72e in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #11: <unknown function> + 0xbb324d (0x7f4f1f9ff24d in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #12: <unknown function> + 0xb3a387 (0x7f4f1f986387 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #13: <unknown function> + 0x1e6d34 (0x7f4f1f032d34 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/torch/lib/libtorch_python.so)\n",
      "<omitting python frames>\n",
      "frame #63: <unknown function> + 0x8225 (0x7f4fa418c225 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so)\n",
      " (function ComputeConstantFolding)\n",
      "[W shape_type_inference.cpp:434] Warning: Constant folding in symbolic shape inference fails: index_select(): Index is supposed to be a vector\n",
      "Exception raised from index_select_out_cpu_ at /opt/conda/conda-bld/pytorch-base_1645116610908/work/aten/src/ATen/native/TensorAdvancedIndexing.cpp:887 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x6a (0x7f4eea37c98a in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/torch/lib/libc10.so)\n",
      "frame #1: at::native::index_select_out_cpu_(at::Tensor const&, long, at::Tensor const&, at::Tensor&) + 0x3d0 (0x7f4f19adbd60 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: at::native::index_select_cpu_(at::Tensor const&, long, at::Tensor const&) + 0xf8 (0x7f4f19addb28 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: <unknown function> + 0x16e7ed7 (0x7f4f1a24fed7 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: at::_ops::index_select::redispatch(c10::DispatchKeySet, at::Tensor const&, long, at::Tensor const&) + 0xce (0x7f4f19d7c5ae in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: <unknown function> + 0x2c0752b (0x7f4f1b76f52b in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #6: <unknown function> + 0x2c07ba9 (0x7f4f1b76fba9 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #7: at::_ops::index_select::call(at::Tensor const&, long, at::Tensor const&) + 0x17c (0x7f4f19e386ec in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #8: torch::jit::onnx_constant_fold::runTorchBackendForOnnx(torch::jit::Node const*, std::vector<at::Tensor, std::allocator<at::Tensor> >&, int) + 0x1b16 (0x7f4f1f9a6156 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #9: <unknown function> + 0xba9085 (0x7f4f1f9f5085 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #10: torch::jit::ONNXShapeTypeInference(torch::jit::Node*, std::map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, c10::IValue, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, c10::IValue> > > const&, int) + 0xa6e (0x7f4f1f9fc72e in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #11: <unknown function> + 0xbb324d (0x7f4f1f9ff24d in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #12: <unknown function> + 0xb3a387 (0x7f4f1f986387 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #13: <unknown function> + 0x1e6d34 (0x7f4f1f032d34 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/torch/lib/libtorch_python.so)\n",
      "<omitting python frames>\n",
      "frame #60: <unknown function> + 0x8225 (0x7f4fa418c225 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so)\n",
      "frame #61: <unknown function> + 0x8ae9 (0x7f4fa418cae9 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so)\n",
      " (function ComputeConstantFolding)\n"
     ]
    }
   ],
   "source": [
    "filename = 'torch_mlp.onnx'\n",
    "tar_filename = filename + '.tgz'\n",
    "\n",
    "torch.onnx.export(\n",
    "        model,\n",
    "        args=tuple([input_sample[\"ids\"], input_sample[\"mask\"]]),\n",
    "        f=filename,\n",
    "        input_names=['ids', 'mask'],\n",
    "        do_constant_folding=False,\n",
    "        use_external_data_format=False,\n",
    "        enable_onnx_checker=True,\n",
    "        opset_version=12,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch_mlp.onnx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmdstring = 'tar -zcvf ' + tar_filename + ' ' + filename\n",
    "os.system(cmdstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset\t\t\t\t\t\t\t    torch_mlp.onnx\r\n",
      "deployment.onnx\t\t\t\t\t\t    torch_mlp.onnx.tgz\r\n",
      "snapshot-ArxivAbstractsClassifierPreprocessData5Epoch.ckpt  uni_df.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "sofware_spec_uid = client.software_specifications.get_id_by_name(\"pytorch-onnx_rt22.1-py3.9\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sofware_spec_uid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/wsuser/ipykernel_1016/4156337151.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m             \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepository\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelMetaNames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNAME\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Classifier deployment'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepository\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelMetaNames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTYPE\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'pytorch-onnx_1.10'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m             \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepository\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelMetaNames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOFTWARE_SPEC_UID\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msofware_spec_uid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sofware_spec_uid' is not defined"
     ]
    }
   ],
   "source": [
    "metadata = {\n",
    "            client.repository.ModelMetaNames.NAME: 'Classifier deployment',\n",
    "            client.repository.ModelMetaNames.TYPE: 'pytorch-onnx_1.10',\n",
    "            client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: sofware_spec_uid,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failure during creating model. (POST https://eu-gb.ml.cloud.ibm.com/ml/v4/models?version=2021-06-24&space_id=6f48a9aa-b319-450c-b6f1-bd3a789704d4)\n",
      "Status code: 400, body: {\n",
      "  \"trace\": \"24550975c4418f6d37bb926da522a5b6\",\n",
      "  \"errors\": [{\n",
      "    \"code\": \"malformed_json\",\n",
      "    \"message\": \"Malformed create model json payload: Object is missing required member 'fields'\",\n",
      "    \"target\": {\n",
      "      \"type\": \"field\",\n",
      "      \"name\": \"fields\"\n",
      "    },\n",
      "    \"more_info\": \"https://cloud.ibm.com/apidocs/machine-learning\"\n",
      "  }],\n",
      "  \"status_code\": \"400\"\n",
      "}\n"
     ]
    },
    {
     "ename": "ApiRequestFailure",
     "evalue": "Failure during creating model. (POST https://eu-gb.ml.cloud.ibm.com/ml/v4/models?version=2021-06-24&space_id=6f48a9aa-b319-450c-b6f1-bd3a789704d4)\nStatus code: 400, body: {\n  \"trace\": \"24550975c4418f6d37bb926da522a5b6\",\n  \"errors\": [{\n    \"code\": \"malformed_json\",\n    \"message\": \"Malformed create model json payload: Object is missing required member 'fields'\",\n    \"target\": {\n      \"type\": \"field\",\n      \"name\": \"fields\"\n    },\n    \"more_info\": \"https://cloud.ibm.com/apidocs/machine-learning\"\n  }],\n  \"status_code\": \"400\"\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mApiRequestFailure\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/wsuser/ipykernel_276/3187467804.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m published_model = client.repository.store_model(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtar_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     meta_props=metadata)\n",
      "\u001b[0;32m/opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/ibm_watson_machine_learning/repository.py\u001b[0m in \u001b[0;36mstore_model\u001b[0;34m(self, model, meta_props, training_data, training_target, pipeline, feature_names, label_column_names, subtrainingId)\u001b[0m\n\u001b[1;32m    409\u001b[0m             \"\"\"\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_props\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta_props\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_target\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_column_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_column_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msubtrainingId\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubtrainingId\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdocstring_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'str_type'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSTR_TYPE_NAME\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/ibm_watson_machine_learning/models.py\u001b[0m in \u001b[0;36mstore\u001b[0;34m(self, model, meta_props, training_data, training_target, pipeline, version, artifactid, feature_names, label_column_names, subtrainingId)\u001b[0m\n\u001b[1;32m   1522\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1523\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mWMLClientError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'Invalid path: neither file nor directory exists under this path: \\'{}\\'.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1524\u001b[0;31m                 \u001b[0msaved_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_publish_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_props\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta_props\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_target\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0martifactid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0martifactid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_column_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_column_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1525\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                  \u001b[0msaved_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_publish_from_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_uid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_props\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta_props\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_target\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0martifactId\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0martifactid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubtrainingId\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubtrainingId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_column_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_column_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/ibm_watson_machine_learning/models.py\u001b[0m in \u001b[0;36m_publish_from_file\u001b[0;34m(self, model, meta_props, training_data, training_target, ver, artifactid, feature_names, label_column_names)\u001b[0m\n\u001b[1;32m   1078\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wsd_publish_from_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_props\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_column_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_column_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_publish_from_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_props\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_column_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_column_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1081\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_h5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_filepath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfigurationMetaNames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTYPE\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmeta_props\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m                 \u001b[0mmeta_props\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfigurationMetaNames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTYPE\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tensorflow_2.1'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/ibm_watson_machine_learning/models.py\u001b[0m in \u001b[0;36m_publish_from_archive\u001b[0;34m(self, path_to_archive, meta_props, version, artifactid, feature_names, label_column_names)\u001b[0m\n\u001b[1;32m   1275\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m             )\n\u001b[0;32m-> 1277\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m201\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'creating model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m             \u001b[0mmodel_uid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_required_element_from_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_details'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'metadata'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/ibm_watson_machine_learning/wml_resource.py\u001b[0m in \u001b[0;36m_handle_response\u001b[0;34m(self, expected_status_code, operationName, response, json_response)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mApiRequestFailure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'Failure during {}.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperationName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mApiRequestFailure\u001b[0m: Failure during creating model. (POST https://eu-gb.ml.cloud.ibm.com/ml/v4/models?version=2021-06-24&space_id=6f48a9aa-b319-450c-b6f1-bd3a789704d4)\nStatus code: 400, body: {\n  \"trace\": \"24550975c4418f6d37bb926da522a5b6\",\n  \"errors\": [{\n    \"code\": \"malformed_json\",\n    \"message\": \"Malformed create model json payload: Object is missing required member 'fields'\",\n    \"target\": {\n      \"type\": \"field\",\n      \"name\": \"fields\"\n    },\n    \"more_info\": \"https://cloud.ibm.com/apidocs/machine-learning\"\n  }],\n  \"status_code\": \"400\"\n}"
     ]
    }
   ],
   "source": [
    "published_model = client.repository.store_model(\n",
    "    model=tar_filename,\n",
    "    meta_props=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This method is deprecated, please use get_model_id()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/ibm_watson_machine_learning/repository.py:1452: UserWarning: This method is deprecated, please use get_model_id()\n",
      "  warn(\"This method is deprecated, please use get_model_id()\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"entity\": {\n",
      "    \"hybrid_pipeline_software_specs\": [],\n",
      "    \"software_spec\": {\n",
      "      \"id\": \"0b848dd4-e681-5599-be41-b5f6fccc6471\",\n",
      "      \"name\": \"pytorch-onnx_rt22.1-py3.9\"\n",
      "    },\n",
      "    \"type\": \"pytorch-onnx_1.10\"\n",
      "  },\n",
      "  \"metadata\": {\n",
      "    \"created_at\": \"2022-06-12T13:02:46.447Z\",\n",
      "    \"id\": \"2c90decd-0dc0-41f7-bbca-e3952282785e\",\n",
      "    \"modified_at\": \"2022-06-12T13:03:23.298Z\",\n",
      "    \"name\": \"Classifier deployment\",\n",
      "    \"owner\": \"IBMid-6650023OBW\",\n",
      "    \"resource_key\": \"6a6047bf-b4bf-4097-8a8c-83250b35ca59\",\n",
      "    \"space_id\": \"6f48a9aa-b319-450c-b6f1-bd3a789704d4\"\n",
      "  },\n",
      "  \"system\": {\n",
      "    \"warnings\": []\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "published_model_uid = client.repository.get_model_uid(published_model)\n",
    "model_details = client.repository.get_details(published_model_uid)\n",
    "print(json.dumps(model_details, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2c90decd-0dc0-41f7-bbca-e3952282785e'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "published_model_uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "#######################################################################################\n",
      "\n",
      "Synchronous deployment creation for uid: '2c90decd-0dc0-41f7-bbca-e3952282785e' started\n",
      "\n",
      "#######################################################################################\n",
      "\n",
      "\n",
      "initializing\n",
      "Note: online_url is deprecated and will be removed in a future release. Use serving_urls instead.\n",
      "...\n",
      "ready\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "Successfully finished deployment creation, deployment_uid='1e3114a8-13ff-4ed1-b2f1-eb731af71e95'\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metadata = {\n",
    "    client.deployments.ConfigurationMetaNames.NAME: \"Deployment of external pytorch model\",\n",
    "    client.deployments.ConfigurationMetaNames.ONLINE: {}\n",
    "}\n",
    "\n",
    "created_deployment = client.deployments.create(published_model_uid, meta_props=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_id = \"1e3114a8-13ff-4ed1-b2f1-eb731af71e95\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "inps = pd.DataFrame([{\"ids\": input_sample[\"ids\"].tolist(), \"mask\": input_sample[\"mask\"].tolist()}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\"ids\", \"mask\"]\n",
    "values = [[input_sample[\"ids\"].numpy()[0], input_sample[\"mask\"].numpy()[0]]]\n",
    "\n",
    "scoring_payload = {\n",
    "client.deployments.ScoringMetaNames.INPUT_DATA: [{  \n",
    "    \"values\": [\n",
    "        {\"ids\": input_sample[\"ids\"].tolist(), \"mask\": input_sample[\"mask\"].tolist()}\n",
    "    ]\n",
    " }]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------  ---------------------  ------------------------  -----------------\n",
      "ID                                    NAME                   CREATED                   TYPE\n",
      "2c90decd-0dc0-41f7-bbca-e3952282785e  Classifier deployment  2022-06-12T13:02:46.002Z  pytorch-onnx_1.10\n",
      "075da344-3252-4b3c-9dc6-c8c190df1434  Classifier deployment  2022-06-12T12:51:07.002Z  pytorch-onnx_1.10\n",
      "0856adc0-edb9-4221-863e-1028b302802a  Classifier deployment  2022-06-12T11:54:13.002Z  pytorch-onnx_1.10\n",
      "1467e23d-ceb2-4800-8b53-5a8fa62a2e60  Classifier deployment  2022-06-12T11:53:31.002Z  pytorch-onnx_1.7\n",
      "------------------------------------  ---------------------  ------------------------  -----------------\n"
     ]
    }
   ],
   "source": [
    "client.repository.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Deployments' object has no attribute 'get_scoring_url'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/wsuser/ipykernel_1016/792989936.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeployments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_scoring_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeployment_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Deployments' object has no attribute 'get_scoring_url'"
     ]
    }
   ],
   "source": [
    "client.deployments.get_scoring_url(deployment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failure during scoring. (POST https://eu-gb.ml.cloud.ibm.com/ml/v4/deployments/1e3114a8-13ff-4ed1-b2f1-eb731af71e95/predictions?version=2021-06-24)\n",
      "Status code: 400, body: {\"trace\": \"210a67ca7df30f943a55d566286ad1d3\", \"errors\": [{\"code\": \"invalid_input_data\", \"message\": \"Incorrect input data: int() argument must be a string, a bytes-like object or a number, not 'dict'\"}], \"status_code\": 400}\n",
      "\n"
     ]
    },
    {
     "ename": "ApiRequestFailure",
     "evalue": "Failure during scoring. (POST https://eu-gb.ml.cloud.ibm.com/ml/v4/deployments/1e3114a8-13ff-4ed1-b2f1-eb731af71e95/predictions?version=2021-06-24)\nStatus code: 400, body: {\"trace\": \"210a67ca7df30f943a55d566286ad1d3\", \"errors\": [{\"code\": \"invalid_input_data\", \"message\": \"Incorrect input data: int() argument must be a string, a bytes-like object or a number, not 'dict'\"}], \"status_code\": 400}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mApiRequestFailure\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/wsuser/ipykernel_1016/1276325531.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeployments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeployment_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring_payload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/ibm_watson_machine_learning/deployments.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, deployment_id, meta_props, transaction_id)\u001b[0m\n\u001b[1;32m    751\u001b[0m                 headers=headers)\n\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'scoring'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_scoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0;31m#########################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/ibm_watson_machine_learning/wml_resource.py\u001b[0m in \u001b[0;36m_handle_response\u001b[0;34m(self, expected_status_code, operationName, response, json_response)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mApiRequestFailure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'Failure during {}.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperationName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mApiRequestFailure\u001b[0m: Failure during scoring. (POST https://eu-gb.ml.cloud.ibm.com/ml/v4/deployments/1e3114a8-13ff-4ed1-b2f1-eb731af71e95/predictions?version=2021-06-24)\nStatus code: 400, body: {\"trace\": \"210a67ca7df30f943a55d566286ad1d3\", \"errors\": [{\"code\": \"invalid_input_data\", \"message\": \"Incorrect input data: int() argument must be a string, a bytes-like object or a number, not 'dict'\"}], \"status_code\": 400}\n"
     ]
    }
   ],
   "source": [
    "predictions = client.deployments.score(deployment_id, scoring_payload)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "model = onnx.load(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx.checker.check_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"ids\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 7\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 16\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 384\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"mask\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 7\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 16\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 384\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.graph.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph torch-jit-export (\n",
      "  %ids[INT64, 16x384]\n",
      "  %mask[INT64, 16x384]\n",
      ") initializers (\n",
      "  %bert.embeddings.position_ids[INT64, 1x512]\n",
      "  %bert.embeddings.word_embeddings.weight[FLOAT, 28996x768]\n",
      "  %bert.embeddings.position_embeddings.weight[FLOAT, 512x768]\n",
      "  %bert.embeddings.token_type_embeddings.weight[FLOAT, 2x768]\n",
      "  %bert.embeddings.LayerNorm.weight[FLOAT, 768]\n",
      "  %bert.embeddings.LayerNorm.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.0.attention.self.query.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.0.attention.self.query.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.0.attention.self.key.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.0.attention.self.key.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.0.attention.self.value.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.0.attention.self.value.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.0.attention.output.dense.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.0.attention.output.dense.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.0.attention.output.LayerNorm.weight[FLOAT, 768]\n",
      "  %bert.encoder.layer.0.attention.output.LayerNorm.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.0.intermediate.dense.weight[FLOAT, 3072x768]\n",
      "  %bert.encoder.layer.0.intermediate.dense.bias[FLOAT, 3072]\n",
      "  %bert.encoder.layer.0.output.dense.weight[FLOAT, 768x3072]\n",
      "  %bert.encoder.layer.0.output.dense.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.0.output.LayerNorm.weight[FLOAT, 768]\n",
      "  %bert.encoder.layer.0.output.LayerNorm.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.1.attention.self.query.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.1.attention.self.query.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.1.attention.self.key.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.1.attention.self.key.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.1.attention.self.value.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.1.attention.self.value.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.1.attention.output.dense.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.1.attention.output.dense.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.1.attention.output.LayerNorm.weight[FLOAT, 768]\n",
      "  %bert.encoder.layer.1.attention.output.LayerNorm.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.1.intermediate.dense.weight[FLOAT, 3072x768]\n",
      "  %bert.encoder.layer.1.intermediate.dense.bias[FLOAT, 3072]\n",
      "  %bert.encoder.layer.1.output.dense.weight[FLOAT, 768x3072]\n",
      "  %bert.encoder.layer.1.output.dense.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.1.output.LayerNorm.weight[FLOAT, 768]\n",
      "  %bert.encoder.layer.1.output.LayerNorm.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.2.attention.self.query.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.2.attention.self.query.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.2.attention.self.key.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.2.attention.self.key.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.2.attention.self.value.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.2.attention.self.value.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.2.attention.output.dense.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.2.attention.output.dense.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.2.attention.output.LayerNorm.weight[FLOAT, 768]\n",
      "  %bert.encoder.layer.2.attention.output.LayerNorm.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.2.intermediate.dense.weight[FLOAT, 3072x768]\n",
      "  %bert.encoder.layer.2.intermediate.dense.bias[FLOAT, 3072]\n",
      "  %bert.encoder.layer.2.output.dense.weight[FLOAT, 768x3072]\n",
      "  %bert.encoder.layer.2.output.dense.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.2.output.LayerNorm.weight[FLOAT, 768]\n",
      "  %bert.encoder.layer.2.output.LayerNorm.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.3.attention.self.query.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.3.attention.self.query.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.3.attention.self.key.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.3.attention.self.key.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.3.attention.self.value.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.3.attention.self.value.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.3.attention.output.dense.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.3.attention.output.dense.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.3.attention.output.LayerNorm.weight[FLOAT, 768]\n",
      "  %bert.encoder.layer.3.attention.output.LayerNorm.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.3.intermediate.dense.weight[FLOAT, 3072x768]\n",
      "  %bert.encoder.layer.3.intermediate.dense.bias[FLOAT, 3072]\n",
      "  %bert.encoder.layer.3.output.dense.weight[FLOAT, 768x3072]\n",
      "  %bert.encoder.layer.3.output.dense.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.3.output.LayerNorm.weight[FLOAT, 768]\n",
      "  %bert.encoder.layer.3.output.LayerNorm.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.4.attention.self.query.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.4.attention.self.query.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.4.attention.self.key.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.4.attention.self.key.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.4.attention.self.value.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.4.attention.self.value.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.4.attention.output.dense.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.4.attention.output.dense.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.4.attention.output.LayerNorm.weight[FLOAT, 768]\n",
      "  %bert.encoder.layer.4.attention.output.LayerNorm.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.4.intermediate.dense.weight[FLOAT, 3072x768]\n",
      "  %bert.encoder.layer.4.intermediate.dense.bias[FLOAT, 3072]\n",
      "  %bert.encoder.layer.4.output.dense.weight[FLOAT, 768x3072]\n",
      "  %bert.encoder.layer.4.output.dense.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.4.output.LayerNorm.weight[FLOAT, 768]\n",
      "  %bert.encoder.layer.4.output.LayerNorm.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.5.attention.self.query.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.5.attention.self.query.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.5.attention.self.key.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.5.attention.self.key.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.5.attention.self.value.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.5.attention.self.value.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.5.attention.output.dense.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.5.attention.output.dense.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.5.attention.output.LayerNorm.weight[FLOAT, 768]\n",
      "  %bert.encoder.layer.5.attention.output.LayerNorm.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.5.intermediate.dense.weight[FLOAT, 3072x768]\n",
      "  %bert.encoder.layer.5.intermediate.dense.bias[FLOAT, 3072]\n",
      "  %bert.encoder.layer.5.output.dense.weight[FLOAT, 768x3072]\n",
      "  %bert.encoder.layer.5.output.dense.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.5.output.LayerNorm.weight[FLOAT, 768]\n",
      "  %bert.encoder.layer.5.output.LayerNorm.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.6.attention.self.query.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.6.attention.self.query.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.6.attention.self.key.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.6.attention.self.key.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.6.attention.self.value.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.6.attention.self.value.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.6.attention.output.dense.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.6.attention.output.dense.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.6.attention.output.LayerNorm.weight[FLOAT, 768]\n",
      "  %bert.encoder.layer.6.attention.output.LayerNorm.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.6.intermediate.dense.weight[FLOAT, 3072x768]\n",
      "  %bert.encoder.layer.6.intermediate.dense.bias[FLOAT, 3072]\n",
      "  %bert.encoder.layer.6.output.dense.weight[FLOAT, 768x3072]\n",
      "  %bert.encoder.layer.6.output.dense.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.6.output.LayerNorm.weight[FLOAT, 768]\n",
      "  %bert.encoder.layer.6.output.LayerNorm.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.7.attention.self.query.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.7.attention.self.query.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.7.attention.self.key.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.7.attention.self.key.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.7.attention.self.value.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.7.attention.self.value.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.7.attention.output.dense.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.7.attention.output.dense.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.7.attention.output.LayerNorm.weight[FLOAT, 768]\n",
      "  %bert.encoder.layer.7.attention.output.LayerNorm.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.7.intermediate.dense.weight[FLOAT, 3072x768]\n",
      "  %bert.encoder.layer.7.intermediate.dense.bias[FLOAT, 3072]\n",
      "  %bert.encoder.layer.7.output.dense.weight[FLOAT, 768x3072]\n",
      "  %bert.encoder.layer.7.output.dense.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.7.output.LayerNorm.weight[FLOAT, 768]\n",
      "  %bert.encoder.layer.7.output.LayerNorm.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.8.attention.self.query.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.8.attention.self.query.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.8.attention.self.key.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.8.attention.self.key.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.8.attention.self.value.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.8.attention.self.value.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.8.attention.output.dense.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.8.attention.output.dense.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.8.attention.output.LayerNorm.weight[FLOAT, 768]\n",
      "  %bert.encoder.layer.8.attention.output.LayerNorm.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.8.intermediate.dense.weight[FLOAT, 3072x768]\n",
      "  %bert.encoder.layer.8.intermediate.dense.bias[FLOAT, 3072]\n",
      "  %bert.encoder.layer.8.output.dense.weight[FLOAT, 768x3072]\n",
      "  %bert.encoder.layer.8.output.dense.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.8.output.LayerNorm.weight[FLOAT, 768]\n",
      "  %bert.encoder.layer.8.output.LayerNorm.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.9.attention.self.query.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.9.attention.self.query.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.9.attention.self.key.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.9.attention.self.key.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.9.attention.self.value.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.9.attention.self.value.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.9.attention.output.dense.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.9.attention.output.dense.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.9.attention.output.LayerNorm.weight[FLOAT, 768]\n",
      "  %bert.encoder.layer.9.attention.output.LayerNorm.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.9.intermediate.dense.weight[FLOAT, 3072x768]\n",
      "  %bert.encoder.layer.9.intermediate.dense.bias[FLOAT, 3072]\n",
      "  %bert.encoder.layer.9.output.dense.weight[FLOAT, 768x3072]\n",
      "  %bert.encoder.layer.9.output.dense.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.9.output.LayerNorm.weight[FLOAT, 768]\n",
      "  %bert.encoder.layer.9.output.LayerNorm.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.10.attention.self.query.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.10.attention.self.query.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.10.attention.self.key.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.10.attention.self.key.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.10.attention.self.value.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.10.attention.self.value.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.10.attention.output.dense.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.10.attention.output.dense.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.10.attention.output.LayerNorm.weight[FLOAT, 768]\n",
      "  %bert.encoder.layer.10.attention.output.LayerNorm.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.10.intermediate.dense.weight[FLOAT, 3072x768]\n",
      "  %bert.encoder.layer.10.intermediate.dense.bias[FLOAT, 3072]\n",
      "  %bert.encoder.layer.10.output.dense.weight[FLOAT, 768x3072]\n",
      "  %bert.encoder.layer.10.output.dense.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.10.output.LayerNorm.weight[FLOAT, 768]\n",
      "  %bert.encoder.layer.10.output.LayerNorm.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.11.attention.self.query.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.11.attention.self.query.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.11.attention.self.key.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.11.attention.self.key.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.11.attention.self.value.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.11.attention.self.value.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.11.attention.output.dense.weight[FLOAT, 768x768]\n",
      "  %bert.encoder.layer.11.attention.output.dense.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.11.attention.output.LayerNorm.weight[FLOAT, 768]\n",
      "  %bert.encoder.layer.11.attention.output.LayerNorm.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.11.intermediate.dense.weight[FLOAT, 3072x768]\n",
      "  %bert.encoder.layer.11.intermediate.dense.bias[FLOAT, 3072]\n",
      "  %bert.encoder.layer.11.output.dense.weight[FLOAT, 768x3072]\n",
      "  %bert.encoder.layer.11.output.dense.bias[FLOAT, 768]\n",
      "  %bert.encoder.layer.11.output.LayerNorm.weight[FLOAT, 768]\n",
      "  %bert.encoder.layer.11.output.LayerNorm.bias[FLOAT, 768]\n",
      "  %bert.pooler.dense.weight[FLOAT, 768x768]\n",
      "  %bert.pooler.dense.bias[FLOAT, 768]\n",
      "  %classifier.weight[FLOAT, 151x768]\n",
      "  %classifier.bias[FLOAT, 151]\n",
      ") {\n",
      "  %204 = Constant[value = <Scalar Tensor []>]()\n",
      "  %205 = Constant[value = <Scalar Tensor []>]()\n",
      "  %206 = Constant[value = <Tensor>]()\n",
      "  %207 = Unsqueeze[axes = [0]](%204)\n",
      "  %208 = Unsqueeze[axes = [0]](%205)\n",
      "  %209 = Concat[axis = 0](%207, %208)\n",
      "  %210 = Constant[value = <Tensor>]()\n",
      "  %211 = Reshape(%209, %210)\n",
      "  %212 = Shape(%211)\n",
      "  %213 = ConstantOfShape[value = <Tensor>](%212)\n",
      "  %214 = Constant[value = <Scalar Tensor []>]()\n",
      "  %215 = Mul(%213, %214)\n",
      "  %216 = Equal(%211, %215)\n",
      "  %217 = Where(%216, %213, %211)\n",
      "  %218 = Expand(%206, %217)\n",
      "  %219 = Unsqueeze[axes = [1]](%mask)\n",
      "  %220 = Unsqueeze[axes = [2]](%219)\n",
      "  %221 = Cast[to = 1](%220)\n",
      "  %222 = Constant[value = <Scalar Tensor []>]()\n",
      "  %223 = Sub(%222, %221)\n",
      "  %224 = Constant[value = <Scalar Tensor []>]()\n",
      "  %225 = Mul(%223, %224)\n",
      "  %226 = Constant[value = <Tensor>]()\n",
      "  %227 = Constant[value = <Tensor>]()\n",
      "  %228 = Constant[value = <Tensor>]()\n",
      "  %229 = Constant[value = <Tensor>]()\n",
      "  %230 = Slice(%bert.embeddings.position_ids, %227, %228, %226, %229)\n",
      "  %231 = Gather(%bert.embeddings.word_embeddings.weight, %ids)\n",
      "  %232 = Gather(%bert.embeddings.token_type_embeddings.weight, %218)\n",
      "  %233 = Add(%231, %232)\n",
      "  %234 = Gather(%bert.embeddings.position_embeddings.weight, %230)\n",
      "  %235 = Add(%233, %234)\n",
      "  %236 = ReduceMean[axes = [-1]](%235)\n",
      "  %237 = Sub(%235, %236)\n",
      "  %238 = Cast[to = 1](%237)\n",
      "  %239 = Constant[value = <Scalar Tensor []>]()\n",
      "  %240 = Pow(%238, %239)\n",
      "  %241 = ReduceMean[axes = [-1]](%240)\n",
      "  %242 = Constant[value = <Scalar Tensor []>]()\n",
      "  %243 = Add(%241, %242)\n",
      "  %244 = Sqrt(%243)\n",
      "  %245 = Div(%237, %244)\n",
      "  %246 = Mul(%245, %bert.embeddings.LayerNorm.weight)\n",
      "  %247 = Add(%246, %bert.embeddings.LayerNorm.bias)\n",
      "  %248 = Transpose[perm = [1, 0]](%bert.encoder.layer.0.attention.self.query.weight)\n",
      "  %249 = MatMul(%247, %248)\n",
      "  %250 = Add(%bert.encoder.layer.0.attention.self.query.bias, %249)\n",
      "  %251 = Transpose[perm = [1, 0]](%bert.encoder.layer.0.attention.self.key.weight)\n",
      "  %252 = MatMul(%247, %251)\n",
      "  %253 = Add(%bert.encoder.layer.0.attention.self.key.bias, %252)\n",
      "  %254 = Constant[value = <Scalar Tensor []>]()\n",
      "  %255 = Constant[value = <Scalar Tensor []>]()\n",
      "  %256 = Constant[value = <Scalar Tensor []>]()\n",
      "  %257 = Constant[value = <Scalar Tensor []>]()\n",
      "  %258 = Unsqueeze[axes = [0]](%254)\n",
      "  %259 = Unsqueeze[axes = [0]](%255)\n",
      "  %260 = Unsqueeze[axes = [0]](%256)\n",
      "  %261 = Unsqueeze[axes = [0]](%257)\n",
      "  %262 = Concat[axis = 0](%258, %259, %260, %261)\n",
      "  %263 = Reshape(%253, %262)\n",
      "  %264 = Transpose[perm = [1, 0]](%bert.encoder.layer.0.attention.self.value.weight)\n",
      "  %265 = MatMul(%247, %264)\n",
      "  %266 = Add(%bert.encoder.layer.0.attention.self.value.bias, %265)\n",
      "  %267 = Constant[value = <Scalar Tensor []>]()\n",
      "  %268 = Constant[value = <Scalar Tensor []>]()\n",
      "  %269 = Constant[value = <Scalar Tensor []>]()\n",
      "  %270 = Constant[value = <Scalar Tensor []>]()\n",
      "  %271 = Unsqueeze[axes = [0]](%267)\n",
      "  %272 = Unsqueeze[axes = [0]](%268)\n",
      "  %273 = Unsqueeze[axes = [0]](%269)\n",
      "  %274 = Unsqueeze[axes = [0]](%270)\n",
      "  %275 = Concat[axis = 0](%271, %272, %273, %274)\n",
      "  %276 = Reshape(%266, %275)\n",
      "  %277 = Transpose[perm = [0, 2, 1, 3]](%276)\n",
      "  %278 = Constant[value = <Scalar Tensor []>]()\n",
      "  %279 = Constant[value = <Scalar Tensor []>]()\n",
      "  %280 = Constant[value = <Scalar Tensor []>]()\n",
      "  %281 = Constant[value = <Scalar Tensor []>]()\n",
      "  %282 = Unsqueeze[axes = [0]](%278)\n",
      "  %283 = Unsqueeze[axes = [0]](%279)\n",
      "  %284 = Unsqueeze[axes = [0]](%280)\n",
      "  %285 = Unsqueeze[axes = [0]](%281)\n",
      "  %286 = Concat[axis = 0](%282, %283, %284, %285)\n",
      "  %287 = Reshape(%250, %286)\n",
      "  %288 = Transpose[perm = [0, 2, 1, 3]](%287)\n",
      "  %289 = Transpose[perm = [0, 2, 3, 1]](%263)\n",
      "  %290 = MatMul(%288, %289)\n",
      "  %291 = Constant[value = <Scalar Tensor []>]()\n",
      "  %292 = Div(%290, %291)\n",
      "  %293 = Add(%292, %225)\n",
      "  %294 = Softmax[axis = 3](%293)\n",
      "  %295 = MatMul(%294, %277)\n",
      "  %296 = Transpose[perm = [0, 2, 1, 3]](%295)\n",
      "  %297 = Constant[value = <Scalar Tensor []>]()\n",
      "  %298 = Constant[value = <Scalar Tensor []>]()\n",
      "  %299 = Constant[value = <Scalar Tensor []>]()\n",
      "  %300 = Unsqueeze[axes = [0]](%297)\n",
      "  %301 = Unsqueeze[axes = [0]](%298)\n",
      "  %302 = Unsqueeze[axes = [0]](%299)\n",
      "  %303 = Concat[axis = 0](%300, %301, %302)\n",
      "  %304 = Reshape(%296, %303)\n",
      "  %305 = Transpose[perm = [1, 0]](%bert.encoder.layer.0.attention.output.dense.weight)\n",
      "  %306 = MatMul(%304, %305)\n",
      "  %307 = Add(%bert.encoder.layer.0.attention.output.dense.bias, %306)\n",
      "  %308 = Add(%307, %247)\n",
      "  %309 = ReduceMean[axes = [-1]](%308)\n",
      "  %310 = Sub(%308, %309)\n",
      "  %311 = Cast[to = 1](%310)\n",
      "  %312 = Constant[value = <Scalar Tensor []>]()\n",
      "  %313 = Pow(%311, %312)\n",
      "  %314 = ReduceMean[axes = [-1]](%313)\n",
      "  %315 = Constant[value = <Scalar Tensor []>]()\n",
      "  %316 = Add(%314, %315)\n",
      "  %317 = Sqrt(%316)\n",
      "  %318 = Div(%310, %317)\n",
      "  %319 = Mul(%318, %bert.encoder.layer.0.attention.output.LayerNorm.weight)\n",
      "  %320 = Add(%319, %bert.encoder.layer.0.attention.output.LayerNorm.bias)\n",
      "  %321 = Transpose[perm = [1, 0]](%bert.encoder.layer.0.intermediate.dense.weight)\n",
      "  %322 = MatMul(%320, %321)\n",
      "  %323 = Add(%bert.encoder.layer.0.intermediate.dense.bias, %322)\n",
      "  %324 = Constant[value = <Scalar Tensor []>]()\n",
      "  %325 = Div(%323, %324)\n",
      "  %326 = Erf(%325)\n",
      "  %327 = Constant[value = <Scalar Tensor []>]()\n",
      "  %328 = Add(%326, %327)\n",
      "  %329 = Mul(%323, %328)\n",
      "  %330 = Constant[value = <Scalar Tensor []>]()\n",
      "  %331 = Mul(%329, %330)\n",
      "  %332 = Transpose[perm = [1, 0]](%bert.encoder.layer.0.output.dense.weight)\n",
      "  %333 = MatMul(%331, %332)\n",
      "  %334 = Add(%bert.encoder.layer.0.output.dense.bias, %333)\n",
      "  %335 = Add(%334, %320)\n",
      "  %336 = ReduceMean[axes = [-1]](%335)\n",
      "  %337 = Sub(%335, %336)\n",
      "  %338 = Cast[to = 1](%337)\n",
      "  %339 = Constant[value = <Scalar Tensor []>]()\n",
      "  %340 = Pow(%338, %339)\n",
      "  %341 = ReduceMean[axes = [-1]](%340)\n",
      "  %342 = Constant[value = <Scalar Tensor []>]()\n",
      "  %343 = Add(%341, %342)\n",
      "  %344 = Sqrt(%343)\n",
      "  %345 = Div(%337, %344)\n",
      "  %346 = Mul(%345, %bert.encoder.layer.0.output.LayerNorm.weight)\n",
      "  %347 = Add(%346, %bert.encoder.layer.0.output.LayerNorm.bias)\n",
      "  %348 = Transpose[perm = [1, 0]](%bert.encoder.layer.1.attention.self.query.weight)\n",
      "  %349 = MatMul(%347, %348)\n",
      "  %350 = Add(%bert.encoder.layer.1.attention.self.query.bias, %349)\n",
      "  %351 = Transpose[perm = [1, 0]](%bert.encoder.layer.1.attention.self.key.weight)\n",
      "  %352 = MatMul(%347, %351)\n",
      "  %353 = Add(%bert.encoder.layer.1.attention.self.key.bias, %352)\n",
      "  %354 = Constant[value = <Scalar Tensor []>]()\n",
      "  %355 = Constant[value = <Scalar Tensor []>]()\n",
      "  %356 = Constant[value = <Scalar Tensor []>]()\n",
      "  %357 = Constant[value = <Scalar Tensor []>]()\n",
      "  %358 = Unsqueeze[axes = [0]](%354)\n",
      "  %359 = Unsqueeze[axes = [0]](%355)\n",
      "  %360 = Unsqueeze[axes = [0]](%356)\n",
      "  %361 = Unsqueeze[axes = [0]](%357)\n",
      "  %362 = Concat[axis = 0](%358, %359, %360, %361)\n",
      "  %363 = Reshape(%353, %362)\n",
      "  %364 = Transpose[perm = [1, 0]](%bert.encoder.layer.1.attention.self.value.weight)\n",
      "  %365 = MatMul(%347, %364)\n",
      "  %366 = Add(%bert.encoder.layer.1.attention.self.value.bias, %365)\n",
      "  %367 = Constant[value = <Scalar Tensor []>]()\n",
      "  %368 = Constant[value = <Scalar Tensor []>]()\n",
      "  %369 = Constant[value = <Scalar Tensor []>]()\n",
      "  %370 = Constant[value = <Scalar Tensor []>]()\n",
      "  %371 = Unsqueeze[axes = [0]](%367)\n",
      "  %372 = Unsqueeze[axes = [0]](%368)\n",
      "  %373 = Unsqueeze[axes = [0]](%369)\n",
      "  %374 = Unsqueeze[axes = [0]](%370)\n",
      "  %375 = Concat[axis = 0](%371, %372, %373, %374)\n",
      "  %376 = Reshape(%366, %375)\n",
      "  %377 = Transpose[perm = [0, 2, 1, 3]](%376)\n",
      "  %378 = Constant[value = <Scalar Tensor []>]()\n",
      "  %379 = Constant[value = <Scalar Tensor []>]()\n",
      "  %380 = Constant[value = <Scalar Tensor []>]()\n",
      "  %381 = Constant[value = <Scalar Tensor []>]()\n",
      "  %382 = Unsqueeze[axes = [0]](%378)\n",
      "  %383 = Unsqueeze[axes = [0]](%379)\n",
      "  %384 = Unsqueeze[axes = [0]](%380)\n",
      "  %385 = Unsqueeze[axes = [0]](%381)\n",
      "  %386 = Concat[axis = 0](%382, %383, %384, %385)\n",
      "  %387 = Reshape(%350, %386)\n",
      "  %388 = Transpose[perm = [0, 2, 1, 3]](%387)\n",
      "  %389 = Transpose[perm = [0, 2, 3, 1]](%363)\n",
      "  %390 = MatMul(%388, %389)\n",
      "  %391 = Constant[value = <Scalar Tensor []>]()\n",
      "  %392 = Div(%390, %391)\n",
      "  %393 = Add(%392, %225)\n",
      "  %394 = Softmax[axis = 3](%393)\n",
      "  %395 = MatMul(%394, %377)\n",
      "  %396 = Transpose[perm = [0, 2, 1, 3]](%395)\n",
      "  %397 = Constant[value = <Scalar Tensor []>]()\n",
      "  %398 = Constant[value = <Scalar Tensor []>]()\n",
      "  %399 = Constant[value = <Scalar Tensor []>]()\n",
      "  %400 = Unsqueeze[axes = [0]](%397)\n",
      "  %401 = Unsqueeze[axes = [0]](%398)\n",
      "  %402 = Unsqueeze[axes = [0]](%399)\n",
      "  %403 = Concat[axis = 0](%400, %401, %402)\n",
      "  %404 = Reshape(%396, %403)\n",
      "  %405 = Transpose[perm = [1, 0]](%bert.encoder.layer.1.attention.output.dense.weight)\n",
      "  %406 = MatMul(%404, %405)\n",
      "  %407 = Add(%bert.encoder.layer.1.attention.output.dense.bias, %406)\n",
      "  %408 = Add(%407, %347)\n",
      "  %409 = ReduceMean[axes = [-1]](%408)\n",
      "  %410 = Sub(%408, %409)\n",
      "  %411 = Cast[to = 1](%410)\n",
      "  %412 = Constant[value = <Scalar Tensor []>]()\n",
      "  %413 = Pow(%411, %412)\n",
      "  %414 = ReduceMean[axes = [-1]](%413)\n",
      "  %415 = Constant[value = <Scalar Tensor []>]()\n",
      "  %416 = Add(%414, %415)\n",
      "  %417 = Sqrt(%416)\n",
      "  %418 = Div(%410, %417)\n",
      "  %419 = Mul(%418, %bert.encoder.layer.1.attention.output.LayerNorm.weight)\n",
      "  %420 = Add(%419, %bert.encoder.layer.1.attention.output.LayerNorm.bias)\n",
      "  %421 = Transpose[perm = [1, 0]](%bert.encoder.layer.1.intermediate.dense.weight)\n",
      "  %422 = MatMul(%420, %421)\n",
      "  %423 = Add(%bert.encoder.layer.1.intermediate.dense.bias, %422)\n",
      "  %424 = Constant[value = <Scalar Tensor []>]()\n",
      "  %425 = Div(%423, %424)\n",
      "  %426 = Erf(%425)\n",
      "  %427 = Constant[value = <Scalar Tensor []>]()\n",
      "  %428 = Add(%426, %427)\n",
      "  %429 = Mul(%423, %428)\n",
      "  %430 = Constant[value = <Scalar Tensor []>]()\n",
      "  %431 = Mul(%429, %430)\n",
      "  %432 = Transpose[perm = [1, 0]](%bert.encoder.layer.1.output.dense.weight)\n",
      "  %433 = MatMul(%431, %432)\n",
      "  %434 = Add(%bert.encoder.layer.1.output.dense.bias, %433)\n",
      "  %435 = Add(%434, %420)\n",
      "  %436 = ReduceMean[axes = [-1]](%435)\n",
      "  %437 = Sub(%435, %436)\n",
      "  %438 = Cast[to = 1](%437)\n",
      "  %439 = Constant[value = <Scalar Tensor []>]()\n",
      "  %440 = Pow(%438, %439)\n",
      "  %441 = ReduceMean[axes = [-1]](%440)\n",
      "  %442 = Constant[value = <Scalar Tensor []>]()\n",
      "  %443 = Add(%441, %442)\n",
      "  %444 = Sqrt(%443)\n",
      "  %445 = Div(%437, %444)\n",
      "  %446 = Mul(%445, %bert.encoder.layer.1.output.LayerNorm.weight)\n",
      "  %447 = Add(%446, %bert.encoder.layer.1.output.LayerNorm.bias)\n",
      "  %448 = Transpose[perm = [1, 0]](%bert.encoder.layer.2.attention.self.query.weight)\n",
      "  %449 = MatMul(%447, %448)\n",
      "  %450 = Add(%bert.encoder.layer.2.attention.self.query.bias, %449)\n",
      "  %451 = Transpose[perm = [1, 0]](%bert.encoder.layer.2.attention.self.key.weight)\n",
      "  %452 = MatMul(%447, %451)\n",
      "  %453 = Add(%bert.encoder.layer.2.attention.self.key.bias, %452)\n",
      "  %454 = Constant[value = <Scalar Tensor []>]()\n",
      "  %455 = Constant[value = <Scalar Tensor []>]()\n",
      "  %456 = Constant[value = <Scalar Tensor []>]()\n",
      "  %457 = Constant[value = <Scalar Tensor []>]()\n",
      "  %458 = Unsqueeze[axes = [0]](%454)\n",
      "  %459 = Unsqueeze[axes = [0]](%455)\n",
      "  %460 = Unsqueeze[axes = [0]](%456)\n",
      "  %461 = Unsqueeze[axes = [0]](%457)\n",
      "  %462 = Concat[axis = 0](%458, %459, %460, %461)\n",
      "  %463 = Reshape(%453, %462)\n",
      "  %464 = Transpose[perm = [1, 0]](%bert.encoder.layer.2.attention.self.value.weight)\n",
      "  %465 = MatMul(%447, %464)\n",
      "  %466 = Add(%bert.encoder.layer.2.attention.self.value.bias, %465)\n",
      "  %467 = Constant[value = <Scalar Tensor []>]()\n",
      "  %468 = Constant[value = <Scalar Tensor []>]()\n",
      "  %469 = Constant[value = <Scalar Tensor []>]()\n",
      "  %470 = Constant[value = <Scalar Tensor []>]()\n",
      "  %471 = Unsqueeze[axes = [0]](%467)\n",
      "  %472 = Unsqueeze[axes = [0]](%468)\n",
      "  %473 = Unsqueeze[axes = [0]](%469)\n",
      "  %474 = Unsqueeze[axes = [0]](%470)\n",
      "  %475 = Concat[axis = 0](%471, %472, %473, %474)\n",
      "  %476 = Reshape(%466, %475)\n",
      "  %477 = Transpose[perm = [0, 2, 1, 3]](%476)\n",
      "  %478 = Constant[value = <Scalar Tensor []>]()\n",
      "  %479 = Constant[value = <Scalar Tensor []>]()\n",
      "  %480 = Constant[value = <Scalar Tensor []>]()\n",
      "  %481 = Constant[value = <Scalar Tensor []>]()\n",
      "  %482 = Unsqueeze[axes = [0]](%478)\n",
      "  %483 = Unsqueeze[axes = [0]](%479)\n",
      "  %484 = Unsqueeze[axes = [0]](%480)\n",
      "  %485 = Unsqueeze[axes = [0]](%481)\n",
      "  %486 = Concat[axis = 0](%482, %483, %484, %485)\n",
      "  %487 = Reshape(%450, %486)\n",
      "  %488 = Transpose[perm = [0, 2, 1, 3]](%487)\n",
      "  %489 = Transpose[perm = [0, 2, 3, 1]](%463)\n",
      "  %490 = MatMul(%488, %489)\n",
      "  %491 = Constant[value = <Scalar Tensor []>]()\n",
      "  %492 = Div(%490, %491)\n",
      "  %493 = Add(%492, %225)\n",
      "  %494 = Softmax[axis = 3](%493)\n",
      "  %495 = MatMul(%494, %477)\n",
      "  %496 = Transpose[perm = [0, 2, 1, 3]](%495)\n",
      "  %497 = Constant[value = <Scalar Tensor []>]()\n",
      "  %498 = Constant[value = <Scalar Tensor []>]()\n",
      "  %499 = Constant[value = <Scalar Tensor []>]()\n",
      "  %500 = Unsqueeze[axes = [0]](%497)\n",
      "  %501 = Unsqueeze[axes = [0]](%498)\n",
      "  %502 = Unsqueeze[axes = [0]](%499)\n",
      "  %503 = Concat[axis = 0](%500, %501, %502)\n",
      "  %504 = Reshape(%496, %503)\n",
      "  %505 = Transpose[perm = [1, 0]](%bert.encoder.layer.2.attention.output.dense.weight)\n",
      "  %506 = MatMul(%504, %505)\n",
      "  %507 = Add(%bert.encoder.layer.2.attention.output.dense.bias, %506)\n",
      "  %508 = Add(%507, %447)\n",
      "  %509 = ReduceMean[axes = [-1]](%508)\n",
      "  %510 = Sub(%508, %509)\n",
      "  %511 = Cast[to = 1](%510)\n",
      "  %512 = Constant[value = <Scalar Tensor []>]()\n",
      "  %513 = Pow(%511, %512)\n",
      "  %514 = ReduceMean[axes = [-1]](%513)\n",
      "  %515 = Constant[value = <Scalar Tensor []>]()\n",
      "  %516 = Add(%514, %515)\n",
      "  %517 = Sqrt(%516)\n",
      "  %518 = Div(%510, %517)\n",
      "  %519 = Mul(%518, %bert.encoder.layer.2.attention.output.LayerNorm.weight)\n",
      "  %520 = Add(%519, %bert.encoder.layer.2.attention.output.LayerNorm.bias)\n",
      "  %521 = Transpose[perm = [1, 0]](%bert.encoder.layer.2.intermediate.dense.weight)\n",
      "  %522 = MatMul(%520, %521)\n",
      "  %523 = Add(%bert.encoder.layer.2.intermediate.dense.bias, %522)\n",
      "  %524 = Constant[value = <Scalar Tensor []>]()\n",
      "  %525 = Div(%523, %524)\n",
      "  %526 = Erf(%525)\n",
      "  %527 = Constant[value = <Scalar Tensor []>]()\n",
      "  %528 = Add(%526, %527)\n",
      "  %529 = Mul(%523, %528)\n",
      "  %530 = Constant[value = <Scalar Tensor []>]()\n",
      "  %531 = Mul(%529, %530)\n",
      "  %532 = Transpose[perm = [1, 0]](%bert.encoder.layer.2.output.dense.weight)\n",
      "  %533 = MatMul(%531, %532)\n",
      "  %534 = Add(%bert.encoder.layer.2.output.dense.bias, %533)\n",
      "  %535 = Add(%534, %520)\n",
      "  %536 = ReduceMean[axes = [-1]](%535)\n",
      "  %537 = Sub(%535, %536)\n",
      "  %538 = Cast[to = 1](%537)\n",
      "  %539 = Constant[value = <Scalar Tensor []>]()\n",
      "  %540 = Pow(%538, %539)\n",
      "  %541 = ReduceMean[axes = [-1]](%540)\n",
      "  %542 = Constant[value = <Scalar Tensor []>]()\n",
      "  %543 = Add(%541, %542)\n",
      "  %544 = Sqrt(%543)\n",
      "  %545 = Div(%537, %544)\n",
      "  %546 = Mul(%545, %bert.encoder.layer.2.output.LayerNorm.weight)\n",
      "  %547 = Add(%546, %bert.encoder.layer.2.output.LayerNorm.bias)\n",
      "  %548 = Transpose[perm = [1, 0]](%bert.encoder.layer.3.attention.self.query.weight)\n",
      "  %549 = MatMul(%547, %548)\n",
      "  %550 = Add(%bert.encoder.layer.3.attention.self.query.bias, %549)\n",
      "  %551 = Transpose[perm = [1, 0]](%bert.encoder.layer.3.attention.self.key.weight)\n",
      "  %552 = MatMul(%547, %551)\n",
      "  %553 = Add(%bert.encoder.layer.3.attention.self.key.bias, %552)\n",
      "  %554 = Constant[value = <Scalar Tensor []>]()\n",
      "  %555 = Constant[value = <Scalar Tensor []>]()\n",
      "  %556 = Constant[value = <Scalar Tensor []>]()\n",
      "  %557 = Constant[value = <Scalar Tensor []>]()\n",
      "  %558 = Unsqueeze[axes = [0]](%554)\n",
      "  %559 = Unsqueeze[axes = [0]](%555)\n",
      "  %560 = Unsqueeze[axes = [0]](%556)\n",
      "  %561 = Unsqueeze[axes = [0]](%557)\n",
      "  %562 = Concat[axis = 0](%558, %559, %560, %561)\n",
      "  %563 = Reshape(%553, %562)\n",
      "  %564 = Transpose[perm = [1, 0]](%bert.encoder.layer.3.attention.self.value.weight)\n",
      "  %565 = MatMul(%547, %564)\n",
      "  %566 = Add(%bert.encoder.layer.3.attention.self.value.bias, %565)\n",
      "  %567 = Constant[value = <Scalar Tensor []>]()\n",
      "  %568 = Constant[value = <Scalar Tensor []>]()\n",
      "  %569 = Constant[value = <Scalar Tensor []>]()\n",
      "  %570 = Constant[value = <Scalar Tensor []>]()\n",
      "  %571 = Unsqueeze[axes = [0]](%567)\n",
      "  %572 = Unsqueeze[axes = [0]](%568)\n",
      "  %573 = Unsqueeze[axes = [0]](%569)\n",
      "  %574 = Unsqueeze[axes = [0]](%570)\n",
      "  %575 = Concat[axis = 0](%571, %572, %573, %574)\n",
      "  %576 = Reshape(%566, %575)\n",
      "  %577 = Transpose[perm = [0, 2, 1, 3]](%576)\n",
      "  %578 = Constant[value = <Scalar Tensor []>]()\n",
      "  %579 = Constant[value = <Scalar Tensor []>]()\n",
      "  %580 = Constant[value = <Scalar Tensor []>]()\n",
      "  %581 = Constant[value = <Scalar Tensor []>]()\n",
      "  %582 = Unsqueeze[axes = [0]](%578)\n",
      "  %583 = Unsqueeze[axes = [0]](%579)\n",
      "  %584 = Unsqueeze[axes = [0]](%580)\n",
      "  %585 = Unsqueeze[axes = [0]](%581)\n",
      "  %586 = Concat[axis = 0](%582, %583, %584, %585)\n",
      "  %587 = Reshape(%550, %586)\n",
      "  %588 = Transpose[perm = [0, 2, 1, 3]](%587)\n",
      "  %589 = Transpose[perm = [0, 2, 3, 1]](%563)\n",
      "  %590 = MatMul(%588, %589)\n",
      "  %591 = Constant[value = <Scalar Tensor []>]()\n",
      "  %592 = Div(%590, %591)\n",
      "  %593 = Add(%592, %225)\n",
      "  %594 = Softmax[axis = 3](%593)\n",
      "  %595 = MatMul(%594, %577)\n",
      "  %596 = Transpose[perm = [0, 2, 1, 3]](%595)\n",
      "  %597 = Constant[value = <Scalar Tensor []>]()\n",
      "  %598 = Constant[value = <Scalar Tensor []>]()\n",
      "  %599 = Constant[value = <Scalar Tensor []>]()\n",
      "  %600 = Unsqueeze[axes = [0]](%597)\n",
      "  %601 = Unsqueeze[axes = [0]](%598)\n",
      "  %602 = Unsqueeze[axes = [0]](%599)\n",
      "  %603 = Concat[axis = 0](%600, %601, %602)\n",
      "  %604 = Reshape(%596, %603)\n",
      "  %605 = Transpose[perm = [1, 0]](%bert.encoder.layer.3.attention.output.dense.weight)\n",
      "  %606 = MatMul(%604, %605)\n",
      "  %607 = Add(%bert.encoder.layer.3.attention.output.dense.bias, %606)\n",
      "  %608 = Add(%607, %547)\n",
      "  %609 = ReduceMean[axes = [-1]](%608)\n",
      "  %610 = Sub(%608, %609)\n",
      "  %611 = Cast[to = 1](%610)\n",
      "  %612 = Constant[value = <Scalar Tensor []>]()\n",
      "  %613 = Pow(%611, %612)\n",
      "  %614 = ReduceMean[axes = [-1]](%613)\n",
      "  %615 = Constant[value = <Scalar Tensor []>]()\n",
      "  %616 = Add(%614, %615)\n",
      "  %617 = Sqrt(%616)\n",
      "  %618 = Div(%610, %617)\n",
      "  %619 = Mul(%618, %bert.encoder.layer.3.attention.output.LayerNorm.weight)\n",
      "  %620 = Add(%619, %bert.encoder.layer.3.attention.output.LayerNorm.bias)\n",
      "  %621 = Transpose[perm = [1, 0]](%bert.encoder.layer.3.intermediate.dense.weight)\n",
      "  %622 = MatMul(%620, %621)\n",
      "  %623 = Add(%bert.encoder.layer.3.intermediate.dense.bias, %622)\n",
      "  %624 = Constant[value = <Scalar Tensor []>]()\n",
      "  %625 = Div(%623, %624)\n",
      "  %626 = Erf(%625)\n",
      "  %627 = Constant[value = <Scalar Tensor []>]()\n",
      "  %628 = Add(%626, %627)\n",
      "  %629 = Mul(%623, %628)\n",
      "  %630 = Constant[value = <Scalar Tensor []>]()\n",
      "  %631 = Mul(%629, %630)\n",
      "  %632 = Transpose[perm = [1, 0]](%bert.encoder.layer.3.output.dense.weight)\n",
      "  %633 = MatMul(%631, %632)\n",
      "  %634 = Add(%bert.encoder.layer.3.output.dense.bias, %633)\n",
      "  %635 = Add(%634, %620)\n",
      "  %636 = ReduceMean[axes = [-1]](%635)\n",
      "  %637 = Sub(%635, %636)\n",
      "  %638 = Cast[to = 1](%637)\n",
      "  %639 = Constant[value = <Scalar Tensor []>]()\n",
      "  %640 = Pow(%638, %639)\n",
      "  %641 = ReduceMean[axes = [-1]](%640)\n",
      "  %642 = Constant[value = <Scalar Tensor []>]()\n",
      "  %643 = Add(%641, %642)\n",
      "  %644 = Sqrt(%643)\n",
      "  %645 = Div(%637, %644)\n",
      "  %646 = Mul(%645, %bert.encoder.layer.3.output.LayerNorm.weight)\n",
      "  %647 = Add(%646, %bert.encoder.layer.3.output.LayerNorm.bias)\n",
      "  %648 = Transpose[perm = [1, 0]](%bert.encoder.layer.4.attention.self.query.weight)\n",
      "  %649 = MatMul(%647, %648)\n",
      "  %650 = Add(%bert.encoder.layer.4.attention.self.query.bias, %649)\n",
      "  %651 = Transpose[perm = [1, 0]](%bert.encoder.layer.4.attention.self.key.weight)\n",
      "  %652 = MatMul(%647, %651)\n",
      "  %653 = Add(%bert.encoder.layer.4.attention.self.key.bias, %652)\n",
      "  %654 = Constant[value = <Scalar Tensor []>]()\n",
      "  %655 = Constant[value = <Scalar Tensor []>]()\n",
      "  %656 = Constant[value = <Scalar Tensor []>]()\n",
      "  %657 = Constant[value = <Scalar Tensor []>]()\n",
      "  %658 = Unsqueeze[axes = [0]](%654)\n",
      "  %659 = Unsqueeze[axes = [0]](%655)\n",
      "  %660 = Unsqueeze[axes = [0]](%656)\n",
      "  %661 = Unsqueeze[axes = [0]](%657)\n",
      "  %662 = Concat[axis = 0](%658, %659, %660, %661)\n",
      "  %663 = Reshape(%653, %662)\n",
      "  %664 = Transpose[perm = [1, 0]](%bert.encoder.layer.4.attention.self.value.weight)\n",
      "  %665 = MatMul(%647, %664)\n",
      "  %666 = Add(%bert.encoder.layer.4.attention.self.value.bias, %665)\n",
      "  %667 = Constant[value = <Scalar Tensor []>]()\n",
      "  %668 = Constant[value = <Scalar Tensor []>]()\n",
      "  %669 = Constant[value = <Scalar Tensor []>]()\n",
      "  %670 = Constant[value = <Scalar Tensor []>]()\n",
      "  %671 = Unsqueeze[axes = [0]](%667)\n",
      "  %672 = Unsqueeze[axes = [0]](%668)\n",
      "  %673 = Unsqueeze[axes = [0]](%669)\n",
      "  %674 = Unsqueeze[axes = [0]](%670)\n",
      "  %675 = Concat[axis = 0](%671, %672, %673, %674)\n",
      "  %676 = Reshape(%666, %675)\n",
      "  %677 = Transpose[perm = [0, 2, 1, 3]](%676)\n",
      "  %678 = Constant[value = <Scalar Tensor []>]()\n",
      "  %679 = Constant[value = <Scalar Tensor []>]()\n",
      "  %680 = Constant[value = <Scalar Tensor []>]()\n",
      "  %681 = Constant[value = <Scalar Tensor []>]()\n",
      "  %682 = Unsqueeze[axes = [0]](%678)\n",
      "  %683 = Unsqueeze[axes = [0]](%679)\n",
      "  %684 = Unsqueeze[axes = [0]](%680)\n",
      "  %685 = Unsqueeze[axes = [0]](%681)\n",
      "  %686 = Concat[axis = 0](%682, %683, %684, %685)\n",
      "  %687 = Reshape(%650, %686)\n",
      "  %688 = Transpose[perm = [0, 2, 1, 3]](%687)\n",
      "  %689 = Transpose[perm = [0, 2, 3, 1]](%663)\n",
      "  %690 = MatMul(%688, %689)\n",
      "  %691 = Constant[value = <Scalar Tensor []>]()\n",
      "  %692 = Div(%690, %691)\n",
      "  %693 = Add(%692, %225)\n",
      "  %694 = Softmax[axis = 3](%693)\n",
      "  %695 = MatMul(%694, %677)\n",
      "  %696 = Transpose[perm = [0, 2, 1, 3]](%695)\n",
      "  %697 = Constant[value = <Scalar Tensor []>]()\n",
      "  %698 = Constant[value = <Scalar Tensor []>]()\n",
      "  %699 = Constant[value = <Scalar Tensor []>]()\n",
      "  %700 = Unsqueeze[axes = [0]](%697)\n",
      "  %701 = Unsqueeze[axes = [0]](%698)\n",
      "  %702 = Unsqueeze[axes = [0]](%699)\n",
      "  %703 = Concat[axis = 0](%700, %701, %702)\n",
      "  %704 = Reshape(%696, %703)\n",
      "  %705 = Transpose[perm = [1, 0]](%bert.encoder.layer.4.attention.output.dense.weight)\n",
      "  %706 = MatMul(%704, %705)\n",
      "  %707 = Add(%bert.encoder.layer.4.attention.output.dense.bias, %706)\n",
      "  %708 = Add(%707, %647)\n",
      "  %709 = ReduceMean[axes = [-1]](%708)\n",
      "  %710 = Sub(%708, %709)\n",
      "  %711 = Cast[to = 1](%710)\n",
      "  %712 = Constant[value = <Scalar Tensor []>]()\n",
      "  %713 = Pow(%711, %712)\n",
      "  %714 = ReduceMean[axes = [-1]](%713)\n",
      "  %715 = Constant[value = <Scalar Tensor []>]()\n",
      "  %716 = Add(%714, %715)\n",
      "  %717 = Sqrt(%716)\n",
      "  %718 = Div(%710, %717)\n",
      "  %719 = Mul(%718, %bert.encoder.layer.4.attention.output.LayerNorm.weight)\n",
      "  %720 = Add(%719, %bert.encoder.layer.4.attention.output.LayerNorm.bias)\n",
      "  %721 = Transpose[perm = [1, 0]](%bert.encoder.layer.4.intermediate.dense.weight)\n",
      "  %722 = MatMul(%720, %721)\n",
      "  %723 = Add(%bert.encoder.layer.4.intermediate.dense.bias, %722)\n",
      "  %724 = Constant[value = <Scalar Tensor []>]()\n",
      "  %725 = Div(%723, %724)\n",
      "  %726 = Erf(%725)\n",
      "  %727 = Constant[value = <Scalar Tensor []>]()\n",
      "  %728 = Add(%726, %727)\n",
      "  %729 = Mul(%723, %728)\n",
      "  %730 = Constant[value = <Scalar Tensor []>]()\n",
      "  %731 = Mul(%729, %730)\n",
      "  %732 = Transpose[perm = [1, 0]](%bert.encoder.layer.4.output.dense.weight)\n",
      "  %733 = MatMul(%731, %732)\n",
      "  %734 = Add(%bert.encoder.layer.4.output.dense.bias, %733)\n",
      "  %735 = Add(%734, %720)\n",
      "  %736 = ReduceMean[axes = [-1]](%735)\n",
      "  %737 = Sub(%735, %736)\n",
      "  %738 = Cast[to = 1](%737)\n",
      "  %739 = Constant[value = <Scalar Tensor []>]()\n",
      "  %740 = Pow(%738, %739)\n",
      "  %741 = ReduceMean[axes = [-1]](%740)\n",
      "  %742 = Constant[value = <Scalar Tensor []>]()\n",
      "  %743 = Add(%741, %742)\n",
      "  %744 = Sqrt(%743)\n",
      "  %745 = Div(%737, %744)\n",
      "  %746 = Mul(%745, %bert.encoder.layer.4.output.LayerNorm.weight)\n",
      "  %747 = Add(%746, %bert.encoder.layer.4.output.LayerNorm.bias)\n",
      "  %748 = Transpose[perm = [1, 0]](%bert.encoder.layer.5.attention.self.query.weight)\n",
      "  %749 = MatMul(%747, %748)\n",
      "  %750 = Add(%bert.encoder.layer.5.attention.self.query.bias, %749)\n",
      "  %751 = Transpose[perm = [1, 0]](%bert.encoder.layer.5.attention.self.key.weight)\n",
      "  %752 = MatMul(%747, %751)\n",
      "  %753 = Add(%bert.encoder.layer.5.attention.self.key.bias, %752)\n",
      "  %754 = Constant[value = <Scalar Tensor []>]()\n",
      "  %755 = Constant[value = <Scalar Tensor []>]()\n",
      "  %756 = Constant[value = <Scalar Tensor []>]()\n",
      "  %757 = Constant[value = <Scalar Tensor []>]()\n",
      "  %758 = Unsqueeze[axes = [0]](%754)\n",
      "  %759 = Unsqueeze[axes = [0]](%755)\n",
      "  %760 = Unsqueeze[axes = [0]](%756)\n",
      "  %761 = Unsqueeze[axes = [0]](%757)\n",
      "  %762 = Concat[axis = 0](%758, %759, %760, %761)\n",
      "  %763 = Reshape(%753, %762)\n",
      "  %764 = Transpose[perm = [1, 0]](%bert.encoder.layer.5.attention.self.value.weight)\n",
      "  %765 = MatMul(%747, %764)\n",
      "  %766 = Add(%bert.encoder.layer.5.attention.self.value.bias, %765)\n",
      "  %767 = Constant[value = <Scalar Tensor []>]()\n",
      "  %768 = Constant[value = <Scalar Tensor []>]()\n",
      "  %769 = Constant[value = <Scalar Tensor []>]()\n",
      "  %770 = Constant[value = <Scalar Tensor []>]()\n",
      "  %771 = Unsqueeze[axes = [0]](%767)\n",
      "  %772 = Unsqueeze[axes = [0]](%768)\n",
      "  %773 = Unsqueeze[axes = [0]](%769)\n",
      "  %774 = Unsqueeze[axes = [0]](%770)\n",
      "  %775 = Concat[axis = 0](%771, %772, %773, %774)\n",
      "  %776 = Reshape(%766, %775)\n",
      "  %777 = Transpose[perm = [0, 2, 1, 3]](%776)\n",
      "  %778 = Constant[value = <Scalar Tensor []>]()\n",
      "  %779 = Constant[value = <Scalar Tensor []>]()\n",
      "  %780 = Constant[value = <Scalar Tensor []>]()\n",
      "  %781 = Constant[value = <Scalar Tensor []>]()\n",
      "  %782 = Unsqueeze[axes = [0]](%778)\n",
      "  %783 = Unsqueeze[axes = [0]](%779)\n",
      "  %784 = Unsqueeze[axes = [0]](%780)\n",
      "  %785 = Unsqueeze[axes = [0]](%781)\n",
      "  %786 = Concat[axis = 0](%782, %783, %784, %785)\n",
      "  %787 = Reshape(%750, %786)\n",
      "  %788 = Transpose[perm = [0, 2, 1, 3]](%787)\n",
      "  %789 = Transpose[perm = [0, 2, 3, 1]](%763)\n",
      "  %790 = MatMul(%788, %789)\n",
      "  %791 = Constant[value = <Scalar Tensor []>]()\n",
      "  %792 = Div(%790, %791)\n",
      "  %793 = Add(%792, %225)\n",
      "  %794 = Softmax[axis = 3](%793)\n",
      "  %795 = MatMul(%794, %777)\n",
      "  %796 = Transpose[perm = [0, 2, 1, 3]](%795)\n",
      "  %797 = Constant[value = <Scalar Tensor []>]()\n",
      "  %798 = Constant[value = <Scalar Tensor []>]()\n",
      "  %799 = Constant[value = <Scalar Tensor []>]()\n",
      "  %800 = Unsqueeze[axes = [0]](%797)\n",
      "  %801 = Unsqueeze[axes = [0]](%798)\n",
      "  %802 = Unsqueeze[axes = [0]](%799)\n",
      "  %803 = Concat[axis = 0](%800, %801, %802)\n",
      "  %804 = Reshape(%796, %803)\n",
      "  %805 = Transpose[perm = [1, 0]](%bert.encoder.layer.5.attention.output.dense.weight)\n",
      "  %806 = MatMul(%804, %805)\n",
      "  %807 = Add(%bert.encoder.layer.5.attention.output.dense.bias, %806)\n",
      "  %808 = Add(%807, %747)\n",
      "  %809 = ReduceMean[axes = [-1]](%808)\n",
      "  %810 = Sub(%808, %809)\n",
      "  %811 = Cast[to = 1](%810)\n",
      "  %812 = Constant[value = <Scalar Tensor []>]()\n",
      "  %813 = Pow(%811, %812)\n",
      "  %814 = ReduceMean[axes = [-1]](%813)\n",
      "  %815 = Constant[value = <Scalar Tensor []>]()\n",
      "  %816 = Add(%814, %815)\n",
      "  %817 = Sqrt(%816)\n",
      "  %818 = Div(%810, %817)\n",
      "  %819 = Mul(%818, %bert.encoder.layer.5.attention.output.LayerNorm.weight)\n",
      "  %820 = Add(%819, %bert.encoder.layer.5.attention.output.LayerNorm.bias)\n",
      "  %821 = Transpose[perm = [1, 0]](%bert.encoder.layer.5.intermediate.dense.weight)\n",
      "  %822 = MatMul(%820, %821)\n",
      "  %823 = Add(%bert.encoder.layer.5.intermediate.dense.bias, %822)\n",
      "  %824 = Constant[value = <Scalar Tensor []>]()\n",
      "  %825 = Div(%823, %824)\n",
      "  %826 = Erf(%825)\n",
      "  %827 = Constant[value = <Scalar Tensor []>]()\n",
      "  %828 = Add(%826, %827)\n",
      "  %829 = Mul(%823, %828)\n",
      "  %830 = Constant[value = <Scalar Tensor []>]()\n",
      "  %831 = Mul(%829, %830)\n",
      "  %832 = Transpose[perm = [1, 0]](%bert.encoder.layer.5.output.dense.weight)\n",
      "  %833 = MatMul(%831, %832)\n",
      "  %834 = Add(%bert.encoder.layer.5.output.dense.bias, %833)\n",
      "  %835 = Add(%834, %820)\n",
      "  %836 = ReduceMean[axes = [-1]](%835)\n",
      "  %837 = Sub(%835, %836)\n",
      "  %838 = Cast[to = 1](%837)\n",
      "  %839 = Constant[value = <Scalar Tensor []>]()\n",
      "  %840 = Pow(%838, %839)\n",
      "  %841 = ReduceMean[axes = [-1]](%840)\n",
      "  %842 = Constant[value = <Scalar Tensor []>]()\n",
      "  %843 = Add(%841, %842)\n",
      "  %844 = Sqrt(%843)\n",
      "  %845 = Div(%837, %844)\n",
      "  %846 = Mul(%845, %bert.encoder.layer.5.output.LayerNorm.weight)\n",
      "  %847 = Add(%846, %bert.encoder.layer.5.output.LayerNorm.bias)\n",
      "  %848 = Transpose[perm = [1, 0]](%bert.encoder.layer.6.attention.self.query.weight)\n",
      "  %849 = MatMul(%847, %848)\n",
      "  %850 = Add(%bert.encoder.layer.6.attention.self.query.bias, %849)\n",
      "  %851 = Transpose[perm = [1, 0]](%bert.encoder.layer.6.attention.self.key.weight)\n",
      "  %852 = MatMul(%847, %851)\n",
      "  %853 = Add(%bert.encoder.layer.6.attention.self.key.bias, %852)\n",
      "  %854 = Constant[value = <Scalar Tensor []>]()\n",
      "  %855 = Constant[value = <Scalar Tensor []>]()\n",
      "  %856 = Constant[value = <Scalar Tensor []>]()\n",
      "  %857 = Constant[value = <Scalar Tensor []>]()\n",
      "  %858 = Unsqueeze[axes = [0]](%854)\n",
      "  %859 = Unsqueeze[axes = [0]](%855)\n",
      "  %860 = Unsqueeze[axes = [0]](%856)\n",
      "  %861 = Unsqueeze[axes = [0]](%857)\n",
      "  %862 = Concat[axis = 0](%858, %859, %860, %861)\n",
      "  %863 = Reshape(%853, %862)\n",
      "  %864 = Transpose[perm = [1, 0]](%bert.encoder.layer.6.attention.self.value.weight)\n",
      "  %865 = MatMul(%847, %864)\n",
      "  %866 = Add(%bert.encoder.layer.6.attention.self.value.bias, %865)\n",
      "  %867 = Constant[value = <Scalar Tensor []>]()\n",
      "  %868 = Constant[value = <Scalar Tensor []>]()\n",
      "  %869 = Constant[value = <Scalar Tensor []>]()\n",
      "  %870 = Constant[value = <Scalar Tensor []>]()\n",
      "  %871 = Unsqueeze[axes = [0]](%867)\n",
      "  %872 = Unsqueeze[axes = [0]](%868)\n",
      "  %873 = Unsqueeze[axes = [0]](%869)\n",
      "  %874 = Unsqueeze[axes = [0]](%870)\n",
      "  %875 = Concat[axis = 0](%871, %872, %873, %874)\n",
      "  %876 = Reshape(%866, %875)\n",
      "  %877 = Transpose[perm = [0, 2, 1, 3]](%876)\n",
      "  %878 = Constant[value = <Scalar Tensor []>]()\n",
      "  %879 = Constant[value = <Scalar Tensor []>]()\n",
      "  %880 = Constant[value = <Scalar Tensor []>]()\n",
      "  %881 = Constant[value = <Scalar Tensor []>]()\n",
      "  %882 = Unsqueeze[axes = [0]](%878)\n",
      "  %883 = Unsqueeze[axes = [0]](%879)\n",
      "  %884 = Unsqueeze[axes = [0]](%880)\n",
      "  %885 = Unsqueeze[axes = [0]](%881)\n",
      "  %886 = Concat[axis = 0](%882, %883, %884, %885)\n",
      "  %887 = Reshape(%850, %886)\n",
      "  %888 = Transpose[perm = [0, 2, 1, 3]](%887)\n",
      "  %889 = Transpose[perm = [0, 2, 3, 1]](%863)\n",
      "  %890 = MatMul(%888, %889)\n",
      "  %891 = Constant[value = <Scalar Tensor []>]()\n",
      "  %892 = Div(%890, %891)\n",
      "  %893 = Add(%892, %225)\n",
      "  %894 = Softmax[axis = 3](%893)\n",
      "  %895 = MatMul(%894, %877)\n",
      "  %896 = Transpose[perm = [0, 2, 1, 3]](%895)\n",
      "  %897 = Constant[value = <Scalar Tensor []>]()\n",
      "  %898 = Constant[value = <Scalar Tensor []>]()\n",
      "  %899 = Constant[value = <Scalar Tensor []>]()\n",
      "  %900 = Unsqueeze[axes = [0]](%897)\n",
      "  %901 = Unsqueeze[axes = [0]](%898)\n",
      "  %902 = Unsqueeze[axes = [0]](%899)\n",
      "  %903 = Concat[axis = 0](%900, %901, %902)\n",
      "  %904 = Reshape(%896, %903)\n",
      "  %905 = Transpose[perm = [1, 0]](%bert.encoder.layer.6.attention.output.dense.weight)\n",
      "  %906 = MatMul(%904, %905)\n",
      "  %907 = Add(%bert.encoder.layer.6.attention.output.dense.bias, %906)\n",
      "  %908 = Add(%907, %847)\n",
      "  %909 = ReduceMean[axes = [-1]](%908)\n",
      "  %910 = Sub(%908, %909)\n",
      "  %911 = Cast[to = 1](%910)\n",
      "  %912 = Constant[value = <Scalar Tensor []>]()\n",
      "  %913 = Pow(%911, %912)\n",
      "  %914 = ReduceMean[axes = [-1]](%913)\n",
      "  %915 = Constant[value = <Scalar Tensor []>]()\n",
      "  %916 = Add(%914, %915)\n",
      "  %917 = Sqrt(%916)\n",
      "  %918 = Div(%910, %917)\n",
      "  %919 = Mul(%918, %bert.encoder.layer.6.attention.output.LayerNorm.weight)\n",
      "  %920 = Add(%919, %bert.encoder.layer.6.attention.output.LayerNorm.bias)\n",
      "  %921 = Transpose[perm = [1, 0]](%bert.encoder.layer.6.intermediate.dense.weight)\n",
      "  %922 = MatMul(%920, %921)\n",
      "  %923 = Add(%bert.encoder.layer.6.intermediate.dense.bias, %922)\n",
      "  %924 = Constant[value = <Scalar Tensor []>]()\n",
      "  %925 = Div(%923, %924)\n",
      "  %926 = Erf(%925)\n",
      "  %927 = Constant[value = <Scalar Tensor []>]()\n",
      "  %928 = Add(%926, %927)\n",
      "  %929 = Mul(%923, %928)\n",
      "  %930 = Constant[value = <Scalar Tensor []>]()\n",
      "  %931 = Mul(%929, %930)\n",
      "  %932 = Transpose[perm = [1, 0]](%bert.encoder.layer.6.output.dense.weight)\n",
      "  %933 = MatMul(%931, %932)\n",
      "  %934 = Add(%bert.encoder.layer.6.output.dense.bias, %933)\n",
      "  %935 = Add(%934, %920)\n",
      "  %936 = ReduceMean[axes = [-1]](%935)\n",
      "  %937 = Sub(%935, %936)\n",
      "  %938 = Cast[to = 1](%937)\n",
      "  %939 = Constant[value = <Scalar Tensor []>]()\n",
      "  %940 = Pow(%938, %939)\n",
      "  %941 = ReduceMean[axes = [-1]](%940)\n",
      "  %942 = Constant[value = <Scalar Tensor []>]()\n",
      "  %943 = Add(%941, %942)\n",
      "  %944 = Sqrt(%943)\n",
      "  %945 = Div(%937, %944)\n",
      "  %946 = Mul(%945, %bert.encoder.layer.6.output.LayerNorm.weight)\n",
      "  %947 = Add(%946, %bert.encoder.layer.6.output.LayerNorm.bias)\n",
      "  %948 = Transpose[perm = [1, 0]](%bert.encoder.layer.7.attention.self.query.weight)\n",
      "  %949 = MatMul(%947, %948)\n",
      "  %950 = Add(%bert.encoder.layer.7.attention.self.query.bias, %949)\n",
      "  %951 = Transpose[perm = [1, 0]](%bert.encoder.layer.7.attention.self.key.weight)\n",
      "  %952 = MatMul(%947, %951)\n",
      "  %953 = Add(%bert.encoder.layer.7.attention.self.key.bias, %952)\n",
      "  %954 = Constant[value = <Scalar Tensor []>]()\n",
      "  %955 = Constant[value = <Scalar Tensor []>]()\n",
      "  %956 = Constant[value = <Scalar Tensor []>]()\n",
      "  %957 = Constant[value = <Scalar Tensor []>]()\n",
      "  %958 = Unsqueeze[axes = [0]](%954)\n",
      "  %959 = Unsqueeze[axes = [0]](%955)\n",
      "  %960 = Unsqueeze[axes = [0]](%956)\n",
      "  %961 = Unsqueeze[axes = [0]](%957)\n",
      "  %962 = Concat[axis = 0](%958, %959, %960, %961)\n",
      "  %963 = Reshape(%953, %962)\n",
      "  %964 = Transpose[perm = [1, 0]](%bert.encoder.layer.7.attention.self.value.weight)\n",
      "  %965 = MatMul(%947, %964)\n",
      "  %966 = Add(%bert.encoder.layer.7.attention.self.value.bias, %965)\n",
      "  %967 = Constant[value = <Scalar Tensor []>]()\n",
      "  %968 = Constant[value = <Scalar Tensor []>]()\n",
      "  %969 = Constant[value = <Scalar Tensor []>]()\n",
      "  %970 = Constant[value = <Scalar Tensor []>]()\n",
      "  %971 = Unsqueeze[axes = [0]](%967)\n",
      "  %972 = Unsqueeze[axes = [0]](%968)\n",
      "  %973 = Unsqueeze[axes = [0]](%969)\n",
      "  %974 = Unsqueeze[axes = [0]](%970)\n",
      "  %975 = Concat[axis = 0](%971, %972, %973, %974)\n",
      "  %976 = Reshape(%966, %975)\n",
      "  %977 = Transpose[perm = [0, 2, 1, 3]](%976)\n",
      "  %978 = Constant[value = <Scalar Tensor []>]()\n",
      "  %979 = Constant[value = <Scalar Tensor []>]()\n",
      "  %980 = Constant[value = <Scalar Tensor []>]()\n",
      "  %981 = Constant[value = <Scalar Tensor []>]()\n",
      "  %982 = Unsqueeze[axes = [0]](%978)\n",
      "  %983 = Unsqueeze[axes = [0]](%979)\n",
      "  %984 = Unsqueeze[axes = [0]](%980)\n",
      "  %985 = Unsqueeze[axes = [0]](%981)\n",
      "  %986 = Concat[axis = 0](%982, %983, %984, %985)\n",
      "  %987 = Reshape(%950, %986)\n",
      "  %988 = Transpose[perm = [0, 2, 1, 3]](%987)\n",
      "  %989 = Transpose[perm = [0, 2, 3, 1]](%963)\n",
      "  %990 = MatMul(%988, %989)\n",
      "  %991 = Constant[value = <Scalar Tensor []>]()\n",
      "  %992 = Div(%990, %991)\n",
      "  %993 = Add(%992, %225)\n",
      "  %994 = Softmax[axis = 3](%993)\n",
      "  %995 = MatMul(%994, %977)\n",
      "  %996 = Transpose[perm = [0, 2, 1, 3]](%995)\n",
      "  %997 = Constant[value = <Scalar Tensor []>]()\n",
      "  %998 = Constant[value = <Scalar Tensor []>]()\n",
      "  %999 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1000 = Unsqueeze[axes = [0]](%997)\n",
      "  %1001 = Unsqueeze[axes = [0]](%998)\n",
      "  %1002 = Unsqueeze[axes = [0]](%999)\n",
      "  %1003 = Concat[axis = 0](%1000, %1001, %1002)\n",
      "  %1004 = Reshape(%996, %1003)\n",
      "  %1005 = Transpose[perm = [1, 0]](%bert.encoder.layer.7.attention.output.dense.weight)\n",
      "  %1006 = MatMul(%1004, %1005)\n",
      "  %1007 = Add(%bert.encoder.layer.7.attention.output.dense.bias, %1006)\n",
      "  %1008 = Add(%1007, %947)\n",
      "  %1009 = ReduceMean[axes = [-1]](%1008)\n",
      "  %1010 = Sub(%1008, %1009)\n",
      "  %1011 = Cast[to = 1](%1010)\n",
      "  %1012 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1013 = Pow(%1011, %1012)\n",
      "  %1014 = ReduceMean[axes = [-1]](%1013)\n",
      "  %1015 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1016 = Add(%1014, %1015)\n",
      "  %1017 = Sqrt(%1016)\n",
      "  %1018 = Div(%1010, %1017)\n",
      "  %1019 = Mul(%1018, %bert.encoder.layer.7.attention.output.LayerNorm.weight)\n",
      "  %1020 = Add(%1019, %bert.encoder.layer.7.attention.output.LayerNorm.bias)\n",
      "  %1021 = Transpose[perm = [1, 0]](%bert.encoder.layer.7.intermediate.dense.weight)\n",
      "  %1022 = MatMul(%1020, %1021)\n",
      "  %1023 = Add(%bert.encoder.layer.7.intermediate.dense.bias, %1022)\n",
      "  %1024 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1025 = Div(%1023, %1024)\n",
      "  %1026 = Erf(%1025)\n",
      "  %1027 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1028 = Add(%1026, %1027)\n",
      "  %1029 = Mul(%1023, %1028)\n",
      "  %1030 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1031 = Mul(%1029, %1030)\n",
      "  %1032 = Transpose[perm = [1, 0]](%bert.encoder.layer.7.output.dense.weight)\n",
      "  %1033 = MatMul(%1031, %1032)\n",
      "  %1034 = Add(%bert.encoder.layer.7.output.dense.bias, %1033)\n",
      "  %1035 = Add(%1034, %1020)\n",
      "  %1036 = ReduceMean[axes = [-1]](%1035)\n",
      "  %1037 = Sub(%1035, %1036)\n",
      "  %1038 = Cast[to = 1](%1037)\n",
      "  %1039 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1040 = Pow(%1038, %1039)\n",
      "  %1041 = ReduceMean[axes = [-1]](%1040)\n",
      "  %1042 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1043 = Add(%1041, %1042)\n",
      "  %1044 = Sqrt(%1043)\n",
      "  %1045 = Div(%1037, %1044)\n",
      "  %1046 = Mul(%1045, %bert.encoder.layer.7.output.LayerNorm.weight)\n",
      "  %1047 = Add(%1046, %bert.encoder.layer.7.output.LayerNorm.bias)\n",
      "  %1048 = Transpose[perm = [1, 0]](%bert.encoder.layer.8.attention.self.query.weight)\n",
      "  %1049 = MatMul(%1047, %1048)\n",
      "  %1050 = Add(%bert.encoder.layer.8.attention.self.query.bias, %1049)\n",
      "  %1051 = Transpose[perm = [1, 0]](%bert.encoder.layer.8.attention.self.key.weight)\n",
      "  %1052 = MatMul(%1047, %1051)\n",
      "  %1053 = Add(%bert.encoder.layer.8.attention.self.key.bias, %1052)\n",
      "  %1054 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1055 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1056 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1057 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1058 = Unsqueeze[axes = [0]](%1054)\n",
      "  %1059 = Unsqueeze[axes = [0]](%1055)\n",
      "  %1060 = Unsqueeze[axes = [0]](%1056)\n",
      "  %1061 = Unsqueeze[axes = [0]](%1057)\n",
      "  %1062 = Concat[axis = 0](%1058, %1059, %1060, %1061)\n",
      "  %1063 = Reshape(%1053, %1062)\n",
      "  %1064 = Transpose[perm = [1, 0]](%bert.encoder.layer.8.attention.self.value.weight)\n",
      "  %1065 = MatMul(%1047, %1064)\n",
      "  %1066 = Add(%bert.encoder.layer.8.attention.self.value.bias, %1065)\n",
      "  %1067 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1068 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1069 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1070 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1071 = Unsqueeze[axes = [0]](%1067)\n",
      "  %1072 = Unsqueeze[axes = [0]](%1068)\n",
      "  %1073 = Unsqueeze[axes = [0]](%1069)\n",
      "  %1074 = Unsqueeze[axes = [0]](%1070)\n",
      "  %1075 = Concat[axis = 0](%1071, %1072, %1073, %1074)\n",
      "  %1076 = Reshape(%1066, %1075)\n",
      "  %1077 = Transpose[perm = [0, 2, 1, 3]](%1076)\n",
      "  %1078 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1079 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1080 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1081 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1082 = Unsqueeze[axes = [0]](%1078)\n",
      "  %1083 = Unsqueeze[axes = [0]](%1079)\n",
      "  %1084 = Unsqueeze[axes = [0]](%1080)\n",
      "  %1085 = Unsqueeze[axes = [0]](%1081)\n",
      "  %1086 = Concat[axis = 0](%1082, %1083, %1084, %1085)\n",
      "  %1087 = Reshape(%1050, %1086)\n",
      "  %1088 = Transpose[perm = [0, 2, 1, 3]](%1087)\n",
      "  %1089 = Transpose[perm = [0, 2, 3, 1]](%1063)\n",
      "  %1090 = MatMul(%1088, %1089)\n",
      "  %1091 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1092 = Div(%1090, %1091)\n",
      "  %1093 = Add(%1092, %225)\n",
      "  %1094 = Softmax[axis = 3](%1093)\n",
      "  %1095 = MatMul(%1094, %1077)\n",
      "  %1096 = Transpose[perm = [0, 2, 1, 3]](%1095)\n",
      "  %1097 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1098 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1099 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1100 = Unsqueeze[axes = [0]](%1097)\n",
      "  %1101 = Unsqueeze[axes = [0]](%1098)\n",
      "  %1102 = Unsqueeze[axes = [0]](%1099)\n",
      "  %1103 = Concat[axis = 0](%1100, %1101, %1102)\n",
      "  %1104 = Reshape(%1096, %1103)\n",
      "  %1105 = Transpose[perm = [1, 0]](%bert.encoder.layer.8.attention.output.dense.weight)\n",
      "  %1106 = MatMul(%1104, %1105)\n",
      "  %1107 = Add(%bert.encoder.layer.8.attention.output.dense.bias, %1106)\n",
      "  %1108 = Add(%1107, %1047)\n",
      "  %1109 = ReduceMean[axes = [-1]](%1108)\n",
      "  %1110 = Sub(%1108, %1109)\n",
      "  %1111 = Cast[to = 1](%1110)\n",
      "  %1112 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1113 = Pow(%1111, %1112)\n",
      "  %1114 = ReduceMean[axes = [-1]](%1113)\n",
      "  %1115 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1116 = Add(%1114, %1115)\n",
      "  %1117 = Sqrt(%1116)\n",
      "  %1118 = Div(%1110, %1117)\n",
      "  %1119 = Mul(%1118, %bert.encoder.layer.8.attention.output.LayerNorm.weight)\n",
      "  %1120 = Add(%1119, %bert.encoder.layer.8.attention.output.LayerNorm.bias)\n",
      "  %1121 = Transpose[perm = [1, 0]](%bert.encoder.layer.8.intermediate.dense.weight)\n",
      "  %1122 = MatMul(%1120, %1121)\n",
      "  %1123 = Add(%bert.encoder.layer.8.intermediate.dense.bias, %1122)\n",
      "  %1124 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1125 = Div(%1123, %1124)\n",
      "  %1126 = Erf(%1125)\n",
      "  %1127 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1128 = Add(%1126, %1127)\n",
      "  %1129 = Mul(%1123, %1128)\n",
      "  %1130 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1131 = Mul(%1129, %1130)\n",
      "  %1132 = Transpose[perm = [1, 0]](%bert.encoder.layer.8.output.dense.weight)\n",
      "  %1133 = MatMul(%1131, %1132)\n",
      "  %1134 = Add(%bert.encoder.layer.8.output.dense.bias, %1133)\n",
      "  %1135 = Add(%1134, %1120)\n",
      "  %1136 = ReduceMean[axes = [-1]](%1135)\n",
      "  %1137 = Sub(%1135, %1136)\n",
      "  %1138 = Cast[to = 1](%1137)\n",
      "  %1139 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1140 = Pow(%1138, %1139)\n",
      "  %1141 = ReduceMean[axes = [-1]](%1140)\n",
      "  %1142 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1143 = Add(%1141, %1142)\n",
      "  %1144 = Sqrt(%1143)\n",
      "  %1145 = Div(%1137, %1144)\n",
      "  %1146 = Mul(%1145, %bert.encoder.layer.8.output.LayerNorm.weight)\n",
      "  %1147 = Add(%1146, %bert.encoder.layer.8.output.LayerNorm.bias)\n",
      "  %1148 = Transpose[perm = [1, 0]](%bert.encoder.layer.9.attention.self.query.weight)\n",
      "  %1149 = MatMul(%1147, %1148)\n",
      "  %1150 = Add(%bert.encoder.layer.9.attention.self.query.bias, %1149)\n",
      "  %1151 = Transpose[perm = [1, 0]](%bert.encoder.layer.9.attention.self.key.weight)\n",
      "  %1152 = MatMul(%1147, %1151)\n",
      "  %1153 = Add(%bert.encoder.layer.9.attention.self.key.bias, %1152)\n",
      "  %1154 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1155 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1156 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1157 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1158 = Unsqueeze[axes = [0]](%1154)\n",
      "  %1159 = Unsqueeze[axes = [0]](%1155)\n",
      "  %1160 = Unsqueeze[axes = [0]](%1156)\n",
      "  %1161 = Unsqueeze[axes = [0]](%1157)\n",
      "  %1162 = Concat[axis = 0](%1158, %1159, %1160, %1161)\n",
      "  %1163 = Reshape(%1153, %1162)\n",
      "  %1164 = Transpose[perm = [1, 0]](%bert.encoder.layer.9.attention.self.value.weight)\n",
      "  %1165 = MatMul(%1147, %1164)\n",
      "  %1166 = Add(%bert.encoder.layer.9.attention.self.value.bias, %1165)\n",
      "  %1167 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1168 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1169 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1170 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1171 = Unsqueeze[axes = [0]](%1167)\n",
      "  %1172 = Unsqueeze[axes = [0]](%1168)\n",
      "  %1173 = Unsqueeze[axes = [0]](%1169)\n",
      "  %1174 = Unsqueeze[axes = [0]](%1170)\n",
      "  %1175 = Concat[axis = 0](%1171, %1172, %1173, %1174)\n",
      "  %1176 = Reshape(%1166, %1175)\n",
      "  %1177 = Transpose[perm = [0, 2, 1, 3]](%1176)\n",
      "  %1178 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1179 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1180 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1181 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1182 = Unsqueeze[axes = [0]](%1178)\n",
      "  %1183 = Unsqueeze[axes = [0]](%1179)\n",
      "  %1184 = Unsqueeze[axes = [0]](%1180)\n",
      "  %1185 = Unsqueeze[axes = [0]](%1181)\n",
      "  %1186 = Concat[axis = 0](%1182, %1183, %1184, %1185)\n",
      "  %1187 = Reshape(%1150, %1186)\n",
      "  %1188 = Transpose[perm = [0, 2, 1, 3]](%1187)\n",
      "  %1189 = Transpose[perm = [0, 2, 3, 1]](%1163)\n",
      "  %1190 = MatMul(%1188, %1189)\n",
      "  %1191 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1192 = Div(%1190, %1191)\n",
      "  %1193 = Add(%1192, %225)\n",
      "  %1194 = Softmax[axis = 3](%1193)\n",
      "  %1195 = MatMul(%1194, %1177)\n",
      "  %1196 = Transpose[perm = [0, 2, 1, 3]](%1195)\n",
      "  %1197 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1198 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1199 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1200 = Unsqueeze[axes = [0]](%1197)\n",
      "  %1201 = Unsqueeze[axes = [0]](%1198)\n",
      "  %1202 = Unsqueeze[axes = [0]](%1199)\n",
      "  %1203 = Concat[axis = 0](%1200, %1201, %1202)\n",
      "  %1204 = Reshape(%1196, %1203)\n",
      "  %1205 = Transpose[perm = [1, 0]](%bert.encoder.layer.9.attention.output.dense.weight)\n",
      "  %1206 = MatMul(%1204, %1205)\n",
      "  %1207 = Add(%bert.encoder.layer.9.attention.output.dense.bias, %1206)\n",
      "  %1208 = Add(%1207, %1147)\n",
      "  %1209 = ReduceMean[axes = [-1]](%1208)\n",
      "  %1210 = Sub(%1208, %1209)\n",
      "  %1211 = Cast[to = 1](%1210)\n",
      "  %1212 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1213 = Pow(%1211, %1212)\n",
      "  %1214 = ReduceMean[axes = [-1]](%1213)\n",
      "  %1215 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1216 = Add(%1214, %1215)\n",
      "  %1217 = Sqrt(%1216)\n",
      "  %1218 = Div(%1210, %1217)\n",
      "  %1219 = Mul(%1218, %bert.encoder.layer.9.attention.output.LayerNorm.weight)\n",
      "  %1220 = Add(%1219, %bert.encoder.layer.9.attention.output.LayerNorm.bias)\n",
      "  %1221 = Transpose[perm = [1, 0]](%bert.encoder.layer.9.intermediate.dense.weight)\n",
      "  %1222 = MatMul(%1220, %1221)\n",
      "  %1223 = Add(%bert.encoder.layer.9.intermediate.dense.bias, %1222)\n",
      "  %1224 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1225 = Div(%1223, %1224)\n",
      "  %1226 = Erf(%1225)\n",
      "  %1227 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1228 = Add(%1226, %1227)\n",
      "  %1229 = Mul(%1223, %1228)\n",
      "  %1230 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1231 = Mul(%1229, %1230)\n",
      "  %1232 = Transpose[perm = [1, 0]](%bert.encoder.layer.9.output.dense.weight)\n",
      "  %1233 = MatMul(%1231, %1232)\n",
      "  %1234 = Add(%bert.encoder.layer.9.output.dense.bias, %1233)\n",
      "  %1235 = Add(%1234, %1220)\n",
      "  %1236 = ReduceMean[axes = [-1]](%1235)\n",
      "  %1237 = Sub(%1235, %1236)\n",
      "  %1238 = Cast[to = 1](%1237)\n",
      "  %1239 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1240 = Pow(%1238, %1239)\n",
      "  %1241 = ReduceMean[axes = [-1]](%1240)\n",
      "  %1242 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1243 = Add(%1241, %1242)\n",
      "  %1244 = Sqrt(%1243)\n",
      "  %1245 = Div(%1237, %1244)\n",
      "  %1246 = Mul(%1245, %bert.encoder.layer.9.output.LayerNorm.weight)\n",
      "  %1247 = Add(%1246, %bert.encoder.layer.9.output.LayerNorm.bias)\n",
      "  %1248 = Transpose[perm = [1, 0]](%bert.encoder.layer.10.attention.self.query.weight)\n",
      "  %1249 = MatMul(%1247, %1248)\n",
      "  %1250 = Add(%bert.encoder.layer.10.attention.self.query.bias, %1249)\n",
      "  %1251 = Transpose[perm = [1, 0]](%bert.encoder.layer.10.attention.self.key.weight)\n",
      "  %1252 = MatMul(%1247, %1251)\n",
      "  %1253 = Add(%bert.encoder.layer.10.attention.self.key.bias, %1252)\n",
      "  %1254 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1255 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1256 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1257 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1258 = Unsqueeze[axes = [0]](%1254)\n",
      "  %1259 = Unsqueeze[axes = [0]](%1255)\n",
      "  %1260 = Unsqueeze[axes = [0]](%1256)\n",
      "  %1261 = Unsqueeze[axes = [0]](%1257)\n",
      "  %1262 = Concat[axis = 0](%1258, %1259, %1260, %1261)\n",
      "  %1263 = Reshape(%1253, %1262)\n",
      "  %1264 = Transpose[perm = [1, 0]](%bert.encoder.layer.10.attention.self.value.weight)\n",
      "  %1265 = MatMul(%1247, %1264)\n",
      "  %1266 = Add(%bert.encoder.layer.10.attention.self.value.bias, %1265)\n",
      "  %1267 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1268 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1269 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1270 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1271 = Unsqueeze[axes = [0]](%1267)\n",
      "  %1272 = Unsqueeze[axes = [0]](%1268)\n",
      "  %1273 = Unsqueeze[axes = [0]](%1269)\n",
      "  %1274 = Unsqueeze[axes = [0]](%1270)\n",
      "  %1275 = Concat[axis = 0](%1271, %1272, %1273, %1274)\n",
      "  %1276 = Reshape(%1266, %1275)\n",
      "  %1277 = Transpose[perm = [0, 2, 1, 3]](%1276)\n",
      "  %1278 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1279 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1280 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1281 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1282 = Unsqueeze[axes = [0]](%1278)\n",
      "  %1283 = Unsqueeze[axes = [0]](%1279)\n",
      "  %1284 = Unsqueeze[axes = [0]](%1280)\n",
      "  %1285 = Unsqueeze[axes = [0]](%1281)\n",
      "  %1286 = Concat[axis = 0](%1282, %1283, %1284, %1285)\n",
      "  %1287 = Reshape(%1250, %1286)\n",
      "  %1288 = Transpose[perm = [0, 2, 1, 3]](%1287)\n",
      "  %1289 = Transpose[perm = [0, 2, 3, 1]](%1263)\n",
      "  %1290 = MatMul(%1288, %1289)\n",
      "  %1291 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1292 = Div(%1290, %1291)\n",
      "  %1293 = Add(%1292, %225)\n",
      "  %1294 = Softmax[axis = 3](%1293)\n",
      "  %1295 = MatMul(%1294, %1277)\n",
      "  %1296 = Transpose[perm = [0, 2, 1, 3]](%1295)\n",
      "  %1297 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1298 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1299 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1300 = Unsqueeze[axes = [0]](%1297)\n",
      "  %1301 = Unsqueeze[axes = [0]](%1298)\n",
      "  %1302 = Unsqueeze[axes = [0]](%1299)\n",
      "  %1303 = Concat[axis = 0](%1300, %1301, %1302)\n",
      "  %1304 = Reshape(%1296, %1303)\n",
      "  %1305 = Transpose[perm = [1, 0]](%bert.encoder.layer.10.attention.output.dense.weight)\n",
      "  %1306 = MatMul(%1304, %1305)\n",
      "  %1307 = Add(%bert.encoder.layer.10.attention.output.dense.bias, %1306)\n",
      "  %1308 = Add(%1307, %1247)\n",
      "  %1309 = ReduceMean[axes = [-1]](%1308)\n",
      "  %1310 = Sub(%1308, %1309)\n",
      "  %1311 = Cast[to = 1](%1310)\n",
      "  %1312 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1313 = Pow(%1311, %1312)\n",
      "  %1314 = ReduceMean[axes = [-1]](%1313)\n",
      "  %1315 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1316 = Add(%1314, %1315)\n",
      "  %1317 = Sqrt(%1316)\n",
      "  %1318 = Div(%1310, %1317)\n",
      "  %1319 = Mul(%1318, %bert.encoder.layer.10.attention.output.LayerNorm.weight)\n",
      "  %1320 = Add(%1319, %bert.encoder.layer.10.attention.output.LayerNorm.bias)\n",
      "  %1321 = Transpose[perm = [1, 0]](%bert.encoder.layer.10.intermediate.dense.weight)\n",
      "  %1322 = MatMul(%1320, %1321)\n",
      "  %1323 = Add(%bert.encoder.layer.10.intermediate.dense.bias, %1322)\n",
      "  %1324 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1325 = Div(%1323, %1324)\n",
      "  %1326 = Erf(%1325)\n",
      "  %1327 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1328 = Add(%1326, %1327)\n",
      "  %1329 = Mul(%1323, %1328)\n",
      "  %1330 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1331 = Mul(%1329, %1330)\n",
      "  %1332 = Transpose[perm = [1, 0]](%bert.encoder.layer.10.output.dense.weight)\n",
      "  %1333 = MatMul(%1331, %1332)\n",
      "  %1334 = Add(%bert.encoder.layer.10.output.dense.bias, %1333)\n",
      "  %1335 = Add(%1334, %1320)\n",
      "  %1336 = ReduceMean[axes = [-1]](%1335)\n",
      "  %1337 = Sub(%1335, %1336)\n",
      "  %1338 = Cast[to = 1](%1337)\n",
      "  %1339 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1340 = Pow(%1338, %1339)\n",
      "  %1341 = ReduceMean[axes = [-1]](%1340)\n",
      "  %1342 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1343 = Add(%1341, %1342)\n",
      "  %1344 = Sqrt(%1343)\n",
      "  %1345 = Div(%1337, %1344)\n",
      "  %1346 = Mul(%1345, %bert.encoder.layer.10.output.LayerNorm.weight)\n",
      "  %1347 = Add(%1346, %bert.encoder.layer.10.output.LayerNorm.bias)\n",
      "  %1348 = Transpose[perm = [1, 0]](%bert.encoder.layer.11.attention.self.query.weight)\n",
      "  %1349 = MatMul(%1347, %1348)\n",
      "  %1350 = Add(%bert.encoder.layer.11.attention.self.query.bias, %1349)\n",
      "  %1351 = Transpose[perm = [1, 0]](%bert.encoder.layer.11.attention.self.key.weight)\n",
      "  %1352 = MatMul(%1347, %1351)\n",
      "  %1353 = Add(%bert.encoder.layer.11.attention.self.key.bias, %1352)\n",
      "  %1354 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1355 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1356 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1357 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1358 = Unsqueeze[axes = [0]](%1354)\n",
      "  %1359 = Unsqueeze[axes = [0]](%1355)\n",
      "  %1360 = Unsqueeze[axes = [0]](%1356)\n",
      "  %1361 = Unsqueeze[axes = [0]](%1357)\n",
      "  %1362 = Concat[axis = 0](%1358, %1359, %1360, %1361)\n",
      "  %1363 = Reshape(%1353, %1362)\n",
      "  %1364 = Transpose[perm = [1, 0]](%bert.encoder.layer.11.attention.self.value.weight)\n",
      "  %1365 = MatMul(%1347, %1364)\n",
      "  %1366 = Add(%bert.encoder.layer.11.attention.self.value.bias, %1365)\n",
      "  %1367 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1368 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1369 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1370 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1371 = Unsqueeze[axes = [0]](%1367)\n",
      "  %1372 = Unsqueeze[axes = [0]](%1368)\n",
      "  %1373 = Unsqueeze[axes = [0]](%1369)\n",
      "  %1374 = Unsqueeze[axes = [0]](%1370)\n",
      "  %1375 = Concat[axis = 0](%1371, %1372, %1373, %1374)\n",
      "  %1376 = Reshape(%1366, %1375)\n",
      "  %1377 = Transpose[perm = [0, 2, 1, 3]](%1376)\n",
      "  %1378 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1379 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1380 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1381 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1382 = Unsqueeze[axes = [0]](%1378)\n",
      "  %1383 = Unsqueeze[axes = [0]](%1379)\n",
      "  %1384 = Unsqueeze[axes = [0]](%1380)\n",
      "  %1385 = Unsqueeze[axes = [0]](%1381)\n",
      "  %1386 = Concat[axis = 0](%1382, %1383, %1384, %1385)\n",
      "  %1387 = Reshape(%1350, %1386)\n",
      "  %1388 = Transpose[perm = [0, 2, 1, 3]](%1387)\n",
      "  %1389 = Transpose[perm = [0, 2, 3, 1]](%1363)\n",
      "  %1390 = MatMul(%1388, %1389)\n",
      "  %1391 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1392 = Div(%1390, %1391)\n",
      "  %1393 = Add(%1392, %225)\n",
      "  %1394 = Softmax[axis = 3](%1393)\n",
      "  %1395 = MatMul(%1394, %1377)\n",
      "  %1396 = Transpose[perm = [0, 2, 1, 3]](%1395)\n",
      "  %1397 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1398 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1399 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1400 = Unsqueeze[axes = [0]](%1397)\n",
      "  %1401 = Unsqueeze[axes = [0]](%1398)\n",
      "  %1402 = Unsqueeze[axes = [0]](%1399)\n",
      "  %1403 = Concat[axis = 0](%1400, %1401, %1402)\n",
      "  %1404 = Reshape(%1396, %1403)\n",
      "  %1405 = Transpose[perm = [1, 0]](%bert.encoder.layer.11.attention.output.dense.weight)\n",
      "  %1406 = MatMul(%1404, %1405)\n",
      "  %1407 = Add(%bert.encoder.layer.11.attention.output.dense.bias, %1406)\n",
      "  %1408 = Add(%1407, %1347)\n",
      "  %1409 = ReduceMean[axes = [-1]](%1408)\n",
      "  %1410 = Sub(%1408, %1409)\n",
      "  %1411 = Cast[to = 1](%1410)\n",
      "  %1412 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1413 = Pow(%1411, %1412)\n",
      "  %1414 = ReduceMean[axes = [-1]](%1413)\n",
      "  %1415 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1416 = Add(%1414, %1415)\n",
      "  %1417 = Sqrt(%1416)\n",
      "  %1418 = Div(%1410, %1417)\n",
      "  %1419 = Mul(%1418, %bert.encoder.layer.11.attention.output.LayerNorm.weight)\n",
      "  %1420 = Add(%1419, %bert.encoder.layer.11.attention.output.LayerNorm.bias)\n",
      "  %1421 = Transpose[perm = [1, 0]](%bert.encoder.layer.11.intermediate.dense.weight)\n",
      "  %1422 = MatMul(%1420, %1421)\n",
      "  %1423 = Add(%bert.encoder.layer.11.intermediate.dense.bias, %1422)\n",
      "  %1424 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1425 = Div(%1423, %1424)\n",
      "  %1426 = Erf(%1425)\n",
      "  %1427 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1428 = Add(%1426, %1427)\n",
      "  %1429 = Mul(%1423, %1428)\n",
      "  %1430 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1431 = Mul(%1429, %1430)\n",
      "  %1432 = Transpose[perm = [1, 0]](%bert.encoder.layer.11.output.dense.weight)\n",
      "  %1433 = MatMul(%1431, %1432)\n",
      "  %1434 = Add(%bert.encoder.layer.11.output.dense.bias, %1433)\n",
      "  %1435 = Add(%1434, %1420)\n",
      "  %1436 = ReduceMean[axes = [-1]](%1435)\n",
      "  %1437 = Sub(%1435, %1436)\n",
      "  %1438 = Cast[to = 1](%1437)\n",
      "  %1439 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1440 = Pow(%1438, %1439)\n",
      "  %1441 = ReduceMean[axes = [-1]](%1440)\n",
      "  %1442 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1443 = Add(%1441, %1442)\n",
      "  %1444 = Sqrt(%1443)\n",
      "  %1445 = Div(%1437, %1444)\n",
      "  %1446 = Mul(%1445, %bert.encoder.layer.11.output.LayerNorm.weight)\n",
      "  %1447 = Add(%1446, %bert.encoder.layer.11.output.LayerNorm.bias)\n",
      "  %1448 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1449 = Gather[axis = 1](%1447, %1448)\n",
      "  %1450 = Gemm[alpha = 1, beta = 1, transB = 1](%1449, %bert.pooler.dense.weight, %bert.pooler.dense.bias)\n",
      "  %1451 = Tanh(%1450)\n",
      "  %1452 = Gemm[alpha = 1, beta = 1, transB = 1](%1451, %classifier.weight, %classifier.bias)\n",
      "  return %1452\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(onnx.helper.printable_graph(model.graph))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnxruntime in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (1.11.1)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from onnxruntime) (1.22.4)\r\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from onnxruntime) (2.0)\r\n",
      "Requirement already satisfied: protobuf in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from onnxruntime) (3.19.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "ort_session = ort.InferenceSession(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_name = ort_session.get_inputs()[0].name\n",
    "output_name = ort_session.get_outputs()[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ids'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tuple([input_sample[\"ids\"].tolist(), input_sample[\"mask\"].tolist()])\n",
    "\n",
    "ort_inputs = {\n",
    "    'ids':  input_sample[\"ids\"].tolist(),\n",
    "    'mask': input_sample[\"mask\"].tolist(),\n",
    "}\n",
    "\n",
    "outputs = ort_session.run(None, ort_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.sigmoid(torch.FloatTensor(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tresh = 0.5):\n",
    "    y_pred = [[int(val >= tresh) for val in pred] for pred in preds]\n",
    "    y_true = input_sample[\"targets\"].tolist()\n",
    "#     print(y_pred)\n",
    "#     print(y_true)\n",
    "    \n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"Acc score: \", accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       1.00      0.50      0.67         2\n",
      "           4       1.00      1.00      1.00         1\n",
      "           5       0.00      0.00      0.00         0\n",
      "           6       1.00      1.00      1.00         1\n",
      "           7       1.00      1.00      1.00         1\n",
      "           8       0.00      0.00      0.00         0\n",
      "           9       0.00      0.00      0.00         1\n",
      "          10       1.00      1.00      1.00         1\n",
      "          11       1.00      1.00      1.00         1\n",
      "          12       1.00      1.00      1.00         1\n",
      "          13       0.00      0.00      0.00         0\n",
      "          14       0.00      0.00      0.00         0\n",
      "          15       0.00      0.00      0.00         0\n",
      "          16       0.00      0.00      0.00         0\n",
      "          17       0.00      0.00      0.00         0\n",
      "          18       0.00      0.00      0.00         0\n",
      "          19       0.00      0.00      0.00         1\n",
      "          20       0.00      0.00      0.00         0\n",
      "          21       1.00      1.00      1.00         1\n",
      "          22       1.00      1.00      1.00         1\n",
      "          23       0.00      0.00      0.00         0\n",
      "          24       0.00      0.00      0.00         1\n",
      "          25       0.00      0.00      0.00         0\n",
      "          26       0.00      0.00      0.00         0\n",
      "          27       0.00      0.00      0.00         0\n",
      "          28       0.00      0.00      0.00         0\n",
      "          29       0.00      0.00      0.00         0\n",
      "          30       0.00      0.00      0.00         0\n",
      "          31       0.00      0.00      0.00         0\n",
      "          32       0.00      0.00      0.00         0\n",
      "          33       0.00      0.00      0.00         0\n",
      "          34       0.00      0.00      0.00         1\n",
      "          35       0.00      0.00      0.00         0\n",
      "          36       0.50      1.00      0.67         1\n",
      "          37       0.00      0.00      0.00         0\n",
      "          38       0.00      0.00      0.00         0\n",
      "          39       0.00      0.00      0.00         0\n",
      "          40       0.00      0.00      0.00         1\n",
      "          41       1.00      1.00      1.00         1\n",
      "          42       0.00      0.00      0.00         0\n",
      "          43       1.00      1.00      1.00         1\n",
      "          44       0.00      0.00      0.00         0\n",
      "          45       0.00      0.00      0.00         0\n",
      "          46       0.00      0.00      0.00         0\n",
      "          47       1.00      1.00      1.00         1\n",
      "          48       0.00      0.00      0.00         0\n",
      "          49       0.00      0.00      0.00         0\n",
      "          50       1.00      1.00      1.00         1\n",
      "          51       1.00      1.00      1.00         2\n",
      "          52       0.00      0.00      0.00         0\n",
      "          53       0.00      0.00      0.00         0\n",
      "          54       0.00      0.00      0.00         0\n",
      "          55       0.00      0.00      0.00         0\n",
      "          56       0.00      0.00      0.00         0\n",
      "          57       0.00      0.00      0.00         0\n",
      "          58       0.00      0.00      0.00         0\n",
      "          59       0.00      0.00      0.00         0\n",
      "          60       0.00      0.00      0.00         0\n",
      "          61       0.00      0.00      0.00         0\n",
      "          62       0.00      0.00      0.00         0\n",
      "          63       0.00      0.00      0.00         0\n",
      "          64       0.00      0.00      0.00         0\n",
      "          65       0.00      0.00      0.00         0\n",
      "          66       0.00      0.00      0.00         0\n",
      "          67       0.00      0.00      0.00         0\n",
      "          68       0.00      0.00      0.00         0\n",
      "          69       0.00      0.00      0.00         0\n",
      "          70       0.00      0.00      0.00         0\n",
      "          71       0.00      0.00      0.00         0\n",
      "          72       0.00      0.00      0.00         0\n",
      "          73       0.00      0.00      0.00         0\n",
      "          74       0.00      0.00      0.00         0\n",
      "          75       0.00      0.00      0.00         0\n",
      "          76       0.00      0.00      0.00         0\n",
      "          77       0.00      0.00      0.00         0\n",
      "          78       0.00      0.00      0.00         0\n",
      "          79       0.00      0.00      0.00         0\n",
      "          80       0.00      0.00      0.00         0\n",
      "          81       0.00      0.00      0.00         0\n",
      "          82       0.00      0.00      0.00         0\n",
      "          83       0.00      0.00      0.00         0\n",
      "          84       0.00      0.00      0.00         0\n",
      "          85       0.00      0.00      0.00         0\n",
      "          86       0.00      0.00      0.00         0\n",
      "          87       1.00      1.00      1.00         1\n",
      "          88       0.00      0.00      0.00         0\n",
      "          89       0.00      0.00      0.00         0\n",
      "          90       0.00      0.00      0.00         0\n",
      "          91       0.00      0.00      0.00         0\n",
      "          92       0.00      0.00      0.00         0\n",
      "          93       0.00      0.00      0.00         0\n",
      "          94       0.00      0.00      0.00         0\n",
      "          95       0.00      0.00      0.00         0\n",
      "          96       0.00      0.00      0.00         0\n",
      "          97       0.00      0.00      0.00         0\n",
      "          98       0.00      0.00      0.00         0\n",
      "          99       0.00      0.00      0.00         0\n",
      "         100       0.00      0.00      0.00         0\n",
      "         101       0.00      0.00      0.00         0\n",
      "         102       0.00      0.00      0.00         0\n",
      "         103       0.00      0.00      0.00         0\n",
      "         104       1.00      0.50      0.67         2\n",
      "         105       0.00      0.00      0.00         0\n",
      "         106       0.00      0.00      0.00         0\n",
      "         107       0.00      0.00      0.00         0\n",
      "         108       0.00      0.00      0.00         0\n",
      "         109       0.00      0.00      0.00         0\n",
      "         110       0.00      0.00      0.00         0\n",
      "         111       0.00      0.00      0.00         0\n",
      "         112       0.00      0.00      0.00         0\n",
      "         113       1.00      1.00      1.00         1\n",
      "         114       0.00      0.00      0.00         0\n",
      "         115       0.00      0.00      0.00         0\n",
      "         116       0.00      0.00      0.00         0\n",
      "         117       0.00      0.00      0.00         0\n",
      "         118       0.00      0.00      0.00         0\n",
      "         119       0.00      0.00      0.00         0\n",
      "         120       0.00      0.00      0.00         0\n",
      "         121       0.00      0.00      0.00         0\n",
      "         122       1.00      0.50      0.67         2\n",
      "         123       0.00      0.00      0.00         0\n",
      "         124       0.50      1.00      0.67         1\n",
      "         125       0.00      0.00      0.00         0\n",
      "         126       0.00      0.00      0.00         0\n",
      "         127       0.00      0.00      0.00         0\n",
      "         128       0.00      0.00      0.00         0\n",
      "         129       0.00      0.00      0.00         0\n",
      "         130       0.00      0.00      0.00         0\n",
      "         131       0.00      0.00      0.00         0\n",
      "         132       0.00      0.00      0.00         0\n",
      "         133       0.00      0.00      0.00         0\n",
      "         134       0.00      0.00      0.00         0\n",
      "         135       0.00      0.00      0.00         0\n",
      "         136       0.00      0.00      0.00         0\n",
      "         137       0.00      0.00      0.00         0\n",
      "         138       0.00      0.00      0.00         0\n",
      "         139       0.00      0.00      0.00         0\n",
      "         140       0.00      0.00      0.00         0\n",
      "         141       0.00      0.00      0.00         0\n",
      "         142       0.00      0.00      0.00         0\n",
      "         143       0.00      0.00      0.00         0\n",
      "         144       0.00      0.00      0.00         1\n",
      "         145       0.00      0.00      0.00         0\n",
      "         146       0.00      0.00      0.00         0\n",
      "         147       0.00      0.00      0.00         0\n",
      "         148       0.00      0.00      0.00         0\n",
      "         149       0.00      0.00      0.00         0\n",
      "         150       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.81      0.70      0.75        30\n",
      "   macro avg       0.13      0.12      0.12        30\n",
      "weighted avg       0.77      0.70      0.71        30\n",
      " samples avg       0.73      0.70      0.70        30\n",
      "\n",
      "Acc score:  0.4375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
